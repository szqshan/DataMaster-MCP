#!/usr/bin/env python3
"""
DataMaster MCP - è¶…çº§æ•°æ®åˆ†æå·¥å…·
ä¸ºAIæä¾›å¼ºå¤§çš„æ•°æ®åˆ†æèƒ½åŠ›

æ ¸å¿ƒç†å¿µï¼šå·¥å…·ä¸“æ³¨æ•°æ®è·å–å’Œè®¡ç®—ï¼ŒAIä¸“æ³¨æ™ºèƒ½åˆ†æå’Œæ´å¯Ÿ
"""

import json
import sqlite3
import pandas as pd
import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging
from mcp.server.fastmcp import FastMCP
import numpy as np
from scipy import stats

# è®¾ç½®æ—¥å¿—ï¼ˆéœ€è¦åœ¨ä½¿ç”¨ logger ä¹‹å‰å®šä¹‰ï¼‰
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("DataMaster_MCP")

# SQLAlchemy imports for pandas to_sql compatibility
try:
    from sqlalchemy import create_engine
    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    logger.warning("SQLAlchemy not available. External database import may not work properly.")

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆå½“ä½œä¸ºåŒ…ä½¿ç”¨æ—¶ï¼‰
    from .config.database_manager import database_manager
    from .config.config_manager import config_manager
    from .config.api_config_manager import api_config_manager
    from .config.api_connector import api_connector
    from .config.data_transformer import data_transformer
    from .config.api_data_storage import api_data_storage
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥ï¼ˆå½“ç›´æ¥è¿è¡Œæ—¶ï¼‰
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
    current_dir = Path(__file__).parent.parent
    if str(current_dir) not in sys.path:
        sys.path.insert(0, str(current_dir))
    
    from datamaster_mcp.config.database_manager import database_manager
    from datamaster_mcp.config.config_manager import config_manager
    from datamaster_mcp.config.api_config_manager import api_config_manager
    from datamaster_mcp.config.api_connector import api_connector
    from datamaster_mcp.config.data_transformer import data_transformer
    from datamaster_mcp.config.api_data_storage import api_data_storage

# ================================
# DataFrameåºåˆ—åŒ–å¤„ç†
# ================================

def _serialize_dataframe(df) -> dict:
    """å°†DataFrameåºåˆ—åŒ–ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    try:
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data = {
            "columns": df.columns.tolist(),
            "data": df.values.tolist(),
            "index": df.index.tolist(),
            "shape": df.shape,
            "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}
        }
        return data
    except Exception as e:
        logger.warning(f"DataFrameåºåˆ—åŒ–å¤±è´¥: {e}")
        # é™çº§å¤„ç†ï¼šè½¬æ¢ä¸ºç®€å•çš„è®°å½•æ ¼å¼
        try:
            return {
                "columns": df.columns.tolist(),
                "records": df.to_dict('records'),
                "shape": df.shape,
                "note": "ä½¿ç”¨ç®€åŒ–æ ¼å¼ï¼Œéƒ¨åˆ†ç±»å‹ä¿¡æ¯å¯èƒ½ä¸¢å¤±"
            }
        except Exception as e2:
            logger.error(f"DataFrameç®€åŒ–åºåˆ—åŒ–ä¹Ÿå¤±è´¥: {e2}")
            return {
                "error": "DataFrameåºåˆ—åŒ–å¤±è´¥",
                "shape": getattr(df, 'shape', 'unknown'),
                "columns": getattr(df, 'columns', []).tolist() if hasattr(df, 'columns') else []
            }

def _handle_data_format(data, format_type: str = "dict"):
    """å¤„ç†ä¸åŒæ•°æ®æ ¼å¼çš„è¾“å‡º"""
    if format_type == "dataframe" and isinstance(data, pd.DataFrame):
        return _serialize_dataframe(data)
    elif isinstance(data, pd.DataFrame):
        # é»˜è®¤è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        try:
            return data.to_dict('records')
        except Exception:
            return _serialize_dataframe(data)
    else:
        return data


# ================================
# 1. é…ç½®å’Œåˆå§‹åŒ–
# ================================
TOOL_NAME = "DataMaster_MCP"
DB_PATH = "data/analysis.db"
DATA_DIR = "data"
EXPORTS_DIR = "exports"

# åˆ›å»ºMCPæœåŠ¡å™¨
mcp = FastMCP(TOOL_NAME)

# ç¡®ä¿ç›®å½•å­˜åœ¨
for directory in [DATA_DIR, EXPORTS_DIR]:
    Path(directory).mkdir(exist_ok=True)

# ================================
# 2. æ•°æ®åº“ç®¡ç†
# ================================

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥æŒ‰åˆ—åè®¿é—®
    return conn

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        with get_db_connection() as conn:
            # åˆ›å»ºå…ƒæ•°æ®è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS _metadata (
                    table_name TEXT PRIMARY KEY,
                    created_at TEXT,
                    source_type TEXT,
                    source_path TEXT,
                    row_count INTEGER
                )
            """)
            
            # åˆ›å»ºæ•°æ®å¤„ç†å…ƒæ•°æ®è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS data_metadata (
                    table_name TEXT PRIMARY KEY,
                    source_type TEXT,
                    source_path TEXT,
                    created_at TEXT,
                    metadata TEXT
                )
            """)
            
            conn.commit()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def _escape_identifier(identifier: str) -> str:
    """è½¬ä¹‰SQLæ ‡è¯†ç¬¦ï¼ˆè¡¨åã€åˆ—åç­‰ï¼‰"""
    # ä½¿ç”¨åŒå¼•å·åŒ…å›´æ ‡è¯†ç¬¦ï¼Œå¹¶è½¬ä¹‰å†…éƒ¨çš„åŒå¼•å·
    return '"' + identifier.replace('"', '""') + '"'

def _safe_table_query(table_name: str, query_template: str) -> str:
    """å®‰å…¨åœ°æ„å»ºåŒ…å«è¡¨åçš„æŸ¥è¯¢"""
    escaped_table = _escape_identifier(table_name)
    return query_template.format(table=escaped_table)

def _table_exists(table_name: str) -> bool:
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    try:
        with get_db_connection() as conn:
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                (table_name,)
            )
            return cursor.fetchone() is not None
    except Exception:
        return False

def _analyze_database_cleanup(conn) -> dict:
    """åˆ†ææ•°æ®åº“å¹¶æä¾›æ¸…ç†å»ºè®®"""
    try:
        # è·å–æ‰€æœ‰è¡¨ï¼ˆæ’é™¤å…ƒæ•°æ®è¡¨ï¼‰
        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name != '_metadata'"
        )
        all_tables = [row[0] for row in cursor.fetchall()]
        
        if not all_tables:
            return {
                "total_tables": 0,
                "cleanup_suggestions": [],
                "summary": "æ•°æ®åº“ä¸­æ²¡æœ‰ç”¨æˆ·è¡¨ï¼Œæ— éœ€æ¸…ç†",
                "ai_recommendation": "æ•°æ®åº“çŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€æ‰§è¡Œæ¸…ç†æ“ä½œã€‚"
            }
        
        cleanup_suggestions = []
        test_tables = []
        temp_tables = []
        empty_tables = []
        duplicate_tables = []
        old_tables = []
        
        # åˆ†ææ¯ä¸ªè¡¨
        for table in all_tables:
            try:
                escaped_table = _escape_identifier(table)
                
                # è·å–è¡¨çš„è¡Œæ•°
                cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                row_count = cursor.fetchone()[0]
                
                # æ£€æµ‹æµ‹è¯•è¡¨ï¼ˆåŒ…å«testã€tempã€tmpã€demoç­‰å…³é”®è¯ï¼‰
                table_lower = table.lower()
                if any(keyword in table_lower for keyword in ['test', 'temp', 'tmp', 'demo', 'sample', 'example', '_bak', '_backup']):
                    test_tables.append({
                        "table_name": table,
                        "row_count": row_count,
                        "reason": "è¡¨ååŒ…å«æµ‹è¯•/ä¸´æ—¶å…³é”®è¯",
                        "risk_level": "low" if row_count == 0 else "medium"
                    })
                
                # æ£€æµ‹ç©ºè¡¨
                if row_count == 0:
                    empty_tables.append({
                        "table_name": table,
                        "row_count": 0,
                        "reason": "è¡¨ä¸ºç©ºï¼Œæ— æ•°æ®",
                        "risk_level": "low"
                    })
                
                # æ£€æµ‹å¯èƒ½çš„é‡å¤è¡¨ï¼ˆç›¸ä¼¼è¡¨åï¼‰
                for other_table in all_tables:
                    if table != other_table and table.startswith(other_table) and len(table) > len(other_table):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰ˆæœ¬åŒ–çš„è¡¨ï¼ˆå¦‚table_v1, table_v2ç­‰ï¼‰
                        suffix = table[len(other_table):]
                        if suffix.startswith('_') and any(char.isdigit() for char in suffix):
                            duplicate_tables.append({
                                "table_name": table,
                                "base_table": other_table,
                                "row_count": row_count,
                                "reason": f"å¯èƒ½æ˜¯ '{other_table}' çš„ç‰ˆæœ¬åŒ–å‰¯æœ¬",
                                "risk_level": "medium"
                            })
                            break
                
                # æ£€æµ‹è¡¨ç»“æ„è·å–åˆ›å»ºä¿¡æ¯ï¼ˆSQLiteé™åˆ¶ï¼Œæ— æ³•ç›´æ¥è·å–åˆ›å»ºæ—¶é—´ï¼‰
                # ä½†å¯ä»¥é€šè¿‡è¡¨åæ¨¡å¼æ¨æ–­æ˜¯å¦ä¸ºæ—§è¡¨
                if any(pattern in table_lower for pattern in ['old', 'archive', 'history', 'backup', 'deprecated']):
                    old_tables.append({
                        "table_name": table,
                        "row_count": row_count,
                        "reason": "è¡¨åæš—ç¤ºä¸ºå†å²/å½’æ¡£æ•°æ®",
                        "risk_level": "medium"
                    })
                    
            except Exception as e:
                logger.warning(f"åˆ†æè¡¨ '{table}' æ—¶å‡ºé”™: {e}")
                continue
        
        # ç”Ÿæˆæ¸…ç†å»ºè®®
        if test_tables:
            cleanup_suggestions.append({
                "category": "æµ‹è¯•/ä¸´æ—¶è¡¨",
                "tables": test_tables,
                "description": "è¿™äº›è¡¨çœ‹èµ·æ¥æ˜¯ç”¨äºæµ‹è¯•æˆ–ä¸´æ—¶ç”¨é€”çš„",
                "recommendation": "å»ºè®®åˆ é™¤è¿™äº›æµ‹è¯•è¡¨ä»¥ä¿æŒæ•°æ®åº“æ•´æ´",
                "action": "DELETE",
                "priority": "HIGH" if any(t['row_count'] == 0 for t in test_tables) else "MEDIUM"
            })
        
        if empty_tables:
            cleanup_suggestions.append({
                "category": "ç©ºè¡¨",
                "tables": empty_tables,
                "description": "è¿™äº›è¡¨æ²¡æœ‰ä»»ä½•æ•°æ®",
                "recommendation": "å»ºè®®åˆ é™¤ç©ºè¡¨ï¼Œå¦‚éœ€è¦å¯ä»¥é‡æ–°åˆ›å»º",
                "action": "DELETE",
                "priority": "HIGH"
            })
        
        if duplicate_tables:
            cleanup_suggestions.append({
                "category": "é‡å¤/ç‰ˆæœ¬åŒ–è¡¨",
                "tables": duplicate_tables,
                "description": "è¿™äº›è¡¨å¯èƒ½æ˜¯å…¶ä»–è¡¨çš„å‰¯æœ¬æˆ–æ—§ç‰ˆæœ¬",
                "recommendation": "è¯·ç¡®è®¤æ˜¯å¦éœ€è¦ä¿ç•™è¿™äº›è¡¨ï¼Œå»ºè®®åˆ é™¤ä¸éœ€è¦çš„ç‰ˆæœ¬",
                "action": "REVIEW",
                "priority": "MEDIUM"
            })
        
        if old_tables:
            cleanup_suggestions.append({
                "category": "å†å²/å½’æ¡£è¡¨",
                "tables": old_tables,
                "description": "è¿™äº›è¡¨çœ‹èµ·æ¥æ˜¯å†å²æ•°æ®æˆ–å½’æ¡£æ•°æ®",
                "recommendation": "è¯·ç¡®è®¤æ˜¯å¦è¿˜éœ€è¦è¿™äº›å†å²æ•°æ®ï¼Œå¯è€ƒè™‘å¯¼å‡ºååˆ é™¤",
                "action": "REVIEW",
                "priority": "LOW"
            })
        
        # ç”ŸæˆAIå»ºè®®
        total_suggested_for_deletion = len([t for cat in cleanup_suggestions for t in cat['tables'] if cat['action'] == 'DELETE'])
        total_suggested_for_review = len([t for cat in cleanup_suggestions for t in cat['tables'] if cat['action'] == 'REVIEW'])
        
        if not cleanup_suggestions:
            ai_recommendation = "ğŸ‰ æ•°æ®åº“çŠ¶æ€è‰¯å¥½ï¼æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„è¿‡æ—¶æ•°æ®æˆ–è¡¨ã€‚"
        else:
            ai_recommendation = f"ğŸ“‹ å‘ç° {len(cleanup_suggestions)} ç±»é—®é¢˜éœ€è¦å¤„ç†ï¼š\n"
            if total_suggested_for_deletion > 0:
                ai_recommendation += f"â€¢ å»ºè®®ç›´æ¥åˆ é™¤ {total_suggested_for_deletion} ä¸ªè¡¨ï¼ˆæµ‹è¯•è¡¨/ç©ºè¡¨ï¼‰\n"
            if total_suggested_for_review > 0:
                ai_recommendation += f"â€¢ éœ€è¦äººå·¥ç¡®è®¤ {total_suggested_for_review} ä¸ªè¡¨ï¼ˆé‡å¤è¡¨/å†å²è¡¨ï¼‰\n"
            ai_recommendation += "\nğŸ’¡ å»ºè®®ï¼šå…ˆå¤‡ä»½é‡è¦æ•°æ®ï¼Œç„¶åæŒ‰ä¼˜å…ˆçº§å¤„ç†æ¸…ç†å»ºè®®ã€‚"
        
        # ç”Ÿæˆæ¸…ç†ç»Ÿè®¡
        cleanup_stats = {
            "total_tables": len(all_tables),
            "test_tables_count": len(test_tables),
            "empty_tables_count": len(empty_tables),
            "duplicate_tables_count": len(duplicate_tables),
            "old_tables_count": len(old_tables),
            "total_issues": len(cleanup_suggestions),
            "high_priority_issues": len([c for c in cleanup_suggestions if c['priority'] == 'HIGH']),
            "medium_priority_issues": len([c for c in cleanup_suggestions if c['priority'] == 'MEDIUM']),
            "low_priority_issues": len([c for c in cleanup_suggestions if c['priority'] == 'LOW'])
        }
        
        return {
            "cleanup_stats": cleanup_stats,
            "cleanup_suggestions": cleanup_suggestions,
            "ai_recommendation": ai_recommendation,
            "summary": f"åˆ†æäº† {len(all_tables)} ä¸ªè¡¨ï¼Œå‘ç° {len(cleanup_suggestions)} ç±»æ¸…ç†å»ºè®®",
            "next_steps": [
                "1. æŸ¥çœ‹æ¸…ç†å»ºè®®è¯¦æƒ…",
                "2. å¤‡ä»½é‡è¦æ•°æ®ï¼ˆå¦‚éœ€è¦ï¼‰",
                "3. æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œæ¸…ç†æ“ä½œ",
                "4. å®šæœŸè¿è¡Œæ¸…ç†åˆ†æä¿æŒæ•°æ®åº“æ•´æ´"
            ]
        }
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ¸…ç†åˆ†æå¤±è´¥: {e}")
        return {
            "error": f"æ¸…ç†åˆ†æå¤±è´¥: {str(e)}",
            "cleanup_suggestions": [],
            "ai_recommendation": "âŒ æ¸…ç†åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæƒé™ã€‚"
        }



# ================================
# 3. æ ¸å¿ƒå·¥å…·å®ç°
# ================================

@mcp.tool()
def connect_data_source(
    source_type: str,
    config: dict,
    target_table: str = None,
    target_database: str = None
) -> str:
    """
    ğŸ”— æ•°æ®æºè¿æ¥è·¯ç”±å™¨ - AIå¿…è¯»ä½¿ç”¨æŒ‡å—
    
    âš ï¸ é‡è¦ï¼šæ•°æ®åº“è¿æ¥é‡‡ç”¨"ä¸¤æ­¥è¿æ¥æ³•"è®¾è®¡æ¨¡å¼ï¼
    
    ğŸ“‹ æ”¯æŒçš„æ•°æ®æºç±»å‹ï¼š
    - "excel" - Excelæ–‡ä»¶å¯¼å…¥åˆ°æ•°æ®åº“
    - "csv" - CSVæ–‡ä»¶å¯¼å…¥åˆ°æ•°æ®åº“
    - "json" - JSONæ–‡ä»¶å¯¼å…¥åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒåµŒå¥—ç»“æ„è‡ªåŠ¨æ‰å¹³åŒ–ï¼‰
    - "sqlite" - SQLiteæ•°æ®åº“æ–‡ä»¶è¿æ¥
    - "mysql" - MySQLæ•°æ®åº“è¿æ¥ï¼ˆç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä¸´æ—¶é…ç½®ï¼‰
    - "postgresql" - PostgreSQLæ•°æ®åº“è¿æ¥ï¼ˆç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä¸´æ—¶é…ç½®ï¼‰
    - "mongodb" - MongoDBæ•°æ®åº“è¿æ¥ï¼ˆç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä¸´æ—¶é…ç½®ï¼‰
    - "database_config" - ä½¿ç”¨å·²æœ‰é…ç½®è¿æ¥ï¼ˆç¬¬äºŒæ­¥ï¼šå®é™…è¿æ¥ï¼‰
    
    ğŸ¯ AIä½¿ç”¨æµç¨‹ï¼š
    1ï¸âƒ£ æ•°æ®åº“è¿æ¥ç¬¬ä¸€æ­¥ï¼š
       connect_data_source(source_type="mysql", config={host, port, user, database, password})
       â†’ è¿”å›ä¸´æ—¶é…ç½®åç§°ï¼ˆå¦‚ï¼štemp_mysql_20250724_173102ï¼‰
    
    2ï¸âƒ£ æ•°æ®åº“è¿æ¥ç¬¬äºŒæ­¥ï¼š
       connect_data_source(source_type="database_config", config={"database_name": "é…ç½®åç§°"})
       â†’ å»ºç«‹å¯æŸ¥è¯¢çš„æ•°æ®åº“è¿æ¥
    
    3ï¸âƒ£ æŸ¥è¯¢æ•°æ®ï¼š
       ä½¿ç”¨ query_external_database(database_name="é…ç½®åç§°", query="SQL")
    
    ğŸ’¡ å‚æ•°å…¼å®¹æ€§ï¼š
    - æ”¯æŒ "user" æˆ– "username" å‚æ•°
    - ç«¯å£å·ä½¿ç”¨æ•°å­—ç±»å‹ï¼ˆå¦‚ï¼š3306ï¼‰
    - å¯†ç ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹
    
    Args:
        source_type: æ•°æ®æºç±»å‹ï¼Œå¿…é¡»æ˜¯ä¸Šè¿°æ”¯æŒçš„ç±»å‹ä¹‹ä¸€
        config: é…ç½®å‚æ•°å­—å…¸ï¼Œæ ¼å¼æ ¹æ®source_typeä¸åŒ
        target_table: ç›®æ ‡è¡¨åï¼ˆæ–‡ä»¶å¯¼å…¥æ—¶å¯é€‰ï¼‰
        target_database: ç›®æ ‡æ•°æ®åº“åç§°ï¼ˆæ–‡ä»¶å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“æ—¶å¯é€‰ï¼‰
    
    Returns:
        str: JSONæ ¼å¼çš„è¿æ¥ç»“æœï¼ŒåŒ…å«çŠ¶æ€ã€æ¶ˆæ¯å’Œé…ç½®ä¿¡æ¯
    
    âš¡ AIå¿«é€Ÿä¸Šæ‰‹ï¼š
    è®°ä½"ä¸¤æ­¥è¿æ¥æ³•"ï¼šå…ˆåˆ›å»ºé…ç½® â†’ å†ä½¿ç”¨é…ç½® â†’ æœ€åæŸ¥è¯¢æ•°æ®
    """
    try:
        if source_type == "excel":
            return _import_excel(config, target_table, target_database)
        elif source_type == "csv":
            return _import_csv(config, target_table, target_database)
        elif source_type == "json":
            return _import_json(config, target_table, target_database)
        elif source_type == "sqlite":
            return _connect_sqlite(config, target_table)
        elif source_type == "mysql":
            return _connect_external_database("mysql", config, target_table)
        elif source_type == "postgresql":
            return _connect_external_database("postgresql", config, target_table)
        elif source_type == "mongodb":
            return _connect_external_database("mongodb", config, target_table)
        elif source_type == "database_config":
            return _connect_from_config(config, target_table)
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}",
                "supported_types": ["excel", "csv", "json", "sqlite", "mysql", "postgresql", "mongodb", "database_config"]
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"æ•°æ®æºè¿æ¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®æºè¿æ¥å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _import_excel(config: dict, target_table: str = None, target_database: str = None) -> str:
    """å¯¼å…¥Excelæ–‡ä»¶åˆ°æœ¬åœ°SQLiteæˆ–å¤–éƒ¨æ•°æ®åº“"""
    try:
        # è·å–é…ç½®å‚æ•°
        file_path = config.get('file_path')
        sheet_name = config.get('sheet_name', 0)  # é»˜è®¤ç¬¬ä¸€ä¸ªsheet
        
        if not file_path:
            raise ValueError("ç¼ºå°‘file_pathå‚æ•°")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        
        # ç”Ÿæˆè¡¨å
        if not target_table:
            file_name = Path(file_path).stem
            target_table = f"excel_{file_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ¸…ç†åˆ—åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        df.columns = [col.replace(' ', '_').replace('-', '_').replace('.', '_') for col in df.columns]
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        if target_database:
            # å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“
            return _import_to_external_database(df, target_table, target_database, 'excel', file_path, sheet_name)
        else:
            # å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“
            with get_db_connection() as conn:
                df.to_sql(target_table, conn, if_exists='replace', index=False)
                
                # æ›´æ–°å…ƒæ•°æ®
                conn.execute("""
                    INSERT OR REPLACE INTO _metadata 
                    (table_name, created_at, source_type, source_path, row_count)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    target_table,
                    datetime.now().isoformat(),
                    'excel',
                    file_path,
                    len(df)
                ))
                conn.commit()
            
            result = {
                "status": "success",
                "message": "Excelæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“",
                "data": {
                    "table_name": target_table,
                    "row_count": len(df),
                    "column_count": len(df.columns),
                    "columns": list(df.columns),
                    "file_path": file_path,
                    "sheet_name": sheet_name,
                    "connection_type": "æœ¬åœ°æ•°æ®å¯¼å…¥",
                    "data_location": f"æœ¬åœ°SQLiteæ•°æ®åº“ ({DB_PATH})",
                    "usage_note": f"ä½¿ç”¨execute_sql('SELECT * FROM \"{target_table}\"')æŸ¥è¯¢æ­¤è¡¨æ•°æ®"
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "source_type": "excel"
                }
            }
        
        return f"âœ… Excelæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"Excelå¯¼å…¥å¤±è´¥: {e}")
        raise

def _import_csv(config: dict, target_table: str = None, target_database: str = None) -> str:
    """å¯¼å…¥CSVæ–‡ä»¶åˆ°æœ¬åœ°SQLiteæˆ–å¤–éƒ¨æ•°æ®åº“"""
    try:
        file_path = config.get('file_path')
        encoding = config.get('encoding', 'utf-8')
        separator = config.get('separator', ',')
        
        if not file_path:
            raise ValueError("ç¼ºå°‘file_pathå‚æ•°")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(file_path, encoding=encoding, sep=separator)
        
        # ç”Ÿæˆè¡¨å
        if not target_table:
            file_name = Path(file_path).stem
            target_table = f"csv_{file_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ¸…ç†åˆ—å
        df.columns = [col.replace(' ', '_').replace('-', '_').replace('.', '_') for col in df.columns]
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        if target_database:
            # å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“
            return _import_to_external_database(df, target_table, target_database, 'csv', file_path, {'encoding': encoding, 'separator': separator})
        else:
            # å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“
            with get_db_connection() as conn:
                df.to_sql(target_table, conn, if_exists='replace', index=False)
                
                # æ›´æ–°å…ƒæ•°æ®
                conn.execute("""
                    INSERT OR REPLACE INTO _metadata 
                    (table_name, created_at, source_type, source_path, row_count)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    target_table,
                    datetime.now().isoformat(),
                    'csv',
                    file_path,
                    len(df)
                ))
                conn.commit()
            
            result = {
                "status": "success",
                "message": "CSVæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“",
                "data": {
                    "table_name": target_table,
                    "row_count": len(df),
                    "column_count": len(df.columns),
                    "columns": list(df.columns),
                    "file_path": file_path,
                    "connection_type": "æœ¬åœ°æ•°æ®å¯¼å…¥",
                    "data_location": f"æœ¬åœ°SQLiteæ•°æ®åº“ ({DB_PATH})",
                    "usage_note": f"ä½¿ç”¨execute_sql('SELECT * FROM \"{target_table}\"')æŸ¥è¯¢æ­¤è¡¨æ•°æ®"
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "source_type": "csv"
                }
            }
            
            return f"âœ… CSVæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"CSVå¯¼å…¥å¤±è´¥: {e}")
        raise

def _import_json(config: dict, target_table: str = None, target_database: str = None) -> str:
    """å¯¼å…¥JSONæ–‡ä»¶åˆ°æœ¬åœ°SQLiteæˆ–å¤–éƒ¨æ•°æ®åº“"""
    try:
        file_path = config.get('file_path')
        encoding = config.get('encoding', 'utf-8')
        flatten_nested = config.get('flatten_nested', True)  # æ˜¯å¦æ‰å¹³åŒ–åµŒå¥—ç»“æ„
        max_nesting_level = config.get('max_nesting_level', 3)  # æœ€å¤§åµŒå¥—å±‚çº§
        
        if not file_path:
            raise ValueError("ç¼ºå°‘file_pathå‚æ•°")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–JSONæ–‡ä»¶
        with open(file_path, 'r', encoding=encoding) as f:
            json_data = json.load(f)
        
        # å¤„ç†ä¸åŒçš„JSONç»“æ„
        if isinstance(json_data, list):
            # JSONæ•°ç»„ï¼Œç›´æ¥è½¬æ¢ä¸ºDataFrame
            df = pd.json_normalize(json_data, max_level=max_nesting_level if flatten_nested else None)
        elif isinstance(json_data, dict):
            # JSONå¯¹è±¡ï¼Œéœ€è¦åˆ¤æ–­ç»“æ„
            if any(isinstance(v, list) for v in json_data.values()):
                # åŒ…å«æ•°ç»„çš„å¯¹è±¡ï¼Œå°è¯•æ‰¾åˆ°ä¸»è¦çš„æ•°æ®æ•°ç»„
                main_data = None
                for key, value in json_data.items():
                    if isinstance(value, list) and len(value) > 0:
                        main_data = value
                        break
                
                if main_data is not None:
                    df = pd.json_normalize(main_data, max_level=max_nesting_level if flatten_nested else None)
                    # æ·»åŠ å…¶ä»–éæ•°ç»„å­—æ®µä½œä¸ºå¸¸é‡åˆ—
                    for key, value in json_data.items():
                        if not isinstance(value, list):
                            df[f'root_{key}'] = value
                else:
                    # æ²¡æœ‰æ‰¾åˆ°æ•°ç»„ï¼Œå°†æ•´ä¸ªå¯¹è±¡ä½œä¸ºå•è¡Œæ•°æ®
                    df = pd.json_normalize([json_data], max_level=max_nesting_level if flatten_nested else None)
            else:
                # çº¯å¯¹è±¡ï¼Œä½œä¸ºå•è¡Œæ•°æ®
                df = pd.json_normalize([json_data], max_level=max_nesting_level if flatten_nested else None)
        else:
            # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå•è¡Œå•åˆ—æ•°æ®
            df = pd.DataFrame({'value': [json_data]})
        
        # æ¸…ç†åˆ—åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        df.columns = [str(col).replace(' ', '_').replace('-', '_').replace('.', '_').replace('[', '_').replace(']', '_') for col in df.columns]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†å¤æ‚æ•°æ®ç±»å‹ï¼Œç¡®ä¿SQLiteå…¼å®¹æ€§
        for col in df.columns:
            df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False, default=str) if isinstance(x, (list, dict)) else x)
        
        # ç”Ÿæˆè¡¨å
        if not target_table:
            file_name = Path(file_path).stem
            target_table = f"json_{file_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        if target_database:
            # å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“
            return _import_to_external_database(df, target_table, target_database, 'json', file_path, 
                                               {'encoding': encoding, 'flatten_nested': flatten_nested, 'max_nesting_level': max_nesting_level})
        else:
            # å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“
            with get_db_connection() as conn:
                df.to_sql(target_table, conn, if_exists='replace', index=False)
                
                # æ›´æ–°å…ƒæ•°æ®
                conn.execute("""
                    INSERT OR REPLACE INTO _metadata 
                    (table_name, created_at, source_type, source_path, row_count)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    target_table,
                    datetime.now().isoformat(),
                    'json',
                    file_path,
                    len(df)
                ))
                conn.commit()
            
            result = {
                "status": "success",
                "message": "JSONæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“",
                "data": {
                    "table_name": target_table,
                    "row_count": len(df),
                    "column_count": len(df.columns),
                    "columns": list(df.columns),
                    "file_path": file_path,
                    "flatten_nested": flatten_nested,
                    "max_nesting_level": max_nesting_level,
                    "connection_type": "æœ¬åœ°æ•°æ®å¯¼å…¥",
                    "data_location": f"æœ¬åœ°SQLiteæ•°æ®åº“ ({DB_PATH})",
                    "usage_note": f"ä½¿ç”¨execute_sql('SELECT * FROM \"{target_table}\"')æŸ¥è¯¢æ­¤è¡¨æ•°æ®"
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "source_type": "json"
                }
            }
            
            return f"âœ… JSONæ–‡ä»¶å·²å¯¼å…¥åˆ°æœ¬åœ°SQLiteæ•°æ®åº“\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"JSONå¯¼å…¥å¤±è´¥: {e}")
        raise

def _create_sqlalchemy_engine(database_name: str):
    """ä¸ºæŒ‡å®šæ•°æ®åº“åˆ›å»ºSQLAlchemyå¼•æ“"""
    try:
        # è·å–æ•°æ®åº“é…ç½®
        available_databases = database_manager.get_available_databases()
        if database_name not in available_databases:
            logger.error(f"Database config not found: {database_name}")
            return None
        
        db_info = available_databases[database_name]
        db_type = db_info.get("type", "unknown").lower()
        
        # è·å–æ•°æ®åº“é…ç½®è¯¦æƒ…
        config = database_manager.config_manager.get_database_config(database_name)
        if not config:
            logger.error(f"Failed to get database config for: {database_name}")
            return None
        
        # æ ¹æ®æ•°æ®åº“ç±»å‹åˆ›å»ºè¿æ¥å­—ç¬¦ä¸²
        if db_type == "mysql":
            # MySQLè¿æ¥å­—ç¬¦ä¸²
            host = config.get("host", "localhost")
            port = config.get("port", 3306)
            user = config.get("user") or config.get("username")
            password = config.get("password")
            database = config.get("database")
            charset = config.get("charset", "utf8mb4")
            
            # ä½¿ç”¨pymysqlé©±åŠ¨
            connection_string = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}?charset={charset}"
            
        elif db_type == "postgresql":
            # PostgreSQLè¿æ¥å­—ç¬¦ä¸²
            host = config.get("host", "localhost")
            port = config.get("port", 5432)
            user = config.get("user") or config.get("username")
            password = config.get("password")
            database = config.get("database")
            
            # ä½¿ç”¨psycopg2é©±åŠ¨
            connection_string = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}"
            
        elif db_type == "sqlite":
            # SQLiteè¿æ¥å­—ç¬¦ä¸²
            file_path = config.get("file_path")
            connection_string = f"sqlite:///{file_path}"
            
        else:
            logger.error(f"Unsupported database type for SQLAlchemy: {db_type}")
            return None
        
        # åˆ›å»ºå¼•æ“
        logger.info(f"Creating SQLAlchemy engine for {database_name} with connection string: {connection_string[:50]}...")
        engine = create_engine(connection_string, echo=False)
        logger.info(f"SQLAlchemy engine created successfully for {database_name}")
        return engine
        
    except Exception as e:
        logger.error(f"Failed to create SQLAlchemy engine for {database_name}: {e}")
        return None

def _import_to_external_database(df, target_table: str, target_database: str, source_type: str, source_path: str, import_config: dict) -> str:
    """å°†DataFrameå¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“"""
    try:
        # éªŒè¯ç›®æ ‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        available_databases = database_manager.get_available_databases()
        if target_database not in available_databases:
            result = {
                "status": "error",
                "message": f"ç›®æ ‡æ•°æ®åº“é…ç½®ä¸å­˜åœ¨: {target_database}",
                "available_databases": list(available_databases.keys())
            }
            return f"âŒ å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        is_valid, message = database_manager.test_connection(target_database)
        if not is_valid:
            result = {
                "status": "error",
                "message": f"ç›®æ ‡æ•°æ®åº“è¿æ¥å¤±è´¥: {message}",
                "database_name": target_database
            }
            return f"âŒ å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = available_databases[target_database]
        db_type = db_info.get("type", "unknown")
        
        # ä½¿ç”¨database_managerå¯¼å…¥æ•°æ®
        if db_type.lower() == 'mongodb':
            # MongoDBç‰¹æ®Šå¤„ç†
            with database_manager.get_connection(target_database) as conn:
                records = df.to_dict('records')
                collection = conn[target_table]
                # æ¸…ç©ºç°æœ‰æ•°æ®
                collection.delete_many({})
                # æ’å…¥æ–°æ•°æ®
                if records:
                    collection.insert_many(records)
                row_count = len(records)
        else:
            # SQLæ•°æ®åº“ï¼ˆMySQL, PostgreSQL, SQLiteï¼‰éœ€è¦SQLAlchemyå¼•æ“
            if not SQLALCHEMY_AVAILABLE:
                raise ImportError("SQLAlchemy is required for SQL database import. Please install: pip install sqlalchemy")
            
            # åˆ›å»ºSQLAlchemyå¼•æ“
            logger.info(f"Starting external database import for {target_database}, table: {target_table}")
            engine = _create_sqlalchemy_engine(target_database)
            if engine is None:
                raise ValueError(f"Failed to create SQLAlchemy engine for database: {target_database}")
            
            # ä½¿ç”¨pandas to_sql with SQLAlchemy engine
            logger.info(f"Importing {len(df)} rows to table {target_table} using pandas to_sql")
            df.to_sql(target_table, engine, if_exists='replace', index=False, method='multi')
            logger.info(f"Successfully imported data to table {target_table}")
            row_count = len(df)
            engine.dispose()  # æ¸…ç†è¿æ¥æ± 
        
        # æ„å»ºæˆåŠŸç»“æœ
        result = {
            "status": "success",
            "message": f"{source_type.upper()}æ–‡ä»¶å·²æˆåŠŸå¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“",
            "data": {
                "table_name": target_table,
                "database_name": target_database,
                "database_type": db_type,
                "source_file": source_path,
                "row_count": row_count,
                "column_count": len(df.columns),
                "columns": list(df.columns),
                "connection_type": "å¤–éƒ¨æ•°æ®åº“å¯¼å…¥",
                "data_location": "è¿œç¨‹æ•°æ®åº“æœåŠ¡å™¨",
                "usage_note": f"ä½¿ç”¨execute_sql(data_source='{target_database}')æˆ–query_external_database(database_name='{target_database}')æŸ¥è¯¢æ­¤æ•°æ®"
            },
            "import_config": import_config,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "import_method": "external_database"
            }
        }
        
        return f"âœ… {source_type.upper()}æ–‡ä»¶å·²å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“ {target_database}\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"å¤–éƒ¨æ•°æ®åº“å¯¼å…¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"å¯¼å…¥åˆ°å¤–éƒ¨æ•°æ®åº“å¤±è´¥: {str(e)}",
            "database_name": target_database,
            "error_type": type(e).__name__
        }
        return f"âŒ å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _connect_sqlite(config: dict, target_table: str = None) -> str:
    """è¿æ¥SQLiteæ•°æ®åº“"""
    try:
        file_path = config.get("file_path")
        if not file_path:
            result = {
                "status": "error",
                "message": "ç¼ºå°‘file_pathå‚æ•°",
                "required_params": ["file_path"]
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        sqlite_path = Path(file_path)
        if not sqlite_path.exists():
            result = {
                "status": "error",
                "message": f"SQLiteæ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                "file_path": file_path
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # åˆ›å»ºä¸´æ—¶é…ç½®
        temp_config_name = f"temp_sqlite_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        sqlite_config = {
            "type": "sqlite",
            "file_path": str(sqlite_path.absolute()),
            "enabled": True,
            "description": f"ä¸´æ—¶SQLiteè¿æ¥: {sqlite_path.name}",
            "_is_temporary": True,
            "_created_at": datetime.now().isoformat()
        }
        
        # æ·»åŠ ä¸´æ—¶é…ç½®
        if config_manager.add_database_config(temp_config_name, sqlite_config):
            try:
                # æµ‹è¯•è¿æ¥
                is_valid, message = database_manager.test_connection(temp_config_name)
                if not is_valid:
                    config_manager.remove_database_config(temp_config_name)
                    result = {
                        "status": "error",
                        "message": f"SQLiteè¿æ¥æµ‹è¯•å¤±è´¥: {message}",
                        "file_path": file_path
                    }
                    return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                # è·å–è¡¨åˆ—è¡¨
                tables = database_manager.get_table_list(temp_config_name)
                
                result = {
                    "status": "success",
                    "message": f"SQLiteæ•°æ®åº“è¿æ¥æˆåŠŸ: {sqlite_path.name}",
                    "data": {
                        "database_type": "sqlite",
                        "file_path": str(sqlite_path.absolute()),
                        "file_name": sqlite_path.name,
                        "file_size": f"{sqlite_path.stat().st_size / 1024:.2f} KB",
                        "tables": tables,
                        "table_count": len(tables),
                        "temp_config_name": temp_config_name,
                        "connection_type": "å¤–éƒ¨SQLiteæ•°æ®åº“è¿æ¥",
                        "data_location": str(sqlite_path.absolute()),
                        "usage_note": f"ä½¿ç”¨execute_sql(data_source='{temp_config_name}')æˆ–query_external_database(database_name='{temp_config_name}')æŸ¥è¯¢æ­¤æ•°æ®åº“"
                    },
                    "metadata": {
                        "timestamp": datetime.now().isoformat(),
                        "connection_method": "sqlite_file"
                    }
                }
                
                return f"âœ… SQLiteæ•°æ®åº“è¿æ¥æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
            except Exception as e:
                config_manager.remove_database_config(temp_config_name)
                raise e
        else:
            result = {
                "status": "error",
                "message": "åˆ›å»ºSQLiteä¸´æ—¶é…ç½®å¤±è´¥"
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"SQLiteæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"SQLiteæ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _connect_external_database(db_type: str, config: dict, target_table: str = None) -> str:
    """è¿æ¥å¤–éƒ¨æ•°æ®åº“ï¼ˆMySQLã€PostgreSQLã€MongoDBï¼‰"""
    try:
        # æµ‹è¯•è¿æ¥
        if isinstance(config, str):
            # å¦‚æœconfigæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ä½œä¸ºæ•°æ®åº“é…ç½®åç§°
            database_name = config
            is_valid, message = database_manager.test_connection(database_name)
            if not is_valid:
                result = {
                    "status": "error",
                    "message": f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {message}",
                    "database_name": database_name
                }
                return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            # è·å–è¡¨åˆ—è¡¨
            tables = database_manager.get_table_list(database_name)
            
            result = {
                "status": "success",
                "message": f"{db_type.upper()} å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸï¼ˆæœªå¯¼å…¥æ•°æ®ï¼‰",
                "data": {
                    "database_name": database_name,
                    "database_type": db_type,
                    "tables": tables,
                    "table_count": len(tables),
                    "connection_type": "å¤–éƒ¨æ•°æ®åº“è¿æ¥",
                    "data_location": "è¿œç¨‹æ•°æ®åº“æœåŠ¡å™¨",
                    "usage_note": f"ä½¿ç”¨execute_sql(data_source='{database_name}')æˆ–query_external_database(database_name='{database_name}')æŸ¥è¯¢æ­¤æ•°æ®åº“"
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "connection_method": "config_name"
                }
            }
            
            return f"âœ… {db_type.upper()} å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        else:
            # ç›´æ¥è¿æ¥é…ç½®
            # åˆ›å»ºæŒä¹…åŒ–ä¸´æ—¶é…ç½®å¹¶æµ‹è¯•è¿æ¥
            temp_config_name = f"temp_{db_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            config_with_type = config.copy()
            config_with_type["type"] = db_type
            config_with_type["enabled"] = True
            config_with_type["description"] = f"ä¸´æ—¶{db_type.upper()}è¿æ¥é…ç½®"
            config_with_type["_is_temporary"] = True
            config_with_type["_created_at"] = datetime.now().isoformat()
            
            # æ·»åŠ ä¸´æ—¶é…ç½®ï¼ˆä¸ç«‹å³åˆ é™¤ï¼‰
            if config_manager.add_database_config(temp_config_name, config_with_type):
                try:
                    # æµ‹è¯•è¿æ¥
                    is_valid, message = database_manager.test_connection(temp_config_name)
                    if not is_valid:
                        # è¿æ¥å¤±è´¥æ—¶æ¸…ç†ä¸´æ—¶é…ç½®
                        config_manager.remove_database_config(temp_config_name)
                        result = {
                            "status": "error",
                            "message": f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {message}"
                        }
                        return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                    # è·å–è¡¨åˆ—è¡¨
                    tables = database_manager.get_table_list(temp_config_name)
                    
                    result = {
                        "status": "success",
                        "message": f"{db_type.upper()} å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸï¼ˆæœªå¯¼å…¥æ•°æ®ï¼‰",
                        "data": {
                            "database_type": db_type,
                            "host": config.get("host", "N/A"),
                            "database": config.get("database", "N/A"),
                            "tables": tables,
                            "table_count": len(tables),
                            "temp_config_name": temp_config_name,
                            "connection_type": "å¤–éƒ¨æ•°æ®åº“è¿æ¥",
                            "data_location": "è¿œç¨‹æ•°æ®åº“æœåŠ¡å™¨",
                            "usage_note": f"ä½¿ç”¨execute_sql(data_source='{temp_config_name}')æˆ–query_external_database(database_name='{temp_config_name}')æŸ¥è¯¢æ­¤æ•°æ®åº“",
                            "config_lifecycle": "ä¸´æ—¶é…ç½®å·²ä¿å­˜ï¼Œå¯åœ¨ä¼šè¯æœŸé—´ä½¿ç”¨"
                        },
                        "metadata": {
                            "timestamp": datetime.now().isoformat(),
                            "connection_method": "direct_config",
                            "config_persistence": "temporary_persistent"
                        }
                    }
                    
                    return f"âœ… {db_type.upper()} å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                except Exception as e:
                    # å‘ç”Ÿå¼‚å¸¸æ—¶æ¸…ç†ä¸´æ—¶é…ç½®
                    config_manager.remove_database_config(temp_config_name)
                    raise e
            else:
                result = {
                    "status": "error",
                    "message": "åˆ›å»ºä¸´æ—¶é…ç½®å¤±è´¥"
                }
                return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
    except Exception as e:
        logger.error(f"{db_type} æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"{db_type.upper()} æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _connect_from_config(config: dict, target_table: str = None) -> str:
    """ä»é…ç½®æ–‡ä»¶è¿æ¥æ•°æ®åº“"""
    try:
        database_name = config.get("database_name")
        if not database_name:
            result = {
                "status": "error",
                "message": "ç¼ºå°‘database_nameå‚æ•°"
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·å–å¯ç”¨æ•°æ®åº“åˆ—è¡¨
        available_databases = database_manager.get_available_databases()
        
        if database_name not in available_databases:
            result = {
                "status": "error",
                "message": f"æ•°æ®åº“é…ç½®ä¸å­˜åœ¨: {database_name}",
                "available_databases": list(available_databases.keys())
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æµ‹è¯•è¿æ¥
        is_valid, message = database_manager.test_connection(database_name)
        if not is_valid:
            result = {
                "status": "error",
                "message": f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {message}",
                "database_name": database_name
            }
            return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = available_databases[database_name]
        tables = database_manager.get_table_list(database_name)
        
        result = {
            "status": "success",
            "message": f"å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸ: {database_name}ï¼ˆæœªå¯¼å…¥æ•°æ®ï¼‰",
            "data": {
                "database_name": database_name,
                "database_type": db_info.get("type"),
                "description": db_info.get("description", ""),
                "tables": tables,
                "table_count": len(tables),
                "connection_type": "å¤–éƒ¨æ•°æ®åº“è¿æ¥",
                "data_location": "è¿œç¨‹æ•°æ®åº“æœåŠ¡å™¨",
                "usage_note": f"ä½¿ç”¨execute_sql(data_source='{database_name}')æˆ–query_external_database(database_name='{database_name}')æŸ¥è¯¢æ­¤æ•°æ®åº“"
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "connection_method": "config_file"
            }
        }
        
        return f"âœ… å¤–éƒ¨æ•°æ®åº“è¿æ¥æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶è¿æ¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"é…ç½®æ–‡ä»¶è¿æ¥å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _preprocess_sql(query: str) -> str:
    """é¢„å¤„ç†SQLè¯­å¥"""
    # ç§»é™¤æœ«å°¾åˆ†å·å’Œå¤šä½™ç©ºæ ¼
    query = query.strip().rstrip(';').strip()
    return query

def _format_sql_error(error: Exception, query: str) -> dict:
    """æ ¼å¼åŒ–SQLé”™è¯¯ä¿¡æ¯"""
    error_msg = str(error)
    suggestions = []
    
    if "syntax error" in error_msg.lower():
        if "-" in query and "near \"-\"" in error_msg:
            suggestions.append("è¡¨åæˆ–åˆ—ååŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œè¯·ä½¿ç”¨åŒå¼•å·åŒ…å›´ï¼Œå¦‚: \"table-name\"")
        suggestions.append("æ£€æŸ¥SQLè¯­æ³•æ˜¯å¦æ­£ç¡®")
        suggestions.append("ç¡®ä¿è¡¨åå’Œåˆ—åå­˜åœ¨")
    elif "no such table" in error_msg.lower():
        suggestions.append("æ£€æŸ¥è¡¨åæ˜¯å¦æ­£ç¡®")
        suggestions.append("ä½¿ç”¨ get_data_info å·¥å…·æŸ¥çœ‹å¯ç”¨çš„è¡¨")
    elif "no such column" in error_msg.lower():
        suggestions.append("æ£€æŸ¥åˆ—åæ˜¯å¦æ­£ç¡®")
        suggestions.append("ä½¿ç”¨ get_data_info å·¥å…·æŸ¥çœ‹è¡¨ç»“æ„")
    elif "only execute one statement" in error_msg.lower():
        suggestions.append("ç§»é™¤SQLè¯­å¥æœ«å°¾çš„åˆ†å·")
        suggestions.append("ä¸€æ¬¡åªèƒ½æ‰§è¡Œä¸€æ¡SQLè¯­å¥")
    
    return {
        "status": "error",
        "message": f"SQLæ‰§è¡Œå¤±è´¥: {error_msg}",
        "error_type": type(error).__name__,
        "suggestions": suggestions,
        "query": query
    }

@mcp.tool()
def execute_sql(
    query: str,
    params: dict = None,
    limit: int = 1000,
    data_source: str = None
) -> str:
    """
    ğŸ“Š SQLæ‰§è¡Œå·¥å…· - æœ¬åœ°æ•°æ®åº“æŸ¥è¯¢ä¸“ç”¨
    
    ğŸ¯ ä½¿ç”¨åœºæ™¯ï¼š
    - æŸ¥è¯¢æœ¬åœ°SQLiteæ•°æ®åº“ï¼ˆé»˜è®¤ï¼‰
    - æŸ¥è¯¢å·²å¯¼å…¥çš„Excel/CSVæ•°æ®
    - æŸ¥è¯¢æŒ‡å®šçš„æœ¬åœ°æ•°æ®æº
    
    âš ï¸ é‡è¦åŒºåˆ«ï¼š
    - æœ¬åœ°æ•°æ®æŸ¥è¯¢ â†’ ä½¿ç”¨æ­¤å·¥å…· (execute_sql)
    - å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢ â†’ ä½¿ç”¨ query_external_database
    
    ğŸ”’ å®‰å…¨ç‰¹æ€§ï¼š
    - è‡ªåŠ¨æ·»åŠ LIMITé™åˆ¶é˜²æ­¢å¤§é‡æ•°æ®è¿”å›
    - æ”¯æŒå‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥
    - åªå…è®¸SELECTæŸ¥è¯¢ï¼Œæ‹’ç»å±é™©æ“ä½œ
    
    Args:
        query: SQLæŸ¥è¯¢è¯­å¥ï¼ˆæ¨èä½¿ç”¨SELECTè¯­å¥ï¼‰
        params: æŸ¥è¯¢å‚æ•°å­—å…¸ï¼Œç”¨äºå‚æ•°åŒ–æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
        limit: ç»“æœè¡Œæ•°é™åˆ¶ï¼Œé»˜è®¤1000è¡Œï¼ˆå¯é€‰ï¼‰
        data_source: æ•°æ®æºåç§°ï¼Œé»˜è®¤æœ¬åœ°SQLiteï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: JSONæ ¼å¼æŸ¥è¯¢ç»“æœï¼ŒåŒ…å«åˆ—åã€æ•°æ®è¡Œå’Œç»Ÿè®¡ä¿¡æ¯
    
    ğŸ’¡ AIä½¿ç”¨æç¤ºï¼š
    - æŸ¥è¯¢æœ¬åœ°æ•°æ®æ—¶ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·
    - æŸ¥è¯¢å¤–éƒ¨æ•°æ®åº“æ—¶ä½¿ç”¨ query_external_database
    - ä½¿ç”¨ get_data_info å…ˆäº†è§£è¡¨ç»“æ„
    """
    try:
        # é¢„å¤„ç†SQLè¯­å¥
        query = _preprocess_sql(query)
        
        # æ·»åŠ LIMITé™åˆ¶
        if "LIMIT" not in query.upper() and "SELECT" in query.upper():
            query = f"{query} LIMIT {limit}"
        
        # æ ¹æ®æ•°æ®æºé€‰æ‹©è¿æ¥æ–¹å¼
        if data_source:
            # ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“è¿æ¥
            try:
                result_data = database_manager.execute_query(data_source, query, params)
                
                if result_data["success"]:
                    result = {
                        "status": "success",
                        "message": f"æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {result_data.get('row_count', len(result_data['data']))} æ¡è®°å½•",
                        "data": {
                            "columns": list(result_data['data'][0].keys()) if result_data['data'] else [],
                            "rows": result_data['data'],
                            "row_count": result_data.get('row_count', len(result_data['data']))
                        },
                        "metadata": {
                            "query": query,
                            "data_source": data_source,
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    return f"âœ… SQLæ‰§è¡ŒæˆåŠŸï¼ˆæ•°æ®æº: {data_source}ï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                else:
                    result = {
                        "status": "error",
                        "message": f"æŸ¥è¯¢å¤±è´¥: {result_data['error']}",
                        "data_source": data_source,
                        "timestamp": datetime.now().isoformat()
                    }
                    return f"âŒ SQLæ‰§è¡Œå¤±è´¥ï¼ˆæ•°æ®æº: {data_source}ï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            except Exception as e:
                result = {
                    "status": "error",
                    "message": f"è¿æ¥æ•°æ®æºå¤±è´¥: {str(e)}",
                    "data_source": data_source,
                    "timestamp": datetime.now().isoformat()
                }
                return f"âŒ æ•°æ®æºè¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        else:
            # ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“
            with get_db_connection() as conn:
                if params:
                    cursor = conn.execute(query, params)
                else:
                    cursor = conn.execute(query)
                
                # è·å–ç»“æœ
                columns = [description[0] for description in cursor.description] if cursor.description else []
                rows = cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                data = [dict(zip(columns, row)) for row in rows] if columns else []
                
                result = {
                    "status": "success",
                    "message": f"æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(data)} æ¡è®°å½•",
                    "data": {
                        "columns": columns,
                        "rows": data,
                        "row_count": len(data)
                    },
                    "metadata": {
                        "query": query,
                        "data_source": "æœ¬åœ°SQLite",
                        "timestamp": datetime.now().isoformat()
                    }
                }
                
                return f"âœ… SQLæ‰§è¡ŒæˆåŠŸï¼ˆæ•°æ®æº: æœ¬åœ°SQLiteï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {e}")
        result = _format_sql_error(e, query)
        result["timestamp"] = datetime.now().isoformat()
        result["data_source"] = data_source or "æœ¬åœ°SQLite"
        return f"âŒ SQLæ‰§è¡Œå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def get_data_info(
    info_type: str = "tables",
    table_name: str = None,
    data_source: str = None
) -> str:
    """
    ğŸ“Š æ•°æ®ä¿¡æ¯è·å–å·¥å…· - æŸ¥çœ‹æ•°æ®åº“ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - è·å–æ•°æ®åº“è¡¨åˆ—è¡¨ã€è¡¨ç»“æ„ã€æ•°æ®ç»Ÿè®¡ç­‰ä¿¡æ¯
    - æ”¯æŒæœ¬åœ°SQLiteå’Œå¤–éƒ¨æ•°æ®åº“
    - æä¾›è¯¦ç»†çš„è¡¨ç»“æ„å’Œæ•°æ®æ¦‚è§ˆ
    - æ™ºèƒ½æ•°æ®åº“æ¸…ç†ç®¡ç†åŠŸèƒ½
    
    Args:
        info_type: ä¿¡æ¯ç±»å‹
            - "tables": è·å–æ‰€æœ‰è¡¨/é›†åˆåˆ—è¡¨ï¼ˆé»˜è®¤ï¼‰
            - "schema": è·å–æŒ‡å®šè¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆéœ€è¦table_nameï¼‰
            - "stats": è·å–æŒ‡å®šè¡¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆéœ€è¦table_nameï¼‰
            - "cleanup": æ™ºèƒ½æ£€æµ‹è¿‡æ—¶æ•°æ®å’Œè¡¨ï¼Œæä¾›æ¸…ç†å»ºè®®
        table_name: è¡¨åï¼ˆå½“info_typeä¸ºschemaæˆ–statsæ—¶å¿…éœ€ï¼‰
        data_source: æ•°æ®æºåç§°
            - None: ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“ï¼ˆé»˜è®¤ï¼‰
            - é…ç½®åç§°: ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“ï¼ˆéœ€å…ˆé€šè¿‡manage_database_configåˆ›å»ºé…ç½®ï¼‰
    
    Returns:
        str: JSONæ ¼å¼çš„æ•°æ®åº“ä¿¡æ¯ï¼ŒåŒ…å«çŠ¶æ€ã€æ•°æ®å’Œå…ƒæ•°æ®
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. æ•°æ®æ¢ç´¢ï¼šå…ˆç”¨info_type="tables"æŸ¥çœ‹æ‰€æœ‰è¡¨
    2. ç»“æ„åˆ†æï¼šç”¨info_type="schema"äº†è§£è¡¨ç»“æ„
    3. æ•°æ®æ¦‚è§ˆï¼šç”¨info_type="stats"è·å–ç»Ÿè®¡ä¿¡æ¯
    4. æ•°æ®åº“ç»´æŠ¤ï¼šç”¨info_type="cleanup"æ£€æµ‹å¹¶æ¸…ç†è¿‡æ—¶æ•°æ®
    5. å¤–éƒ¨æ•°æ®åº“ï¼šç¡®ä¿data_sourceé…ç½®å·²å­˜åœ¨
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - åœ¨æŸ¥è¯¢æ•°æ®å‰å…ˆäº†è§£è¡¨ç»“æ„
    - ä½¿ç”¨statsäº†è§£æ•°æ®åˆ†å¸ƒå’Œè´¨é‡
    - å®šæœŸä½¿ç”¨cleanupåŠŸèƒ½ç»´æŠ¤æ•°æ®åº“æ•´æ´
    - ç»“åˆanalyze_dataå·¥å…·è¿›è¡Œæ·±åº¦åˆ†æ
    
    âš ï¸ å¸¸è§é”™è¯¯é¿å…ï¼š
    - schemaå’Œstatså¿…é¡»æŒ‡å®štable_name
    - å¤–éƒ¨æ•°æ®åº“éœ€è¦æœ‰æ•ˆçš„data_sourceé…ç½®
    - è¡¨ååŒºåˆ†å¤§å°å†™
    - cleanupåŠŸèƒ½ä»…é€‚ç”¨äºæœ¬åœ°SQLiteæ•°æ®åº“
    
    ğŸ“ˆ é«˜æ•ˆä½¿ç”¨æµç¨‹ï¼š
    1. get_data_info(info_type="tables") â†’ æŸ¥çœ‹æ‰€æœ‰è¡¨
    2. get_data_info(info_type="schema", table_name="è¡¨å") â†’ äº†è§£ç»“æ„
    3. get_data_info(info_type="stats", table_name="è¡¨å") â†’ æŸ¥çœ‹ç»Ÿè®¡
    4. get_data_info(info_type="cleanup") â†’ æ£€æµ‹è¿‡æ—¶æ•°æ®
    5. analyze_data() â†’ æ·±åº¦åˆ†æ
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - è¿™æ˜¯æ•°æ®æ¢ç´¢çš„ç¬¬ä¸€æ­¥å·¥å…·
    - ä¸ºåç»­åˆ†ææä¾›åŸºç¡€ä¿¡æ¯
    - æ”¯æŒæœ¬åœ°å’Œè¿œç¨‹æ•°æ®æº
    - æ™ºèƒ½ç»´æŠ¤æ•°æ®åº“æ•´æ´æ€§
    
    ğŸ§¹ æ•°æ®åº“æ¸…ç†åŠŸèƒ½ï¼ˆinfo_type="cleanup"ï¼‰ï¼š
    - è‡ªåŠ¨æ£€æµ‹æµ‹è¯•è¡¨ã€ä¸´æ—¶è¡¨ã€è¿‡æ—¶è¡¨
    - è¯†åˆ«ç©ºè¡¨å’Œé‡å¤è¡¨
    - åˆ†æè¡¨çš„åˆ›å»ºæ—¶é—´å’Œæœ€åè®¿é—®æ—¶é—´
    - æä¾›æ™ºèƒ½æ¸…ç†å»ºè®®ï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦æ‰§è¡Œæ¸…ç†
    - æ”¯æŒæ‰¹é‡æ¸…ç†å’Œé€‰æ‹©æ€§æ¸…ç†
    """
    try:
        if data_source:
            # ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“è¿æ¥
            try:
                if info_type == "tables":
                    tables = database_manager.get_table_list(data_source)
                    
                    result = {
                        "status": "success",
                        "message": f"æ‰¾åˆ° {len(tables)} ä¸ªè¡¨/é›†åˆ",
                        "data": {
                            "tables": [{
                                "table_name": table,
                                "row_count": "N/A"  # å¤–éƒ¨æ•°æ®åº“æš‚ä¸ç»Ÿè®¡è¡Œæ•°
                            } for table in tables],
                            "table_count": len(tables)
                        },
                        "metadata": {
                            "data_source": data_source,
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"âœ… è¡¨ä¿¡æ¯è·å–æˆåŠŸï¼ˆæ•°æ®æº: {data_source}ï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                elif info_type == "schema" and table_name:
                    schema = database_manager.get_table_schema(data_source, table_name)
                    
                    result = {
                        "status": "success",
                        "message": f"è¡¨/é›†åˆ '{table_name}' ç»“æ„ä¿¡æ¯",
                        "data": {
                            "table_name": table_name,
                            "schema": schema,
                            "column_count": len(schema) if isinstance(schema, list) else "N/A"
                        },
                        "metadata": {
                            "data_source": data_source,
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"âœ… è¡¨ç»“æ„è·å–æˆåŠŸï¼ˆæ•°æ®æº: {data_source}ï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                else:
                    result = {
                        "status": "error",
                        "message": "å¤–éƒ¨æ•°æ®æºæš‚ä¸æ”¯æŒstatså’Œcleanupä¿¡æ¯ç±»å‹æˆ–ç¼ºå°‘å¿…è¦å‚æ•°",
                        "supported_types": ["tables", "schema"],
                        "data_source": data_source,
                        "note": "cleanupåŠŸèƒ½ä»…é€‚ç”¨äºæœ¬åœ°SQLiteæ•°æ®åº“"
                    }
                    return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
            except Exception as e:
                result = {
                    "status": "error",
                    "message": f"è¿æ¥æ•°æ®æºå¤±è´¥: {str(e)}",
                    "data_source": data_source,
                    "timestamp": datetime.now().isoformat()
                }
                return f"âŒ æ•°æ®æºè¿æ¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        else:
            # ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“
            with get_db_connection() as conn:
                if info_type == "tables":
                    # è·å–æ‰€æœ‰è¡¨åï¼ˆæ’é™¤å…ƒæ•°æ®è¡¨ï¼‰
                    cursor = conn.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name != '_metadata'"
                    )
                    tables = [row[0] for row in cursor.fetchall()]
                    
                    # è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯
                    table_info = []
                    for table in tables:
                        try:
                            escaped_table = _escape_identifier(table)
                            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                            row_count = cursor.fetchone()[0]
                            table_info.append({
                                "table_name": table,
                                "row_count": row_count
                            })
                        except Exception as e:
                            logger.warning(f"æ— æ³•è·å–è¡¨ '{table}' çš„è¡Œæ•°: {e}")
                            table_info.append({
                                "table_name": table,
                                "row_count": "N/A",
                                "error": str(e)
                            })
                    
                    result = {
                        "status": "success",
                        "message": f"æ‰¾åˆ° {len(tables)} ä¸ªè¡¨",
                        "data": {
                            "tables": table_info,
                            "table_count": len(tables)
                        },
                        "metadata": {
                            "data_source": "æœ¬åœ°SQLite",
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"âœ… è¡¨ä¿¡æ¯è·å–æˆåŠŸï¼ˆæ•°æ®æº: æœ¬åœ°SQLiteï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                elif info_type == "schema" and table_name:
                    # è·å–è¡¨ç»“æ„
                    escaped_table = _escape_identifier(table_name)
                    cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                    columns = cursor.fetchall()
                    
                    schema = []
                    for col in columns:
                        schema.append({
                            "column_name": col[1],
                            "data_type": col[2],
                            "not_null": bool(col[3]),
                            "default_value": col[4],
                            "primary_key": bool(col[5])
                        })
                    
                    result = {
                        "status": "success",
                        "message": f"è¡¨ '{table_name}' ç»“æ„ä¿¡æ¯",
                        "data": {
                            "table_name": table_name,
                            "columns": schema,
                            "column_count": len(schema)
                        },
                        "metadata": {
                            "data_source": "æœ¬åœ°SQLite",
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"âœ… è¡¨ç»“æ„è·å–æˆåŠŸï¼ˆæ•°æ®æº: æœ¬åœ°SQLiteï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                elif info_type == "stats" and table_name:
                    # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
                    escaped_table = _escape_identifier(table_name)
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                    row_count = cursor.fetchone()[0]
                    
                    result = {
                        "status": "success",
                        "message": f"è¡¨ '{table_name}' ç»Ÿè®¡ä¿¡æ¯",
                        "data": {
                            "table_name": table_name,
                            "row_count": row_count
                        },
                        "metadata": {
                            "data_source": "æœ¬åœ°SQLite",
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"âœ… ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸï¼ˆæ•°æ®æº: æœ¬åœ°SQLiteï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                elif info_type == "cleanup":
                    # æ™ºèƒ½æ•°æ®åº“æ¸…ç†åŠŸèƒ½
                    cleanup_result = _analyze_database_cleanup(conn)
                    
                    result = {
                        "status": "success",
                        "message": "æ•°æ®åº“æ¸…ç†åˆ†æå®Œæˆ",
                        "data": cleanup_result,
                        "metadata": {
                            "data_source": "æœ¬åœ°SQLite",
                            "timestamp": datetime.now().isoformat()
                        }
                    }
                    
                    return f"ğŸ§¹ æ•°æ®åº“æ¸…ç†åˆ†æå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                elif info_type == "api_storage":
                    # APIå­˜å‚¨æ•°æ®æ¦‚è§ˆ - è§£å†³APIæ•°æ®å­˜å‚¨ä½ç½®ä¸é€æ˜é—®é¢˜
                    try:
                        from config.api_data_storage import api_data_storage
                        
                        # è·å–æ‰€æœ‰APIå­˜å‚¨ä¼šè¯
                        success, sessions, message = api_data_storage.list_storage_sessions()
                        
                        if not success:
                            result = {
                                "status": "error",
                                "message": f"è·å–APIå­˜å‚¨ä¿¡æ¯å¤±è´¥: {message}",
                                "data_source": "APIå­˜å‚¨"
                            }
                            return f"âŒ APIå­˜å‚¨ä¿¡æ¯è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                        
                        # ç»Ÿè®¡APIå­˜å‚¨ä¿¡æ¯
                        total_sessions = len(sessions)
                        total_records = sum(session.get('total_records', 0) for session in sessions)
                        api_names = list(set(session['api_name'] for session in sessions))
                        endpoint_names = list(set(session['endpoint_name'] for session in sessions))
                        
                        # æŒ‰APIåˆ†ç»„ç»Ÿè®¡
                        api_stats = {}
                        for session in sessions:
                            api_name = session['api_name']
                            if api_name not in api_stats:
                                api_stats[api_name] = {
                                    "sessions": 0,
                                    "total_records": 0,
                                    "endpoints": set()
                                }
                            api_stats[api_name]["sessions"] += 1
                            api_stats[api_name]["total_records"] += session.get('total_records', 0)
                            api_stats[api_name]["endpoints"].add(session['endpoint_name'])
                        
                        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                        api_summary = []
                        for api_name, stats in api_stats.items():
                            api_summary.append({
                                "api_name": api_name,
                                "sessions": stats["sessions"],
                                "total_records": stats["total_records"],
                                "endpoints": list(stats["endpoints"])
                            })
                        
                        result = {
                            "status": "success",
                            "message": f"æ‰¾åˆ° {total_sessions} ä¸ªAPIå­˜å‚¨ä¼šè¯ï¼Œå…± {total_records} æ¡è®°å½•",
                            "data": {
                                "summary": {
                                    "total_sessions": total_sessions,
                                    "total_records": total_records,
                                    "unique_apis": len(api_names),
                                    "unique_endpoints": len(endpoint_names)
                                },
                                "api_breakdown": api_summary,
                                "recent_sessions": sessions[:5]  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªä¼šè¯
                            },
                            "storage_info": {
                                "storage_type": "api_storage",
                                "storage_directory": "data/api_storage",
                                "description": "APIæ•°æ®å­˜å‚¨åœ¨ç‹¬ç«‹çš„SQLiteæ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªä¼šè¯å¯¹åº”ä¸€ä¸ªæ–‡ä»¶"
                            },
                            "metadata": {
                                "data_source": "APIå­˜å‚¨",
                                "timestamp": datetime.now().isoformat()
                            },
                            "usage_tips": [
                                "ä½¿ç”¨ query_api_storage_data() æŸ¥çœ‹æ‰€æœ‰APIå­˜å‚¨ä¼šè¯",
                                "ä½¿ç”¨ query_api_storage_data(session_id='xxx') æŸ¥è¯¢ç‰¹å®šä¼šè¯æ•°æ®",
                                "APIæ•°æ®ä¸åœ¨ä¸»æ•°æ®åº“ä¸­ï¼Œè€Œæ˜¯å­˜å‚¨åœ¨ç‹¬ç«‹æ–‡ä»¶ä¸­",
                                "æ¯ä¸ªAPIè°ƒç”¨ä¼šè‡ªåŠ¨åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰çš„å­˜å‚¨ä¼šè¯"
                            ]
                        }
                        
                        return f"ğŸ“Š APIå­˜å‚¨ä¿¡æ¯æ¦‚è§ˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                        
                    except Exception as e:
                        logger.error(f"è·å–APIå­˜å‚¨ä¿¡æ¯å¤±è´¥: {e}")
                        result = {
                            "status": "error",
                            "message": f"è·å–APIå­˜å‚¨ä¿¡æ¯å¤±è´¥: {str(e)}",
                            "error_type": type(e).__name__,
                            "data_source": "APIå­˜å‚¨"
                        }
                        return f"âŒ APIå­˜å‚¨ä¿¡æ¯è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                else:
                    result = {
                        "status": "error",
                        "message": "æ— æ•ˆçš„ä¿¡æ¯ç±»å‹æˆ–ç¼ºå°‘å¿…è¦å‚æ•°",
                        "supported_types": ["tables", "schema", "stats", "cleanup", "api_storage"],
                        "data_source": "æœ¬åœ°SQLite"
                    }
                    return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ ä¿¡æ¯è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# ================================
# è¾…åŠ©å‡½æ•°ï¼šæ•°æ®åˆ†æç›¸å…³
# ================================

def _table_exists(table_name: str) -> bool:
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    try:
        with get_db_connection() as conn:
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                (table_name,)
            )
            return cursor.fetchone() is not None
    except Exception:
        return False

def _calculate_basic_stats(table_name: str, columns: list, options: dict) -> dict:
    """è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ - æ™ºèƒ½å¤„ç†æ•°å€¼å’Œæ–‡æœ¬åˆ—"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–åˆ—ä¿¡æ¯
            if columns:
                target_columns = columns
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                target_columns = [col[1] for col in cursor.fetchall()]
            
            if not target_columns:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„åˆ—"}
            
            # åˆ†ææ¯ä¸€åˆ—
            stats_result = {}
            numeric_columns = []
            text_columns = []
            
            for col in target_columns:
                escaped_col = _escape_identifier(col)
                
                # æ£€æµ‹åˆ—ç±»å‹
                cursor = conn.execute(f"SELECT typeof({escaped_col}) FROM {escaped_table} WHERE {escaped_col} IS NOT NULL LIMIT 1")
                result = cursor.fetchone()
                col_type = result[0] if result else 'null'
                
                # è·å–åŸºæœ¬ä¿¡æ¯
                cursor = conn.execute(f"""
                    SELECT 
                        COUNT(*) as total_count,
                        COUNT({escaped_col}) as non_null_count,
                        COUNT(CASE WHEN {escaped_col} IS NULL THEN 1 END) as null_count
                    FROM {escaped_table}
                """)
                basic_info = cursor.fetchone()
                
                if col_type in ['integer', 'real']:
                    # æ•°å€¼åˆ—ç»Ÿè®¡
                    numeric_columns.append(col)
                    cursor = conn.execute(f"""
                        SELECT 
                            AVG({escaped_col}) as mean,
                            MIN({escaped_col}) as min_val,
                            MAX({escaped_col}) as max_val
                        FROM {escaped_table}
                        WHERE {escaped_col} IS NOT NULL
                    """)
                    numeric_stats = cursor.fetchone()
                    
                    # è®¡ç®—ä¸­ä½æ•°å’Œæ ‡å‡†å·®
                    cursor = conn.execute(f"SELECT {escaped_col} FROM {escaped_table} WHERE {escaped_col} IS NOT NULL ORDER BY {escaped_col}")
                    values = [row[0] for row in cursor.fetchall()]
                    
                    if values:
                        median = np.median(values)
                        std_dev = np.std(values)
                        q25 = np.percentile(values, 25)
                        q75 = np.percentile(values, 75)
                    else:
                        median = std_dev = q25 = q75 = None
                    
                    stats_result[col] = {
                        "column_type": "numeric",
                        "data_type": col_type,
                        "total_count": basic_info[0],
                        "non_null_count": basic_info[1],
                        "null_count": basic_info[2],
                        "null_percentage": round((basic_info[2] / basic_info[0]) * 100, 2) if basic_info[0] > 0 else 0,
                        "mean": round(numeric_stats[0], 4) if numeric_stats[0] else None,
                        "median": round(median, 4) if median is not None else None,
                        "std_dev": round(std_dev, 4) if std_dev is not None else None,
                        "min": numeric_stats[1],
                        "max": numeric_stats[2],
                        "q25": round(q25, 4) if q25 is not None else None,
                        "q75": round(q75, 4) if q75 is not None else None
                    }
                    
                else:
                    # æ–‡æœ¬åˆ—ç»Ÿè®¡
                    text_columns.append(col)
                    
                    # è·å–å”¯ä¸€å€¼æ•°é‡
                    cursor = conn.execute(f"SELECT COUNT(DISTINCT {escaped_col}) FROM {escaped_table} WHERE {escaped_col} IS NOT NULL")
                    unique_count = cursor.fetchone()[0]
                    
                    # è·å–æœ€å¸¸è§çš„å€¼ï¼ˆå‰5ä¸ªï¼‰
                    cursor = conn.execute(f"""
                        SELECT {escaped_col}, COUNT(*) as freq 
                        FROM {escaped_table} 
                        WHERE {escaped_col} IS NOT NULL 
                        GROUP BY {escaped_col} 
                        ORDER BY freq DESC 
                        LIMIT 5
                    """)
                    top_values = cursor.fetchall()
                    
                    # è®¡ç®—å­—ç¬¦ä¸²é•¿åº¦ç»Ÿè®¡ï¼ˆå¦‚æœæ˜¯æ–‡æœ¬ï¼‰
                    length_stats = None
                    if col_type == 'text':
                        cursor = conn.execute(f"""
                            SELECT 
                                AVG(LENGTH({escaped_col})) as avg_length,
                                MIN(LENGTH({escaped_col})) as min_length,
                                MAX(LENGTH({escaped_col})) as max_length
                            FROM {escaped_table}
                            WHERE {escaped_col} IS NOT NULL
                        """)
                        length_result = cursor.fetchone()
                        if length_result[0] is not None:
                            length_stats = {
                                "avg_length": round(length_result[0], 2),
                                "min_length": length_result[1],
                                "max_length": length_result[2]
                            }
                    
                    stats_result[col] = {
                        "column_type": "categorical",
                        "data_type": col_type,
                        "total_count": basic_info[0],
                        "non_null_count": basic_info[1],
                        "null_count": basic_info[2],
                        "null_percentage": round((basic_info[2] / basic_info[0]) * 100, 2) if basic_info[0] > 0 else 0,
                        "unique_count": unique_count,
                        "unique_percentage": round((unique_count / basic_info[1]) * 100, 2) if basic_info[1] > 0 else 0,
                        "top_values": [{
                            "value": str(val[0]),
                            "frequency": val[1],
                            "percentage": round((val[1] / basic_info[1]) * 100, 2) if basic_info[1] > 0 else 0
                        } for val in top_values],
                        "length_stats": length_stats
                    }
            
            # æ·»åŠ æ±‡æ€»ä¿¡æ¯
            summary = {
                "total_columns": len(target_columns),
                "numeric_columns": len(numeric_columns),
                "categorical_columns": len(text_columns),
                "numeric_column_names": numeric_columns,
                "categorical_column_names": text_columns
            }
            
            return {
                "column_stats": stats_result,
                "summary": summary
            }
            
    except Exception as e:
        return {"error": f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"}

def _calculate_correlation(table_name: str, columns: list, options: dict) -> dict:
    """è®¡ç®—ç›¸å…³ç³»æ•°"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–æ•°å€¼åˆ—
            if columns and len(columns) >= 2:
                numeric_columns = columns[:10]  # é™åˆ¶æœ€å¤š10åˆ—
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                all_columns = cursor.fetchall()
                numeric_columns = [col[1] for col in all_columns if col[2] in ['INTEGER', 'REAL', 'NUMERIC']][:10]
            
            if len(numeric_columns) < 2:
                return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼åˆ—æ¥è®¡ç®—ç›¸å…³æ€§"}
            
            # è·å–æ•°æ®
            escaped_columns = [_escape_identifier(col) for col in numeric_columns]
            columns_str = ", ".join(escaped_columns)
            df = pd.read_sql(f"SELECT {columns_str} FROM {escaped_table}", conn)
            
            # é‡å‘½ååˆ—ä¸ºåŸå§‹åç§°ï¼ˆå»æ‰è½¬ä¹‰ç¬¦å·ï¼‰
            df.columns = numeric_columns
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            correlation_matrix = df.corr().round(4)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            result = {}
            for i, col1 in enumerate(numeric_columns):
                result[col1] = {}
                for j, col2 in enumerate(numeric_columns):
                    result[col1][col2] = correlation_matrix.iloc[i, j]
            
            return {
                "correlation_matrix": result,
                "columns": numeric_columns,
                "method": "pearson"
            }
            
    except Exception as e:
        return {"error": f"è®¡ç®—ç›¸å…³æ€§å¤±è´¥: {str(e)}"}

def _detect_outliers(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æµ‹å¼‚å¸¸å€¼"""
    try:
        method = options.get("method", "iqr")  # iqr æˆ– zscore
        threshold = options.get("threshold", 3)  # Z-scoreé˜ˆå€¼
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            if columns:
                numeric_columns = columns[:5]  # é™åˆ¶æœ€å¤š5åˆ—
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                all_columns = cursor.fetchall()
                numeric_columns = [col[1] for col in all_columns if col[2] in ['INTEGER', 'REAL', 'NUMERIC']][:5]
            
            if not numeric_columns:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼ç±»å‹çš„åˆ—"}
            
            outliers_result = {}
            
            for col in numeric_columns:
                escaped_col = _escape_identifier(col)
                cursor = conn.execute(f"SELECT {escaped_col} FROM {escaped_table} WHERE {escaped_col} IS NOT NULL")
                values = [row[0] for row in cursor.fetchall()]
                
                if not values:
                    outliers_result[col] = {"outliers": [], "count": 0}
                    continue
                
                outliers = []
                
                if method == "iqr":
                    q25 = np.percentile(values, 25)
                    q75 = np.percentile(values, 75)
                    iqr = q75 - q25
                    lower_bound = q25 - 1.5 * iqr
                    upper_bound = q75 + 1.5 * iqr
                    outliers = [v for v in values if v < lower_bound or v > upper_bound]
                    
                elif method == "zscore":
                    z_scores = np.abs(stats.zscore(values))
                    outliers = [values[i] for i, z in enumerate(z_scores) if z > threshold]
                
                outliers_result[col] = {
                    "outliers": outliers[:100],  # é™åˆ¶è¿”å›å‰100ä¸ªå¼‚å¸¸å€¼
                    "count": len(outliers),
                    "percentage": round((len(outliers) / len(values)) * 100, 2),
                    "method": method
                }
            
            return outliers_result
            
    except Exception as e:
        return {"error": f"å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {str(e)}"}

def _check_missing_values(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æŸ¥ç¼ºå¤±å€¼"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            if columns:
                target_columns = columns
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                target_columns = [col[1] for col in cursor.fetchall()]
            
            missing_result = {}
            total_rows = 0
            
            # è·å–æ€»è¡Œæ•°
            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
            total_rows = cursor.fetchone()[0]
            
            for col in target_columns:
                escaped_col = _escape_identifier(col)
                cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table} WHERE {escaped_col} IS NULL")
                null_count = cursor.fetchone()[0]
                
                missing_result[col] = {
                    "null_count": null_count,
                    "null_percentage": round((null_count / total_rows) * 100, 2) if total_rows > 0 else 0,
                    "non_null_count": total_rows - null_count
                }
            
            return {
                "missing_values": missing_result,
                "total_rows": total_rows,
                "columns_analyzed": len(target_columns)
            }
            
    except Exception as e:
        return {"error": f"ç¼ºå¤±å€¼æ£€æŸ¥å¤±è´¥: {str(e)}"}

def _escape_identifier(identifier: str) -> str:
    """è½¬ä¹‰SQLæ ‡è¯†ç¬¦ï¼ˆè¡¨åã€åˆ—åï¼‰ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦"""
    # ç§»é™¤å¯èƒ½çš„å¼•å·
    identifier = identifier.strip('"').strip("'")
    # ç”¨åŒå¼•å·åŒ…å›´ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
    return f'"{identifier}"'

def _check_duplicates(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æŸ¥é‡å¤å€¼"""
    try:
        with get_db_connection() as conn:
            # è½¬ä¹‰è¡¨å
            escaped_table = _escape_identifier(table_name)
            
            if columns:
                # è½¬ä¹‰åˆ—å
                escaped_columns = [_escape_identifier(col) for col in columns]
                columns_str = ", ".join(escaped_columns)
                group_by_str = columns_str
            else:
                # æ£€æŸ¥æ‰€æœ‰åˆ—çš„é‡å¤
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                all_columns = [col[1] for col in cursor.fetchall()]
                # è½¬ä¹‰æ‰€æœ‰åˆ—å
                escaped_columns = [_escape_identifier(col) for col in all_columns]
                columns_str = ", ".join(escaped_columns)
                group_by_str = columns_str
            
            # æŸ¥æ‰¾é‡å¤è®°å½•
            cursor = conn.execute(f"""
                SELECT {columns_str}, COUNT(*) as duplicate_count
                FROM {escaped_table}
                GROUP BY {group_by_str}
                HAVING COUNT(*) > 1
                ORDER BY duplicate_count DESC
                LIMIT 100
            """)
            
            duplicates = cursor.fetchall()
            
            # è·å–æ€»é‡å¤è¡Œæ•°
            cursor = conn.execute(f"""
                SELECT SUM(cnt - 1) as total_duplicates
                FROM (
                    SELECT COUNT(*) as cnt
                    FROM {escaped_table}
                    GROUP BY {group_by_str}
                    HAVING COUNT(*) > 1
                )
            """)
            
            total_duplicates = cursor.fetchone()[0] or 0
            
            # è·å–æ€»è¡Œæ•°
            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
            total_rows = cursor.fetchone()[0]
            
            return {
                "duplicate_groups": len(duplicates),
                "total_duplicate_rows": total_duplicates,
                "duplicate_percentage": round((total_duplicates / total_rows) * 100, 2) if total_rows > 0 else 0,
                "sample_duplicates": [dict(zip([desc[0] for desc in cursor.description], row)) for row in duplicates[:10]],
                "total_rows": total_rows
            }
            
    except Exception as e:
        return {"error": f"é‡å¤å€¼æ£€æŸ¥å¤±è´¥: {str(e)}"}

@mcp.tool()
def analyze_data(
    analysis_type: str,
    table_name: str,
    columns: list = None,
    options: dict = None
) -> str:
    """
    ğŸ” æ•°æ®åˆ†æå·¥å…· - æ‰§è¡Œå„ç§ç»Ÿè®¡åˆ†æå’Œæ•°æ®è´¨é‡æ£€æŸ¥
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æä¾›5ç§æ ¸å¿ƒæ•°æ®åˆ†æåŠŸèƒ½
    - æ”¯æŒæŒ‡å®šåˆ—åˆ†ææˆ–å…¨è¡¨åˆ†æ
    - è‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼
    - è¿”å›è¯¦ç»†çš„åˆ†æç»“æœå’Œå¯è§†åŒ–å»ºè®®
    
    Args:
        analysis_type: åˆ†æç±»å‹
            - "basic_stats": åŸºç¡€ç»Ÿè®¡åˆ†æï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰ï¼‰
            - "correlation": ç›¸å…³æ€§åˆ†æï¼ˆæ•°å€¼åˆ—ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼‰
            - "outliers": å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRã€Z-scoreæ–¹æ³•ï¼‰
            - "missing_values": ç¼ºå¤±å€¼åˆ†æï¼ˆç¼ºå¤±ç‡ã€åˆ†å¸ƒæ¨¡å¼ï¼‰
            - "duplicates": é‡å¤å€¼æ£€æµ‹ï¼ˆå®Œå…¨é‡å¤ã€éƒ¨åˆ†é‡å¤ï¼‰
        table_name: è¦åˆ†æçš„æ•°æ®è¡¨å
        columns: åˆ†æçš„åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            - None: åˆ†ææ‰€æœ‰é€‚ç”¨åˆ—
            - ["col1", "col2"]: åªåˆ†ææŒ‡å®šåˆ—
        options: åˆ†æé€‰é¡¹ï¼ˆå¯é€‰å­—å…¸ï¼‰
            - outliers: {"method": "iqr|zscore", "threshold": 1.5}
            - correlation: {"method": "pearson|spearman"}
            - basic_stats: {"percentiles": [25, 50, 75, 90, 95]}
    
    Returns:
        str: JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ç»Ÿè®¡æ•°æ®ã€å›¾è¡¨å»ºè®®å’Œæ´å¯Ÿ
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. æ•°æ®æ¦‚è§ˆï¼šå…ˆç”¨"basic_stats"äº†è§£æ•°æ®åˆ†å¸ƒ
    2. è´¨é‡æ£€æŸ¥ï¼šç”¨"missing_values"å’Œ"duplicates"æ£€æŸ¥æ•°æ®è´¨é‡
    3. å…³ç³»æ¢ç´¢ï¼šç”¨"correlation"å‘ç°å˜é‡å…³ç³»
    4. å¼‚å¸¸æ£€æµ‹ï¼šç”¨"outliers"è¯†åˆ«å¼‚å¸¸æ•°æ®
    5. é€æ­¥æ·±å…¥ï¼šä»åŸºç¡€ç»Ÿè®¡åˆ°é«˜çº§åˆ†æ
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - å…ˆè¿›è¡Œbasic_statsäº†è§£æ•°æ®æ¦‚å†µ
    - æ•°å€¼åˆ—ç”¨correlationåˆ†æå…³ç³»
    - å¤§æ•°æ®é›†æŒ‡å®šcolumnsæé«˜æ•ˆç‡
    - ç»“åˆget_data_infoäº†è§£è¡¨ç»“æ„
    
    âš ï¸ å¸¸è§é”™è¯¯é¿å…ï¼š
    - ç¡®ä¿table_nameå­˜åœ¨
    - correlationåªé€‚ç”¨äºæ•°å€¼åˆ—
    - columnsåç§°å¿…é¡»å‡†ç¡®åŒ¹é…
    - ç©ºè¡¨æˆ–å•åˆ—è¡¨æŸäº›åˆ†æä¼šå¤±è´¥
    
    ğŸ“ˆ é«˜æ•ˆä½¿ç”¨æµç¨‹ï¼š
    1. get_data_info() â†’ äº†è§£è¡¨ç»“æ„
    2. analyze_data("basic_stats") â†’ åŸºç¡€ç»Ÿè®¡
    3. analyze_data("missing_values") â†’ è´¨é‡æ£€æŸ¥
    4. analyze_data("correlation") â†’ å…³ç³»åˆ†æ
    5. analyze_data("outliers") â†’ å¼‚å¸¸æ£€æµ‹
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - æ¯ç§åˆ†æç±»å‹æœ‰ç‰¹å®šé€‚ç”¨åœºæ™¯
    - ç»“æœåŒ…å«ç»Ÿè®¡æ•°æ®å’Œä¸šåŠ¡æ´å¯Ÿ
    - æ”¯æŒå‚æ•°åŒ–å®šåˆ¶åˆ†æè¡Œä¸º
    """
    try:
        # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        if not _table_exists(table_name):
            result = {
                "status": "error",
                "message": f"è¡¨ '{table_name}' ä¸å­˜åœ¨"
            }
            return f"âŒ è¡¨ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·¯ç”±åˆ°å…·ä½“çš„åˆ†æå‡½æ•°
        analysis_map = {
            "basic_stats": _calculate_basic_stats,
            "correlation": _calculate_correlation,
            "outliers": _detect_outliers,
            "missing_values": _check_missing_values,
            "duplicates": _check_duplicates
        }
        
        if analysis_type not in analysis_map:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}",
                "supported_types": list(analysis_map.keys())
            }
            return f"âŒ åˆ†æç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = analysis_map[analysis_type](table_name, columns or [], options or {})
        
        if "error" in analysis_result:
            result = {
                "status": "error",
                "message": analysis_result["error"]
            }
            return f"âŒ åˆ†æå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è¿”å›æˆåŠŸç»“æœ
        result = {
            "status": "success",
            "message": f"{analysis_type} åˆ†æå®Œæˆ",
            "data": analysis_result,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "table_name": table_name,
                "analysis_type": analysis_type,
                "columns": columns or []
            }
        }
        
        return f"âœ… åˆ†æå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ åˆ†æå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# ================================
# è¾…åŠ©å‡½æ•°ï¼šæ•°æ®å¯¼å‡ºç›¸å…³
# ================================

def _export_to_excel(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°Excelæ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with get_db_connection() as conn:
            if data_source.upper().startswith('SELECT'):
                # SQLæŸ¥è¯¢
                df = pd.read_sql(data_source, conn)
            else:
                # è¡¨å
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            # å¯¼å‡ºåˆ°Excel
            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                sheet_name = options.get('sheet_name', 'Data')
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # æ·»åŠ æ ¼å¼åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if options.get('auto_adjust_columns', True):
                    worksheet = writer.sheets[sheet_name]
                    for column in worksheet.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 50)
                        worksheet.column_dimensions[column_letter].width = adjusted_width
            
            file_size = os.path.getsize(file_path)
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        raise Exception(f"Excelå¯¼å‡ºå¤±è´¥: {str(e)}")

def _export_to_csv(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°CSVæ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with get_db_connection() as conn:
            if data_source.upper().startswith('SELECT'):
                # SQLæŸ¥è¯¢
                df = pd.read_sql(data_source, conn)
            else:
                # è¡¨å
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            # å¯¼å‡ºåˆ°CSV
            encoding = options.get('encoding', 'utf-8-sig')  # ä½¿ç”¨utf-8-sigæ·»åŠ BOMå¤´
            separator = options.get('separator', ',')
            df.to_csv(file_path, index=False, encoding=encoding, sep=separator)
            
            file_size = os.path.getsize(file_path)
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        raise Exception(f"CSVå¯¼å‡ºå¤±è´¥: {str(e)}")

def _format_user_friendly_error(error_type: str, error_message: str, context: dict = None) -> dict:
    """æ ¼å¼åŒ–ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
    error_solutions = {
        "session_not_found": {
            "friendly_message": "å­˜å‚¨ä¼šè¯ä¸å­˜åœ¨",
            "explanation": "æ‚¨æŒ‡å®šçš„æ•°æ®å­˜å‚¨ä¼šè¯IDä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ä¼šè¯å·²è¢«åˆ é™¤æˆ–IDè¾“å…¥é”™è¯¯ã€‚",
            "solutions": [
                "æ£€æŸ¥ä¼šè¯IDæ˜¯å¦æ­£ç¡®",
                "ä½¿ç”¨ list_api_storage_sessions() æŸ¥çœ‹æ‰€æœ‰å¯ç”¨ä¼šè¯",
                "å¦‚æœä¼šè¯ç¡®å®ä¸å­˜åœ¨ï¼Œå·¥å…·ä¼šè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯"
            ]
        },
        "api_call_failed": {
            "friendly_message": "APIè°ƒç”¨å¤±è´¥",
            "explanation": "æ— æ³•æˆåŠŸè°ƒç”¨æŒ‡å®šçš„APIæ¥å£ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ã€APIå‚æ•°é”™è¯¯æˆ–æœåŠ¡ä¸å¯ç”¨ã€‚",
            "solutions": [
                "æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸",
                "éªŒè¯APIå‚æ•°æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤APIæœåŠ¡æ˜¯å¦å¯ç”¨",
                "æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ"
            ]
        },
        "invalid_parameters": {
            "friendly_message": "å‚æ•°æ— æ•ˆ",
            "explanation": "æä¾›çš„å‚æ•°ä¸ç¬¦åˆAPIè¦æ±‚ï¼Œå¯èƒ½å¯¼è‡´æ— æ³•è·å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚",
            "solutions": [
                "æ£€æŸ¥å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®",
                "å‚è€ƒAPIæ–‡æ¡£ç¡®è®¤å‚æ•°è¦æ±‚",
                "å°è¯•ä½¿ç”¨æ¨èçš„å‚æ•°ç»„åˆ",
                "ä½¿ç”¨ api_data_preview å…ˆæµ‹è¯•å‚æ•°"
            ]
        },
        "data_format_error": {
            "friendly_message": "æ•°æ®æ ¼å¼é”™è¯¯",
            "explanation": "è¿”å›çš„æ•°æ®æ ¼å¼æ— æ³•æ­£ç¡®è§£æï¼Œå¯èƒ½æ˜¯APIè¿”å›äº†æ„å¤–çš„æ•°æ®ç»“æ„ã€‚",
            "solutions": [
                "æ£€æŸ¥APIè¿”å›çš„åŸå§‹æ•°æ®",
                "å°è¯•ä¸åŒçš„æ•°æ®è½¬æ¢é…ç½®",
                "è”ç³»APIæä¾›å•†ç¡®è®¤æ•°æ®æ ¼å¼",
                "ä½¿ç”¨åŸå§‹æ ¼å¼æŸ¥çœ‹æ•°æ®å†…å®¹"
            ]
        },
        "file_not_found": {
            "friendly_message": "æ–‡ä»¶ä¸å­˜åœ¨",
            "explanation": "æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚",
            "solutions": [
                "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨",
                "æ£€æŸ¥æ–‡ä»¶è®¿é—®æƒé™",
                "ä½¿ç”¨ç»å¯¹è·¯å¾„è€Œéç›¸å¯¹è·¯å¾„"
            ]
        },
        "database_error": {
            "friendly_message": "æ•°æ®åº“æ“ä½œå¤±è´¥",
            "explanation": "æ•°æ®åº“æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œå¯èƒ½æ˜¯è¿æ¥é—®é¢˜æˆ–SQLè¯­æ³•é”™è¯¯ã€‚",
            "solutions": [
                "æ£€æŸ¥æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸",
                "éªŒè¯SQLè¯­æ³•æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤è¡¨å’Œå­—æ®µåç§°æ˜¯å¦å­˜åœ¨",
                "æ£€æŸ¥æ•°æ®åº“æƒé™è®¾ç½®"
            ]
        }
    }
    
    error_info = error_solutions.get(error_type, {
        "friendly_message": "æ“ä½œå¤±è´¥",
        "explanation": "æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿäº†æœªçŸ¥é”™è¯¯ã€‚",
        "solutions": [
            "æ£€æŸ¥è¾“å…¥å‚æ•°æ˜¯å¦æ­£ç¡®",
            "é‡è¯•æ“ä½œ",
            "å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ"
        ]
    })
    
    result = {
        "error_type": error_type,
        "friendly_message": error_info["friendly_message"],
        "explanation": error_info["explanation"],
        "solutions": error_info["solutions"],
        "technical_details": error_message
    }
    
    if context:
        result["context"] = context
    
    return result

def _generate_enhanced_preview(data, max_rows=10, max_cols=10, preview_fields=None, 
                             preview_depth=3, show_data_types=True, truncate_length=100):
    """ç”Ÿæˆå¢å¼ºçš„æ•°æ®é¢„è§ˆ"""
    try:
        preview_text = ""
        structure_info = {}
        
        # åˆ†ææ•°æ®ç»“æ„
        if isinstance(data, dict):
            structure_info["type"] = "object"
            structure_info["keys"] = list(data.keys())
            structure_info["total_keys"] = len(data.keys())
            
            # æŸ¥æ‰¾ä¸»è¦æ•°æ®æ•°ç»„
            main_data_key = None
            for key, value in data.items():
                if isinstance(value, list) and len(value) > 0:
                    main_data_key = key
                    break
            
            if main_data_key:
                structure_info["main_data_key"] = main_data_key
                structure_info["main_data_count"] = len(data[main_data_key])
                preview_data = data[main_data_key]
            else:
                preview_data = [data]  # å°†å•ä¸ªå¯¹è±¡åŒ…è£…ä¸ºåˆ—è¡¨
                
        elif isinstance(data, list):
            structure_info["type"] = "array"
            structure_info["total_items"] = len(data)
            preview_data = data
        else:
            structure_info["type"] = "primitive"
            structure_info["value_type"] = type(data).__name__
            preview_text = f"åŸå§‹æ•°æ®å€¼: {str(data)[:truncate_length]}"
            if len(str(data)) > truncate_length:
                preview_text += "...(å·²æˆªæ–­)"
            return {"preview_text": preview_text, "structure_info": structure_info}
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œé¢„è§ˆ
        try:
            if preview_data and isinstance(preview_data, list) and len(preview_data) > 0:
                # æ‰å¹³åŒ–åµŒå¥—æ•°æ®
                df = pd.json_normalize(preview_data, max_level=preview_depth)
                
                # è¿‡æ»¤æŒ‡å®šå­—æ®µ
                if preview_fields:
                    available_fields = [col for col in preview_fields if col in df.columns]
                    if available_fields:
                        df = df[available_fields]
                        structure_info["filtered_fields"] = available_fields
                        structure_info["missing_fields"] = [col for col in preview_fields if col not in df.columns]
                    else:
                        preview_text += "âš ï¸ æŒ‡å®šçš„é¢„è§ˆå­—æ®µéƒ½ä¸å­˜åœ¨\n"
                        preview_text += f"å¯ç”¨å­—æ®µ: {list(df.columns)[:10]}\n\n"
                
                # é™åˆ¶è¡Œæ•°å’Œåˆ—æ•°
                original_rows, original_cols = df.shape
                structure_info["original_shape"] = {"rows": original_rows, "columns": original_cols}
                
                if original_rows > max_rows:
                    df_preview = df.head(max_rows)
                    row_info = f"æ˜¾ç¤ºå‰{max_rows}è¡Œï¼Œå…±{original_rows}è¡Œ"
                else:
                    df_preview = df
                    row_info = f"å…±{original_rows}è¡Œ"
                
                if original_cols > max_cols:
                    df_preview = df_preview.iloc[:, :max_cols]
                    col_info = f"æ˜¾ç¤ºå‰{max_cols}åˆ—ï¼Œå…±{original_cols}åˆ—"
                else:
                    col_info = f"å…±{original_cols}åˆ—"
                
                # ç”Ÿæˆé¢„è§ˆæ–‡æœ¬
                preview_text += f"ğŸ“Š æ•°æ®é¢„è§ˆ ({row_info}, {col_info}):\n\n"
                
                # æˆªæ–­é•¿å­—æ®µå€¼
                df_display = df_preview.copy()
                for col in df_display.columns:
                    if df_display[col].dtype == 'object':
                        df_display[col] = df_display[col].astype(str).apply(
                            lambda x: x[:truncate_length] + "..." if len(x) > truncate_length else x
                        )
                
                preview_text += df_display.to_string(index=False, max_colwidth=truncate_length)
                
                # æ·»åŠ æ•°æ®ç±»å‹ä¿¡æ¯
                if show_data_types and original_cols <= max_cols:
                    preview_text += "\n\nğŸ“‹ æ•°æ®ç±»å‹:\n"
                    for col in df_preview.columns:
                        preview_text += f"  {col}: {df[col].dtype}\n"
                
                # æ·»åŠ å­—æ®µç»Ÿè®¡
                if original_cols > max_cols:
                    preview_text += f"\nğŸ’¡ æç¤º: è¿˜æœ‰ {original_cols - max_cols} ä¸ªå­—æ®µæœªæ˜¾ç¤º\n"
                    preview_text += f"æ‰€æœ‰å­—æ®µ: {', '.join(list(df.columns)[:20])}{'...' if original_cols > 20 else ''}\n"
                
                structure_info["preview_shape"] = {"rows": len(df_preview), "columns": len(df_preview.columns)}
                structure_info["all_columns"] = list(df.columns)
                
            else:
                preview_text = "ğŸ“­ æ•°æ®ä¸ºç©ºæˆ–æ— æ³•è§£æ"
                structure_info["empty"] = True
                
        except Exception as e:
            # å¦‚æœDataFrameè½¬æ¢å¤±è´¥ï¼Œæ˜¾ç¤ºåŸå§‹æ•°æ®ç»“æ„
            preview_text += f"âš ï¸ æ— æ³•è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼ï¼Œæ˜¾ç¤ºåŸå§‹ç»“æ„:\n\n"
            preview_text += json.dumps(preview_data[:3] if isinstance(preview_data, list) else preview_data, 
                                     indent=2, ensure_ascii=False, default=str)[:1000]
            if len(str(preview_data)) > 1000:
                preview_text += "\n...(æ•°æ®å·²æˆªæ–­)"
            structure_info["conversion_error"] = str(e)
        
        return {"preview_text": preview_text, "structure_info": structure_info}
        
    except Exception as e:
        return {
            "preview_text": f"âŒ é¢„è§ˆç”Ÿæˆå¤±è´¥: {str(e)}",
            "structure_info": {"error": str(e)}
        }

def _export_to_json(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°JSONæ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with get_db_connection() as conn:
            if data_source.upper().startswith('SELECT'):
                # SQLæŸ¥è¯¢
                df = pd.read_sql(data_source, conn)
            else:
                # è¡¨å
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼
            orient = options.get('orient', 'records')  # records, index, values, split, table
            
            if orient == 'records':
                data = df.to_dict('records')
            elif orient == 'index':
                data = df.to_dict('index')
            elif orient == 'values':
                data = df.values.tolist()
            elif orient == 'split':
                data = df.to_dict('split')
            elif orient == 'table':
                data = df.to_dict('table')
            else:
                data = df.to_dict('records')
            
            # å†™å…¥JSONæ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            file_size = os.path.getsize(file_path)
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        raise Exception(f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}")

@mcp.tool()
def export_data(
    export_type: str,
    data_source: str,
    file_path: str = None,
    options: dict = None
) -> str:
    """
    ğŸ“¤ æ•°æ®å¯¼å‡ºå·¥å…· - å°†æ•°æ®å¯¼å‡ºä¸ºå„ç§æ ¼å¼æ–‡ä»¶
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼šExcelã€CSVã€JSON
    - å¯å¯¼å‡ºè¡¨æ•°æ®æˆ–SQLæŸ¥è¯¢ç»“æœ
    - è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨æŒ‡å®šè·¯å¾„
    - æ”¯æŒå¯¼å‡ºé€‰é¡¹è‡ªå®šä¹‰
    
    Args:
        export_type: å¯¼å‡ºæ ¼å¼ç±»å‹
            - "excel": Excelæ–‡ä»¶(.xlsx)
            - "csv": CSVæ–‡ä»¶(.csv)
            - "json": JSONæ–‡ä»¶(.json)
        data_source: æ•°æ®æº
            - è¡¨å: ç›´æ¥å¯¼å‡ºæ•´ä¸ªè¡¨
            - SQLæŸ¥è¯¢: å¯¼å‡ºæŸ¥è¯¢ç»“æœï¼ˆä»¥SELECTå¼€å¤´ï¼‰
        file_path: å¯¼å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            - None: è‡ªåŠ¨ç”Ÿæˆè·¯å¾„åˆ°exports/ç›®å½•
            - æŒ‡å®šè·¯å¾„: ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
        options: å¯¼å‡ºé€‰é¡¹ï¼ˆå¯é€‰å­—å…¸ï¼‰
            - Excel: {"sheet_name": "å·¥ä½œè¡¨å", "auto_adjust_columns": True}
            - CSV: {"encoding": "utf-8", "separator": ","}
            - JSON: {"orient": "records", "indent": 2}
    
    Returns:
        str: JSONæ ¼å¼çš„å¯¼å‡ºç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€è®°å½•æ•°ç­‰ä¿¡æ¯
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. è¡¨å¯¼å‡ºï¼šexport_data("excel", "table_name")
    2. æŸ¥è¯¢å¯¼å‡ºï¼šexport_data("csv", "SELECT * FROM table WHERE condition")
    3. è‡ªå®šä¹‰æ ¼å¼ï¼šä½¿ç”¨optionså‚æ•°è°ƒæ•´å¯¼å‡ºæ ¼å¼
    4. æ‰¹é‡å¯¼å‡ºï¼šç»“åˆå¾ªç¯å¯¼å‡ºå¤šä¸ªè¡¨æˆ–æŸ¥è¯¢
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - Excelé€‚åˆæŠ¥è¡¨å’Œå¯è§†åŒ–
    - CSVé€‚åˆæ•°æ®äº¤æ¢å’Œå¯¼å…¥å…¶ä»–ç³»ç»Ÿ
    - JSONé€‚åˆAPIå’Œç¨‹åºå¤„ç†
    - å¤§æ•°æ®é‡ä¼˜å…ˆä½¿ç”¨CSV
    
    âš ï¸ å¸¸è§é”™è¯¯é¿å…ï¼š
    - ç¡®ä¿data_sourceå­˜åœ¨ï¼ˆè¡¨åï¼‰æˆ–è¯­æ³•æ­£ç¡®ï¼ˆSQLï¼‰
    - æ–‡ä»¶è·¯å¾„ç›®å½•å¿…é¡»å­˜åœ¨æˆ–å¯åˆ›å»º
    - æ³¨æ„æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´
    
    ğŸ“ˆ é«˜æ•ˆä½¿ç”¨æµç¨‹ï¼š
    1. ç¡®å®šå¯¼å‡ºéœ€æ±‚ï¼ˆæ ¼å¼ã€å†…å®¹ï¼‰
    2. é€‰æ‹©åˆé€‚çš„export_type
    3. å‡†å¤‡data_sourceï¼ˆè¡¨åæˆ–SQLï¼‰
    4. è®¾ç½®optionsï¼ˆå¦‚éœ€è¦ï¼‰
    5. æ‰§è¡Œå¯¼å‡ºå¹¶æ£€æŸ¥ç»“æœ
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - æ”¯æŒè¡¨å’ŒæŸ¥è¯¢ä¸¤ç§æ•°æ®æº
    - è‡ªåŠ¨å¤„ç†æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼
    - æä¾›è¯¦ç»†çš„å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # ç”Ÿæˆé»˜è®¤æ–‡ä»¶è·¯å¾„
        if not file_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä»æ•°æ®æºæå–åç§°
            if data_source.upper().startswith('SELECT'):
                source_name = "query_result"
            else:
                source_name = data_source
            
            # æ˜ å°„å¯¼å‡ºç±»å‹åˆ°æ–‡ä»¶æ‰©å±•å
            extension_map = {
                "excel": "xlsx",
                "csv": "csv",
                "json": "json"
            }
            extension = extension_map.get(export_type, export_type)
            file_path = f"exports/{source_name}_{timestamp}.{extension}"
        
        # è·¯ç”±åˆ°å…·ä½“çš„å¯¼å‡ºå‡½æ•°
        export_map = {
            "excel": _export_to_excel,
            "csv": _export_to_csv,
            "json": _export_to_json
        }
        
        if export_type not in export_map:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {export_type}",
                "supported_types": list(export_map.keys())
            }
            return f"âŒ å¯¼å‡ºç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æ‰§è¡Œå¯¼å‡º
        export_result = export_map[export_type](data_source, file_path, options or {})
        
        result = {
            "status": "success",
            "message": f"æ•°æ®å¯¼å‡ºå®Œæˆ",
            "data": {
                "file_path": file_path,
                "export_type": export_type,
                "file_size": export_result.get("file_size"),
                "record_count": export_result.get("record_count"),
                "columns": export_result.get("columns")
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "data_source": data_source
            }
        }
        
        return f"âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ å¯¼å‡ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"




















# ================================
# æ•°æ®å¤„ç†å·¥å…· (Excelæ•°æ®å¤„ç†)
# ================================

def _process_clean(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®æ¸…æ´—å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            original_count = len(df)
            operations_performed = []
            
            # åˆ é™¤é‡å¤è¡Œ
            if config.get('remove_duplicates', False):
                before_count = len(df)
                df = df.drop_duplicates()
                removed_count = before_count - len(df)
                operations_performed.append(f"åˆ é™¤é‡å¤è¡Œ: {removed_count}è¡Œ")
            
            # å¤„ç†ç¼ºå¤±å€¼
            if 'fill_missing' in config:
                fill_config = config['fill_missing']
                for column, fill_method in fill_config.items():
                    if column in df.columns:
                        method = fill_method.get('method', 'mean')
                        missing_count = df[column].isnull().sum()
                        
                        if method == 'mean' and df[column].dtype in ['int64', 'float64']:
                            df[column] = df[column].fillna(df[column].mean())
                        elif method == 'median' and df[column].dtype in ['int64', 'float64']:
                            df[column] = df[column].fillna(df[column].median())
                        elif method == 'mode':
                            df[column] = df[column].fillna(df[column].mode().iloc[0] if not df[column].mode().empty else '')
                        elif method == 'forward':
                            df[column] = df[column].fillna(method='ffill')
                        elif method == 'backward':
                            df[column] = df[column].fillna(method='bfill')
                        else:
                            # è‡ªå®šä¹‰å€¼
                            fill_value = fill_method.get('value', '')
                            df[column] = df[column].fillna(fill_value)
                        
                        operations_performed.append(f"å¡«å……ç¼ºå¤±å€¼ {column}: {missing_count}ä¸ª")
            
            # å¼‚å¸¸å€¼å¤„ç†
            if 'remove_outliers' in config:
                outlier_config = config['remove_outliers']
                columns = outlier_config.get('columns', [])
                method = outlier_config.get('method', 'iqr')
                threshold = outlier_config.get('threshold', 1.5)
                
                for column in columns:
                    if column in df.columns and df[column].dtype in ['int64', 'float64']:
                        before_count = len(df)
                        
                        if method == 'iqr':
                            Q1 = df[column].quantile(0.25)
                            Q3 = df[column].quantile(0.75)
                            IQR = Q3 - Q1
                            lower_bound = Q1 - threshold * IQR
                            upper_bound = Q3 + threshold * IQR
                            df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
                        elif method == 'zscore':
                            z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())
                            df = df[z_scores < threshold]
                        
                        removed_count = before_count - len(df)
                        operations_performed.append(f"ç§»é™¤å¼‚å¸¸å€¼ {column}: {removed_count}è¡Œ")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'clean',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    _ensure_metadata_table(conn)
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "original_rows": original_count,
                "processed_rows": len(df),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")

def _process_transform(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®è½¬æ¢å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            operations_performed = []
            
            # åˆ—é‡å‘½å
            if 'rename_columns' in config:
                rename_map = config['rename_columns']
                df.rename(columns=rename_map, inplace=True)
                operations_performed.append(f"é‡å‘½ååˆ—: {list(rename_map.keys())}")
            
            # æ•°æ®æ ‡å‡†åŒ–/å½’ä¸€åŒ–
            if 'normalize' in config:
                normalize_config = config['normalize']
                columns = normalize_config.get('columns', [])
                method = normalize_config.get('method', 'minmax')
                
                for column in columns:
                    if column in df.columns and df[column].dtype in ['int64', 'float64']:
                        if method == 'minmax':
                            df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())
                        elif method == 'zscore':
                            df[column] = (df[column] - df[column].mean()) / df[column].std()
                        operations_performed.append(f"æ ‡å‡†åŒ– {column} ({method})")
            
            # åˆ›å»ºæ–°åˆ—
            if 'new_columns' in config:
                new_columns_config = config['new_columns']
                for new_col_name, expression in new_columns_config.items():
                    try:
                        # ä½¿ç”¨evalè®¡ç®—æ–°åˆ—ï¼ˆå®‰å…¨æ€§è€ƒè™‘ï¼Œä»…æ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
                        if isinstance(expression, str):
                            # æ›¿æ¢åˆ—åä¸ºå®é™…çš„æ•°æ®æ¡†åˆ—å¼•ç”¨
                            safe_expr = expression
                            for col in df.columns:
                                safe_expr = safe_expr.replace(col, f"df['{col}']")
                            
                            # æ”¯æŒåŸºæœ¬çš„æ•°å­¦è¿ç®—å’Œæ¡ä»¶è¡¨è¾¾å¼
                            if any(op in safe_expr for op in ['+', '-', '*', '/', 'CASE', 'WHEN']):
                                if 'CASE' in safe_expr and 'WHEN' in safe_expr:
                                    # å¤„ç†SQLé£æ ¼çš„CASE WHENè¡¨è¾¾å¼
                                    # ç®€åŒ–å¤„ç†ï¼Œè½¬æ¢ä¸ºpandasçš„whereè¯­å¥
                                    if 'age' in safe_expr and ('é’å¹´' in safe_expr or 'THEN' in safe_expr):
                                        df[new_col_name] = df['age'].apply(
                                            lambda x: 'é’å¹´' if x < 30 else ('ä¸­å¹´' if x < 40 else 'èµ„æ·±')
                                        )
                                    else:
                                        df[new_col_name] = 'Unknown'  # é»˜è®¤å€¼
                                else:
                                    # åŸºæœ¬æ•°å­¦è¿ç®—
                                    df[new_col_name] = eval(safe_expr)
                            else:
                                df[new_col_name] = expression
                        else:
                            df[new_col_name] = expression
                        
                        operations_performed.append(f"åˆ›å»ºæ–°åˆ—: {new_col_name}")
                    except Exception as e:
                        operations_performed.append(f"åˆ›å»ºæ–°åˆ—å¤±è´¥ {new_col_name}: {str(e)}")
            
            # åˆ†ç±»å˜é‡ç¼–ç 
            if 'encode_categorical' in config:
                encode_config = config['encode_categorical']
                for column, encode_method in encode_config.items():
                    if column in df.columns:
                        method = encode_method.get('method', 'label')
                        
                        if method == 'label':
                            # æ ‡ç­¾ç¼–ç 
                            unique_values = df[column].unique()
                            label_map = {val: idx for idx, val in enumerate(unique_values)}
                            df[column] = df[column].map(label_map)
                            operations_performed.append(f"æ ‡ç­¾ç¼–ç  {column}")
                        elif method == 'onehot':
                            # ç‹¬çƒ­ç¼–ç 
                            dummies = pd.get_dummies(df[column], prefix=column)
                            df = pd.concat([df.drop(column, axis=1), dummies], axis=1)
                            operations_performed.append(f"ç‹¬çƒ­ç¼–ç  {column}")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'transform',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    _ensure_metadata_table(conn)
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "processed_rows": len(df),
                "columns": list(df.columns),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®è½¬æ¢å¤±è´¥: {str(e)}")

def _process_filter(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®ç­›é€‰å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            original_count = len(df)
            operations_performed = []
            
            # æ¡ä»¶ç­›é€‰
            if 'filter_condition' in config:
                condition = config['filter_condition']
                try:
                    # å°è¯•ä½¿ç”¨pandas queryæ–¹æ³•ï¼ˆé€‚ç”¨äºç®€å•çš„è‹±æ–‡åˆ—åï¼‰
                    try:
                        df = df.query(condition)
                        operations_performed.append(f"æ¡ä»¶ç­›é€‰: {condition}")
                    except (SyntaxError, ValueError, KeyError) as query_error:
                        # å¦‚æœqueryæ–¹æ³•å¤±è´¥ï¼Œå°è¯•è½¬æ¢ä¸ºSQL WHEREå­å¥
                        # å°†pandas DataFrameé‡æ–°å†™å…¥ä¸´æ—¶è¡¨ï¼Œç„¶åç”¨SQLç­›é€‰
                        temp_table = f"temp_filter_{int(time.time() * 1000)}"
                        df.to_sql(temp_table, conn, if_exists='replace', index=False)
                        
                        # æ„å»ºSQLæŸ¥è¯¢ï¼Œæ”¯æŒä¸­æ–‡åˆ—å
                        sql_condition = condition
                        # è½¬æ¢å¸¸è§çš„pandasè¯­æ³•åˆ°SQLè¯­æ³•
                        sql_condition = sql_condition.replace(' and ', ' AND ').replace(' or ', ' OR ')
                        sql_condition = sql_condition.replace(' & ', ' AND ').replace(' | ', ' OR ')
                        
                        # æ‰§è¡ŒSQLæŸ¥è¯¢
                        sql_query = f'SELECT * FROM "{temp_table}" WHERE {sql_condition}'
                        df = pd.read_sql(sql_query, conn)
                        
                        # æ¸…ç†ä¸´æ—¶è¡¨
                        conn.execute(f'DROP TABLE IF EXISTS "{temp_table}"')
                        conn.commit()
                        
                        operations_performed.append(f"æ¡ä»¶ç­›é€‰(SQL): {condition}")
                        
                except Exception as e:
                    operations_performed.append(f"æ¡ä»¶ç­›é€‰å¤±è´¥: {str(e)}")
                    # å¦‚æœç­›é€‰å¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ä½†ä¸ä¸­æ–­å¤„ç†
                    import traceback
                    logger.error(f"ç­›é€‰æ¡ä»¶è§£æå¤±è´¥: {condition}, é”™è¯¯: {traceback.format_exc()}")
            
            # åˆ—é€‰æ‹©
            if 'select_columns' in config:
                columns = config['select_columns']
                available_columns = [col for col in columns if col in df.columns]
                if available_columns:
                    df = df[available_columns]
                    operations_performed.append(f"é€‰æ‹©åˆ—: {available_columns}")
            
            # æ•°æ®é‡‡æ ·
            if 'sample' in config:
                sample_config = config['sample']
                n = sample_config.get('n', 1000)
                method = sample_config.get('method', 'random')
                
                if method == 'random' and len(df) > n:
                    df = df.sample(n=n, random_state=42)
                    operations_performed.append(f"éšæœºé‡‡æ ·: {n}è¡Œ")
                elif method == 'head':
                    df = df.head(n)
                    operations_performed.append(f"å¤´éƒ¨é‡‡æ ·: {n}è¡Œ")
                elif method == 'tail':
                    df = df.tail(n)
                    operations_performed.append(f"å°¾éƒ¨é‡‡æ ·: {n}è¡Œ")
            
            # æ’åº
            if 'sort_by' in config:
                sort_config = config['sort_by']
                column = sort_config.get('column')
                ascending = sort_config.get('ascending', True)
                
                if column and column in df.columns:
                    df = df.sort_values(by=column, ascending=ascending)
                    operations_performed.append(f"æ’åº: {column} ({'å‡åº' if ascending else 'é™åº'})")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'filter',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    _ensure_metadata_table(conn)
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "original_rows": original_count,
                "filtered_rows": len(df),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®ç­›é€‰å¤±è´¥: {str(e)}")

def _ensure_metadata_table(conn) -> None:
    """ç¡®ä¿data_metadataè¡¨å­˜åœ¨"""
    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS data_metadata (
                table_name TEXT PRIMARY KEY,
                source_type TEXT,
                source_path TEXT,
                created_at TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        logger.warning(f"åˆ›å»ºmetadataè¡¨å¤±è´¥: {e}")

def _process_aggregate(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®èšåˆå¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # ç¡®ä¿metadataè¡¨å­˜åœ¨
            _ensure_metadata_table(conn)
            
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            operations_performed = []
            
            # åˆ†ç»„èšåˆ
            if 'group_by' in config:
                # æ”¯æŒä¸¤ç§é…ç½®æ ¼å¼
                if isinstance(config['group_by'], dict):
                    # æ ¼å¼1: {'columns': [...], 'agg': {...}}
                    group_config = config['group_by']
                    group_columns = group_config.get('columns', [])
                    agg_functions = group_config.get('agg', {})
                elif isinstance(config['group_by'], list):
                    # æ ¼å¼2: ç›´æ¥æ˜¯åˆ—ååˆ—è¡¨ï¼Œé…åˆ 'aggregations' é”®
                    group_columns = config['group_by']
                    agg_functions = config.get('aggregations', {})
                else:
                    group_columns = []
                    agg_functions = {}
                
                if group_columns and agg_functions:
                    # å¤„ç†èšåˆå‡½æ•°æ ¼å¼
                    processed_agg = {}
                    for col, funcs in agg_functions.items():
                        if isinstance(funcs, list):
                            # å¤šä¸ªèšåˆå‡½æ•°
                            processed_agg[col] = funcs
                        else:
                            # å•ä¸ªèšåˆå‡½æ•°
                            processed_agg[col] = funcs
                    
                    df = df.groupby(group_columns).agg(processed_agg).reset_index()
                    # æ‰å¹³åŒ–åˆ—å
                    df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df.columns.values]
                    operations_performed.append(f"åˆ†ç»„èšåˆ: {group_columns}")
            
            # æ•°æ®é€è§†è¡¨
            if 'pivot_table' in config:
                pivot_config = config['pivot_table']
                index = pivot_config.get('index')
                columns = pivot_config.get('columns')
                values = pivot_config.get('values')
                aggfunc = pivot_config.get('aggfunc', 'mean')
                
                if index and columns and values:
                    df = pd.pivot_table(df, index=index, columns=columns, values=values, aggfunc=aggfunc, fill_value=0).reset_index()
                    operations_performed.append(f"é€è§†è¡¨: {index} x {columns}")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'aggregate',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "processed_rows": len(df),
                "columns": list(df.columns),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®èšåˆå¤±è´¥: {str(e)}")

def _process_merge(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®åˆå¹¶å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–ä¸»æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df1 = pd.read_sql(data_source, conn)
            else:
                df1 = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            operations_performed = []
            
            # è¡¨è¿æ¥
            if 'join_with' in config:
                join_table = config['join_with']
                join_type = config.get('join_type', 'inner')
                on_column = config.get('on')
                suffixes = config.get('suffixes', ['_x', '_y'])
                
                # è·å–è¦è¿æ¥çš„è¡¨
                df2 = pd.read_sql(f'SELECT * FROM "{join_table}"', conn)
                
                if on_column:
                    df = df1.merge(df2, on=on_column, how=join_type, suffixes=suffixes)
                    operations_performed.append(f"{join_type}è¿æ¥: {data_source} + {join_table} on {on_column}")
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¿æ¥åˆ—ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
                    common_columns = list(set(df1.columns) & set(df2.columns))
                    if common_columns:
                        df = df1.merge(df2, on=common_columns[0], how=join_type, suffixes=suffixes)
                        operations_performed.append(f"{join_type}è¿æ¥: {data_source} + {join_table} on {common_columns[0]}")
                    else:
                        df = df1  # æ— æ³•è¿æ¥ï¼Œä¿æŒåŸæ•°æ®
                        operations_performed.append(f"è¿æ¥å¤±è´¥: æ— å…±åŒåˆ—")
            else:
                df = df1
            
            # æ•°æ®è¿½åŠ 
            if 'append_table' in config:
                append_table = config['append_table']
                df2 = pd.read_sql(f"SELECT * FROM {append_table}", conn)
                
                # ç¡®ä¿åˆ—ä¸€è‡´
                common_columns = list(set(df.columns) & set(df2.columns))
                if common_columns:
                    df = pd.concat([df[common_columns], df2[common_columns]], ignore_index=True)
                    operations_performed.append(f"è¿½åŠ æ•°æ®: {append_table}")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'merge',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    _ensure_metadata_table(conn)
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "processed_rows": len(df),
                "columns": list(df.columns),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")

def _process_reshape(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®é‡å¡‘å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                df = pd.read_sql(f'SELECT * FROM "{data_source}"', conn)
            
            operations_performed = []
            
            # å®½è¡¨è½¬é•¿è¡¨ (melt)
            if 'melt' in config:
                melt_config = config['melt']
                id_vars = melt_config.get('id_vars', [])
                value_vars = melt_config.get('value_vars')
                var_name = melt_config.get('var_name', 'variable')
                value_name = melt_config.get('value_name', 'value')
                
                df = pd.melt(df, id_vars=id_vars, value_vars=value_vars, var_name=var_name, value_name=value_name)
                operations_performed.append(f"å®½è¡¨è½¬é•¿è¡¨: {len(id_vars)}ä¸ªIDåˆ—")
            
            # é•¿è¡¨è½¬å®½è¡¨ (pivot)
            if 'pivot' in config:
                pivot_config = config['pivot']
                index = pivot_config.get('index')
                columns = pivot_config.get('columns')
                values = pivot_config.get('values')
                
                if index and columns and values:
                    df = df.pivot(index=index, columns=columns, values=values).reset_index()
                    df.columns.name = None  # ç§»é™¤åˆ—å
                    operations_performed.append(f"é•¿è¡¨è½¬å®½è¡¨: {index} -> {columns}")
            
            # æ•°æ®è½¬ç½®
            if config.get('transpose', False):
                df = df.T
                df.reset_index(inplace=True)
                operations_performed.append("æ•°æ®è½¬ç½®")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            table_name = target_table or data_source
            if not table_name.upper().startswith('SELECT'):
                df.to_sql(table_name, conn, if_exists='replace', index=False)
                
                # è®°å½•å…ƒæ•°æ®
                metadata = {
                    'table_name': table_name,
                    'source_type': 'processed_data',
                    'original_source': data_source,
                    'processing_type': 'reshape',
                    'operations': operations_performed,
                    'created_at': datetime.now().isoformat()
                }
                
                try:
                    _ensure_metadata_table(conn)
                    conn.execute("""
                        INSERT OR REPLACE INTO data_metadata 
                        (table_name, source_type, source_path, created_at, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (table_name, 'processed_data', data_source, datetime.now().isoformat(), json.dumps(metadata)))
                    conn.commit()
                except Exception as metadata_error:
                    logger.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {metadata_error}")
            
            return {
                "processed_rows": len(df),
                "columns": list(df.columns),
                "operations": operations_performed,
                "target_table": table_name
            }
            
    except Exception as e:
        raise Exception(f"æ•°æ®é‡å¡‘å¤±è´¥: {str(e)}")

@mcp.tool()
def process_data(
    operation_type: str,
    data_source: str,
    config: dict,
    target_table: str = None
) -> str:
    """
    âš™ï¸ æ•°æ®å¤„ç†å·¥å…· - æ‰§è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€ç­›é€‰ç­‰æ“ä½œ
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æä¾›6ç§æ ¸å¿ƒæ•°æ®å¤„ç†åŠŸèƒ½
    - æ”¯æŒè¡¨å’ŒSQLæŸ¥è¯¢ä½œä¸ºæ•°æ®æº
    - çµæ´»çš„é…ç½®å‚æ•°ç³»ç»Ÿ
    - å¯æŒ‡å®šç›®æ ‡è¡¨æˆ–è¦†ç›–åŸè¡¨
    
    Args:
        operation_type: å¤„ç†æ“ä½œç±»å‹
            - "clean": æ•°æ®æ¸…æ´—ï¼ˆå»é‡ã€å¡«å……ç¼ºå¤±å€¼ã€æ•°æ®ç±»å‹è½¬æ¢ï¼‰
            - "transform": æ•°æ®è½¬æ¢ï¼ˆåˆ—é‡å‘½åã€æ ‡å‡†åŒ–ã€æ–°åˆ—è®¡ç®—ï¼‰
            - "filter": æ•°æ®ç­›é€‰ï¼ˆæ¡ä»¶è¿‡æ»¤ã€åˆ—é€‰æ‹©ã€æ•°æ®é‡‡æ ·ï¼‰
            - "aggregate": æ•°æ®èšåˆï¼ˆåˆ†ç»„ç»Ÿè®¡ã€æ±‡æ€»è®¡ç®—ï¼‰
            - "merge": æ•°æ®åˆå¹¶ï¼ˆè¡¨è¿æ¥ã€æ•°æ®æ‹¼æ¥ï¼‰
            - "reshape": æ•°æ®é‡å¡‘ï¼ˆé€è§†è¡¨ã€å®½é•¿è½¬æ¢ï¼‰
        data_source: æ•°æ®æº
            - è¡¨å: å¤„ç†æ•´ä¸ªè¡¨
            - SQLæŸ¥è¯¢: å¤„ç†æŸ¥è¯¢ç»“æœ
        config: æ“ä½œé…ç½®å­—å…¸ï¼ˆå¿…éœ€ï¼‰
            - clean: {"remove_duplicates": True, "fill_missing": {"col": {"method": "mean"}}}
            - transform: {"rename_columns": {"old": "new"}, "normalize": {"columns": ["col1"]}}
            - filter: {"filter_condition": "age > 18", "select_columns": ["name", "age"]}
            - aggregate: {"group_by": {"columns": ["dept"], "agg": {"salary": "mean"}}}
            - merge: {"right_table": "table2", "on": "id", "how": "inner"}
            - reshape: {"pivot": {"index": "date", "columns": "product", "values": "sales"}}
        target_table: ç›®æ ‡è¡¨åï¼ˆå¯é€‰ï¼‰
            - None: è¦†ç›–åŸè¡¨ï¼ˆé»˜è®¤ï¼‰
            - è¡¨å: ä¿å­˜åˆ°æ–°è¡¨
    
    Returns:
        str: JSONæ ¼å¼çš„å¤„ç†ç»“æœï¼ŒåŒ…å«æ“ä½œè¯¦æƒ…ã€å½±å“è¡Œæ•°å’Œæ–°è¡¨ä¿¡æ¯
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. æ•°æ®æ¸…æ´—ï¼šå…ˆç”¨"clean"å¤„ç†æ•°æ®è´¨é‡é—®é¢˜
    2. æ•°æ®è½¬æ¢ï¼šç”¨"transform"æ ‡å‡†åŒ–å’Œè®¡ç®—æ–°å­—æ®µ
    3. æ•°æ®ç­›é€‰ï¼šç”¨"filter"è·å–ç›®æ ‡æ•°æ®å­é›†
    4. æ•°æ®èšåˆï¼šç”¨"aggregate"ç”Ÿæˆæ±‡æ€»æŠ¥è¡¨
    5. æ•°æ®åˆå¹¶ï¼šç”¨"merge"å…³è”å¤šä¸ªæ•°æ®æº
    6. æ•°æ®é‡å¡‘ï¼šç”¨"reshape"æ”¹å˜æ•°æ®ç»“æ„
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - å¤„ç†å‰å…ˆå¤‡ä»½é‡è¦æ•°æ®
    - ä½¿ç”¨target_tableé¿å…è¦†ç›–åŸæ•°æ®
    - å¤æ‚æ“ä½œåˆ†æ­¥éª¤æ‰§è¡Œ
    - ç»“åˆanalyze_dataéªŒè¯å¤„ç†ç»“æœ
    
    âš ï¸ å¸¸è§é”™è¯¯é¿å…ï¼š
    - configå‚æ•°å¿…é¡»ç¬¦åˆoperation_typeè¦æ±‚
    - ç¡®ä¿å¼•ç”¨çš„åˆ—åå­˜åœ¨
    - mergeæ“ä½œéœ€è¦ç¡®ä¿å…³è”é”®å­˜åœ¨
    - å¤§æ•°æ®é‡æ“ä½œæ³¨æ„æ€§èƒ½
    
    ğŸ“ˆ é«˜æ•ˆä½¿ç”¨æµç¨‹ï¼š
    1. analyze_data() â†’ äº†è§£æ•°æ®è´¨é‡
    2. process_data("clean") â†’ æ¸…æ´—æ•°æ®
    3. process_data("transform") â†’ è½¬æ¢æ•°æ®
    4. process_data("filter") â†’ ç­›é€‰æ•°æ®
    5. analyze_data() â†’ éªŒè¯å¤„ç†ç»“æœ
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - æ¯ç§æ“ä½œç±»å‹æœ‰ç‰¹å®šçš„configæ ¼å¼
    - æ”¯æŒé“¾å¼å¤„ç†ï¼ˆä¸Šä¸€æ­¥è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥ï¼‰
    - æä¾›è¯¦ç»†çš„æ“ä½œæ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
    
    ğŸ“‹ é…ç½®ç¤ºä¾‹ï¼š
    ```python
    # æ•°æ®æ¸…æ´—
    config = {
        "remove_duplicates": True,
        "fill_missing": {
            "age": {"method": "mean"},
            "name": {"method": "mode"}
        }
    }
    
    # æ•°æ®ç­›é€‰
    config = {
        "filter_condition": "age > 18 and salary > 5000",
        "select_columns": ["name", "age", "department"]
    }
    
    # æ•°æ®èšåˆ
    config = {
        "group_by": {
            "columns": ["department"],
            "agg": {
                "salary": "mean",
                "age": "count"
            }
        }
    }
    ```
    """
    try:
        # è·¯ç”±æ˜ å°„ (Excelæ•°æ®å¤„ç†)
        processors = {
            "clean": _process_clean,
            "transform": _process_transform,
            "filter": _process_filter,
            "aggregate": _process_aggregate,
            "merge": _process_merge,
            "reshape": _process_reshape
        }
        
        if operation_type not in processors:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation_type}",
                "supported_types": list(processors.keys()),
                "note": "å½“å‰ç‰ˆæœ¬æ”¯æŒExcelæ•°æ®å¤„ç†ï¼Œåç»­å°†æ·»åŠ APIå’ŒSQLæ•°æ®å¤„ç†"
            }
            return f"âŒ æ“ä½œç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·¯ç”±åˆ°å¯¹åº”å¤„ç†å™¨
        process_result = processors[operation_type](data_source, config, target_table)
        
        result = {
            "status": "success",
            "message": f"æ•°æ®å¤„ç†å®Œæˆ",
            "data": {
                "operation_type": operation_type,
                "data_source": data_source,
                "target_table": process_result.get("target_table"),
                "processed_rows": process_result.get("processed_rows", process_result.get("filtered_rows")),
                "operations": process_result.get("operations", []),
                "columns": process_result.get("columns")
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "processing_category": "excel_data_processing"
            }
        }
        
        return f"âœ… æ•°æ®å¤„ç†æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ å¤„ç†å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def list_data_sources() -> str:
    """
    ğŸ“‹ æ•°æ®æºåˆ—è¡¨å·¥å…· - æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº
    
    ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
    - æ˜¾ç¤ºæœ¬åœ°SQLiteæ•°æ®åº“çŠ¶æ€
    - åˆ—å‡ºæ‰€æœ‰å¤–éƒ¨æ•°æ®åº“é…ç½®
    - æ˜¾ç¤ºæ¯ä¸ªæ•°æ®æºçš„è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬ä¿¡æ¯
    - åŒºåˆ†ä¸´æ—¶é…ç½®å’Œæ°¸ä¹…é…ç½®
    
    ğŸ“Š è¿”å›ä¿¡æ¯åŒ…æ‹¬ï¼š
    - æ•°æ®æºåç§°å’Œç±»å‹
    - è¿æ¥çŠ¶æ€ï¼ˆå¯ç”¨/å·²é…ç½®/å·²ç¦ç”¨ï¼‰
    - ä¸»æœºåœ°å€å’Œæ•°æ®åº“å
    - æ˜¯å¦ä¸ºé»˜è®¤æ•°æ®æº
    - é…ç½®åˆ›å»ºæ—¶é—´ï¼ˆä¸´æ—¶é…ç½®ï¼‰
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - ä¸ç¡®å®šæœ‰å“ªäº›æ•°æ®æºæ—¶æŸ¥çœ‹
    - æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€
    - æŸ¥æ‰¾ä¸´æ—¶é…ç½®åç§°
    - äº†è§£å¯ç”¨çš„æŸ¥è¯¢ç›®æ ‡
    
    Args:
        æ— éœ€å‚æ•°
    
    Returns:
        str: JSONæ ¼å¼çš„æ•°æ®æºåˆ—è¡¨ï¼ŒåŒ…å«è¯¦ç»†çš„é…ç½®ä¿¡æ¯
    
    ğŸš€ AIä½¿ç”¨å»ºè®®ï¼š
    - åœ¨æŸ¥è¯¢æ•°æ®å‰å…ˆè°ƒç”¨æ­¤å·¥å…·äº†è§£å¯ç”¨æ•°æ®æº
    - ç”¨äºè·å–æ­£ç¡®çš„database_nameå‚æ•°
    - æ£€æŸ¥ä¸´æ—¶é…ç½®æ˜¯å¦è¿˜å­˜åœ¨
    """
    try:
        # è·å–å¤–éƒ¨æ•°æ®åº“é…ç½®
        external_databases = database_manager.get_available_databases()
        
        # æ„å»ºæ•°æ®æºåˆ—è¡¨
        data_sources = {
            "æœ¬åœ°SQLite": {
                "type": "sqlite",
                "description": "æœ¬åœ°SQLiteæ•°æ®åº“ï¼ˆé»˜è®¤æ•°æ®æºï¼‰",
                "status": "å¯ç”¨",
                "database_path": DB_PATH,
                "is_default": True
            }
        }
        
        # æ·»åŠ å¤–éƒ¨æ•°æ®åº“
        for db_name, db_config in external_databases.items():
            data_sources[db_name] = {
                "type": db_config.get("type", "unknown"),
                "description": db_config.get("description", ""),
                "status": "å·²é…ç½®" if db_config.get("enabled", True) else "å·²ç¦ç”¨",
                "host": db_config.get("host", ""),
                "database": db_config.get("database", ""),
                "file_path": db_config.get("file_path", ""),
                "is_default": False
            }
        
        result = {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(data_sources)} ä¸ªæ•°æ®æº",
            "data": {
                "data_sources": data_sources,
                "count": len(data_sources),
                "usage_note": "ä½¿ç”¨execute_sqlæˆ–get_data_infoæ—¶ï¼Œå¯é€šè¿‡data_sourceå‚æ•°æŒ‡å®šæ•°æ®æºã€‚ä¸æŒ‡å®šåˆ™é»˜è®¤ä½¿ç”¨æœ¬åœ°SQLiteã€‚"
            },
            "metadata": {
                "timestamp": datetime.now().isoformat()
            }
        }
        
        return f"âœ… æ•°æ®æºåˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºåˆ—è¡¨å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–æ•°æ®æºåˆ—è¡¨å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def manage_database_config(
    action: str,
    config: dict = None
) -> str:
    """
    âš™ï¸ æ•°æ®åº“é…ç½®ç®¡ç†å·¥å…· - ç®¡ç†æ‰€æœ‰æ•°æ®åº“è¿æ¥é…ç½®
    
    ğŸ¯ æ”¯æŒçš„æ“ä½œç±»å‹ï¼š
    - "list" - åˆ—å‡ºæ‰€æœ‰æ•°æ®åº“é…ç½®ï¼ˆåŒ…æ‹¬ä¸´æ—¶å’Œæ°¸ä¹…ï¼‰
    - "test" - æµ‹è¯•æŒ‡å®šé…ç½®çš„è¿æ¥çŠ¶æ€
    - "add" - æ·»åŠ æ°¸ä¹…æ•°æ®åº“é…ç½®
    - "remove" - åˆ é™¤æŒ‡å®šé…ç½®
    - "reload" - é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶
    - "list_temp" - ä»…åˆ—å‡ºä¸´æ—¶é…ç½®
    - "cleanup_temp" - æ¸…ç†æ‰€æœ‰ä¸´æ—¶é…ç½®
    
    ğŸ“‹ å¸¸ç”¨æ“ä½œç¤ºä¾‹ï¼š
    
    1ï¸âƒ£ æŸ¥çœ‹æ‰€æœ‰é…ç½®ï¼š
       manage_database_config(action="list")
    
    2ï¸âƒ£ æµ‹è¯•è¿æ¥ï¼š
       manage_database_config(action="test", config={"database_name": "é…ç½®å"})
    
    3ï¸âƒ£ æ·»åŠ æ°¸ä¹…é…ç½®ï¼š
       manage_database_config(action="add", config={
           "database_name": "my_mysql",
           "database_config": {
               "host": "localhost",
               "port": 3306,
               "type": "mysql",
               "user": "root",
               "database": "test_db",
               "password": "password"
           }
       })
    
    4ï¸âƒ£ æ¸…ç†ä¸´æ—¶é…ç½®ï¼š
       manage_database_config(action="cleanup_temp")
    
    Args:
        action: æ“ä½œç±»å‹ï¼Œå¿…é¡»æ˜¯ä¸Šè¿°æ”¯æŒçš„æ“ä½œä¹‹ä¸€
        config: é…ç½®å‚æ•°å­—å…¸ï¼Œæ ¹æ®actionç±»å‹æä¾›ä¸åŒå‚æ•°
    
    Returns:
        str: JSONæ ¼å¼æ“ä½œç»“æœï¼ŒåŒ…å«çŠ¶æ€ã€æ¶ˆæ¯å’Œç›¸å…³æ•°æ®
    
    ğŸ’¡ AIä½¿ç”¨å»ºè®®ï¼š
    - ä¸ç¡®å®šæœ‰å“ªäº›é…ç½®æ—¶ï¼Œå…ˆç”¨action="list"æŸ¥çœ‹
    - è¿æ¥é—®é¢˜æ—¶ï¼Œç”¨action="test"æ£€æŸ¥é…ç½®çŠ¶æ€
    - ä¸´æ—¶é…ç½®è¿‡å¤šæ—¶ï¼Œç”¨action="cleanup_temp"æ¸…ç†
    """
    try:
        if action == "list":
            # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®åº“é…ç½®
            databases = database_manager.get_available_databases()
            
            result = {
                "status": "success",
                "message": f"æ‰¾åˆ° {len(databases)} ä¸ªæ•°æ®åº“é…ç½®",
                "data": {
                    "databases": databases,
                    "count": len(databases)
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            return f"âœ… æ•°æ®åº“é…ç½®åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        elif action == "test":
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            database_name = config.get("database_name") if config else None
            if not database_name:
                result = {
                    "status": "error",
                    "message": "ç¼ºå°‘database_nameå‚æ•°"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            is_valid, message = database_manager.test_connection(database_name)
            
            result = {
                "status": "success" if is_valid else "error",
                "message": message,
                "data": {
                    "database_name": database_name,
                    "connection_status": "success" if is_valid else "failed"
                }
            }
            
            status_icon = "âœ…" if is_valid else "âŒ"
            return f"{status_icon} è¿æ¥æµ‹è¯•ç»“æœ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        elif action == "add":
            # æ·»åŠ æ–°çš„æ•°æ®åº“é…ç½®
            if not config:
                result = {
                    "status": "error",
                    "message": "ç¼ºå°‘é…ç½®å‚æ•°"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            database_name = config.get("database_name")
            database_config = config.get("database_config")
            
            if not database_name or not database_config:
                result = {
                    "status": "error",
                    "message": "ç¼ºå°‘database_nameæˆ–database_configå‚æ•°"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            success = config_manager.add_database_config(database_name, database_config)
            
            if success:
                result = {
                    "status": "success",
                    "message": f"æ•°æ®åº“é…ç½®å·²æ·»åŠ : {database_name}",
                    "data": {
                        "database_name": database_name,
                        "database_type": database_config.get("type")
                    }
                }
                return f"âœ… é…ç½®æ·»åŠ æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            else:
                result = {
                    "status": "error",
                    "message": f"æ·»åŠ æ•°æ®åº“é…ç½®å¤±è´¥: {database_name}"
                }
                return f"âŒ é…ç½®æ·»åŠ å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
        elif action == "remove":
            # åˆ é™¤æ•°æ®åº“é…ç½®
            database_name = config.get("database_name") if config else None
            if not database_name:
                result = {
                    "status": "error",
                    "message": "ç¼ºå°‘database_nameå‚æ•°"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            success = config_manager.remove_database_config(database_name)
            
            if success:
                result = {
                    "status": "success",
                    "message": f"æ•°æ®åº“é…ç½®å·²åˆ é™¤: {database_name}",
                    "data": {
                        "database_name": database_name
                    }
                }
                return f"âœ… é…ç½®åˆ é™¤æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            else:
                result = {
                    "status": "error",
                    "message": f"åˆ é™¤æ•°æ®åº“é…ç½®å¤±è´¥: {database_name}"
                }
                return f"âŒ é…ç½®åˆ é™¤å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
        elif action == "list_temp":
            # åˆ—å‡ºæ‰€æœ‰ä¸´æ—¶é…ç½®
            temp_configs = config_manager.get_temporary_configs()
            
            result = {
                "status": "success",
                "message": f"æ‰¾åˆ° {len(temp_configs)} ä¸ªä¸´æ—¶é…ç½®",
                "data": {
                    "temporary_configs": temp_configs,
                    "count": len(temp_configs)
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            return f"âœ… ä¸´æ—¶é…ç½®åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        elif action == "cleanup_temp":
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶é…ç½®
            success, message = config_manager.cleanup_temporary_configs()
            
            result = {
                "status": "success" if success else "error",
                "message": message,
                "data": {
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            status_icon = "âœ…" if success else "âŒ"
            return f"{status_icon} ä¸´æ—¶é…ç½®æ¸…ç†ç»“æœ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        elif action == "reload":
            # é‡æ–°åŠ è½½é…ç½®
            config_manager.reload_config()
            
            result = {
                "status": "success",
                "message": "é…ç½®å·²é‡æ–°åŠ è½½",
                "data": {
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            return f"âœ… é…ç½®é‡æ–°åŠ è½½æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}",
                "supported_actions": ["list", "test", "add", "remove", "reload", "list_temp", "cleanup_temp"]
            }
            return f"âŒ æ“ä½œç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"æ•°æ®åº“é…ç½®ç®¡ç†å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®åº“é…ç½®ç®¡ç†å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ æ“ä½œå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def query_external_database(
    database_name: str,
    query: str,
    limit: int = 1000
) -> str:
    """
    ğŸŒ å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢å·¥å…· - ä¸“é—¨æŸ¥è¯¢å¤–éƒ¨æ•°æ®åº“
    
    ğŸ¯ ä½¿ç”¨åœºæ™¯ï¼š
    - æŸ¥è¯¢MySQLæ•°æ®åº“
    - æŸ¥è¯¢PostgreSQLæ•°æ®åº“
    - æŸ¥è¯¢MongoDBæ•°æ®åº“
    - æŸ¥è¯¢æ‰€æœ‰é€šè¿‡connect_data_sourceè¿æ¥çš„å¤–éƒ¨æ•°æ®åº“
    
    âš ï¸ å‰ç½®æ¡ä»¶ï¼š
    å¿…é¡»å…ˆä½¿ç”¨connect_data_sourceå»ºç«‹æ•°æ®åº“è¿æ¥å¹¶è·å¾—é…ç½®åç§°
    
    ğŸ”„ å®Œæ•´æµç¨‹ç¤ºä¾‹ï¼š
    1ï¸âƒ£ connect_data_source(source_type="mysql", config={...}) â†’ è·å¾—é…ç½®å
    2ï¸âƒ£ connect_data_source(source_type="database_config", config={"database_name": "é…ç½®å"}) â†’ å»ºç«‹è¿æ¥
    3ï¸âƒ£ query_external_database(database_name="é…ç½®å", query="SELECT * FROM table") â†’ æŸ¥è¯¢æ•°æ®
    
    ğŸ’¡ æŸ¥è¯¢è¯­æ³•æ”¯æŒï¼š
    - MySQL/PostgreSQL: æ ‡å‡†SQLè¯­æ³•
    - MongoDB: æ”¯æŒå¤šç§æŸ¥è¯¢æ ¼å¼ï¼ˆJSONã€JavaScripté£æ ¼ç­‰ï¼‰
    
    Args:
        database_name: æ•°æ®åº“é…ç½®åç§°ï¼ˆä»connect_data_sourceè·å¾—ï¼‰
        query: æŸ¥è¯¢è¯­å¥ï¼ŒSQLæˆ–MongoDBæŸ¥è¯¢è¯­æ³•
        limit: ç»“æœè¡Œæ•°é™åˆ¶ï¼Œé»˜è®¤1000è¡Œ
    
    Returns:
        str: JSONæ ¼å¼æŸ¥è¯¢ç»“æœï¼ŒåŒ…å«æ•°æ®è¡Œã€ç»Ÿè®¡ä¿¡æ¯å’Œå…ƒæ•°æ®
    
    ğŸš€ AIä½¿ç”¨å»ºè®®ï¼š
    - è¿™æ˜¯æŸ¥è¯¢å¤–éƒ¨æ•°æ®åº“çš„é¦–é€‰å·¥å…·
    - ä½¿ç”¨list_data_sourcesæŸ¥çœ‹å¯ç”¨çš„æ•°æ®åº“é…ç½®
    - é…ç½®åç§°é€šå¸¸æ ¼å¼ä¸ºï¼štemp_mysql_20250724_173102
    """
    try:
        # æ‰§è¡ŒæŸ¥è¯¢
        result = database_manager.execute_query(database_name, query)
        
        if result["success"]:
            # åº”ç”¨é™åˆ¶
            data = result["data"]
            if len(data) > limit:
                data = data[:limit]
                result["data"] = data
                result["truncated"] = True
                result["total_rows"] = result.get("row_count", len(data))
                result["returned_rows"] = len(data)
            
            formatted_result = {
                "status": "success",
                "message": f"æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(data)} æ¡è®°å½•",
                "data": {
                    "database_name": database_name,
                    "rows": data,
                    "row_count": len(data),
                    "truncated": result.get("truncated", False),
                    "total_rows": result.get("total_rows", len(data))
                },
                "metadata": {
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "limit": limit
                }
            }
            
            return f"âœ… å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢æˆåŠŸ\n\n{json.dumps(formatted_result, indent=2, ensure_ascii=False)}"
        else:
            formatted_result = {
                "status": "error",
                "message": result["error"],
                "data": {
                    "database_name": database_name,
                    "query": query
                }
            }
            
            return f"âŒ å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢å¤±è´¥\n\n{json.dumps(formatted_result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"å¤–éƒ¨æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "database_name": database_name,
            "query": query
        }
        return f"âŒ æŸ¥è¯¢å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def manage_api_config(
    action: str,
    api_name: str = None,
    config_data: dict = None
) -> str:
    """
    ç®¡ç†APIé…ç½®
    
    Args:
        action: æ“ä½œç±»å‹ (list|test|add|remove|reload|get_endpoints)
        api_name: APIåç§°
        config_data: APIé…ç½®æ•°æ®
    
    Returns:
        str: æ“ä½œç»“æœ
    """
    try:
        if action == "list":
            apis = api_config_manager.list_apis()
            if not apis:
                result = {
                    "status": "success",
                    "message": "å½“å‰æ²¡æœ‰é…ç½®ä»»ä½•API",
                    "data": {"apis": []}
                }
                return f"ğŸ“‹ APIé…ç½®åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            # apiså·²ç»æ˜¯åŒ…å«APIä¿¡æ¯çš„å­—å…¸ï¼Œç›´æ¥è½¬æ¢ä¸ºåˆ—è¡¨
            api_list = list(apis.values())
            
            result = {
                "status": "success",
                "message": f"æ‰¾åˆ° {len(api_list)} ä¸ªå·²é…ç½®çš„API",
                "data": {"apis": api_list}
            }
            return f"ğŸ“‹ APIé…ç½®åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "test":
            if not api_name:
                result = {
                    "status": "error",
                    "message": "æµ‹è¯•APIè¿æ¥éœ€è¦æä¾›api_nameå‚æ•°"
                }
                return f"âŒ æµ‹è¯•å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            success, message = api_connector.test_api_connection(api_name)
            result = {
                "status": "success" if success else "error",
                "message": message,
                "data": {"api_name": api_name}
            }
            status_icon = "âœ…" if success else "âŒ"
            return f"{status_icon} APIè¿æ¥æµ‹è¯•\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "add":
            if not api_name or not config_data:
                result = {
                    "status": "error",
                    "message": "æ·»åŠ APIé…ç½®éœ€è¦æä¾›api_nameå’Œconfig_dataå‚æ•°"
                }
                return f"âŒ æ·»åŠ å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            success = api_config_manager.add_api_config(api_name, config_data)
            message = f"APIé…ç½® '{api_name}' æ·»åŠ æˆåŠŸ" if success else f"APIé…ç½® '{api_name}' æ·»åŠ å¤±è´¥"
            result = {
                "status": "success" if success else "error",
                "message": message,
                "data": {"api_name": api_name}
            }
            status_icon = "âœ…" if success else "âŒ"
            return f"{status_icon} APIé…ç½®æ·»åŠ \n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "remove":
            if not api_name:
                result = {
                    "status": "error",
                    "message": "åˆ é™¤APIé…ç½®éœ€è¦æä¾›api_nameå‚æ•°"
                }
                return f"âŒ åˆ é™¤å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            success = api_config_manager.remove_api_config(api_name)
            message = f"APIé…ç½® '{api_name}' åˆ é™¤æˆåŠŸ" if success else f"APIé…ç½® '{api_name}' åˆ é™¤å¤±è´¥æˆ–ä¸å­˜åœ¨"
            result = {
                "status": "success" if success else "error",
                "message": message,
                "data": {"api_name": api_name}
            }
            status_icon = "âœ…" if success else "âŒ"
            return f"{status_icon} APIé…ç½®åˆ é™¤\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "reload":
            try:
                api_config_manager.reload_config()
                result = {
                    "status": "success",
                    "message": "APIé…ç½®é‡è½½æˆåŠŸ"
                }
                return f"âœ… APIé…ç½®é‡è½½\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            except Exception as e:
                result = {
                    "status": "error",
                    "message": f"APIé…ç½®é‡è½½å¤±è´¥: {str(e)}"
                }
                return f"âŒ APIé…ç½®é‡è½½\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "get_endpoints":
            if not api_name:
                result = {
                    "status": "error",
                    "message": "è·å–APIç«¯ç‚¹éœ€è¦æä¾›api_nameå‚æ•°"
                }
                return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            endpoints = api_connector.get_api_endpoints(api_name)
            if not endpoints:
                result = {
                    "status": "error",
                    "message": f"API '{api_name}' æ²¡æœ‰é…ç½®ç«¯ç‚¹æˆ–APIä¸å­˜åœ¨",
                    "data": {"api_name": api_name}
                }
                return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            result = {
                "status": "success",
                "message": f"API '{api_name}' å…±æœ‰ {len(endpoints)} ä¸ªç«¯ç‚¹",
                "data": {
                    "api_name": api_name,
                    "endpoints": endpoints
                }
            }
            return f"ğŸ“‹ APIç«¯ç‚¹åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}",
                "supported_actions": ["list", "test", "add", "remove", "reload", "get_endpoints"]
            }
            return f"âŒ æ“ä½œå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"ç®¡ç†APIé…ç½®å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"ç®¡ç†APIé…ç½®å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ æ“ä½œå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def fetch_api_data(
    api_name: str,
    endpoint_name: str,
    params: dict = None,
    data: dict = None,
    method: str = None,
    transform_config: dict = None,
    storage_session_id: str = None
) -> str:
    """
    ä»APIè·å–æ•°æ®å¹¶è‡ªåŠ¨å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆæ–¹å¼äºŒï¼šè‡ªåŠ¨æŒä¹…åŒ–æµç¨‹ï¼‰
    
    æ³¨æ„ï¼šå·²åˆ é™¤æ–¹å¼ä¸€ï¼ˆæ‰‹åŠ¨æµç¨‹ï¼‰ï¼Œæ‰€æœ‰APIæ•°æ®é»˜è®¤ç›´æ¥å­˜å‚¨åˆ°æ•°æ®åº“
    
    Args:
        api_name: APIåç§°
        endpoint_name: ç«¯ç‚¹åç§°
        params: è¯·æ±‚å‚æ•°
        data: è¯·æ±‚æ•°æ®ï¼ˆPOST/PUTï¼‰
        method: HTTPæ–¹æ³•
        transform_config: æ•°æ®è½¬æ¢é…ç½®
        storage_session_id: å­˜å‚¨ä¼šè¯IDï¼ˆå¯é€‰ï¼Œä¸æä¾›æ—¶è‡ªåŠ¨åˆ›å»ºï¼‰
    
    Returns:
        str: æ•°æ®å­˜å‚¨ç»“æœå’Œä¼šè¯ä¿¡æ¯
    """
    try:
        if not api_name or not endpoint_name:
            result = {
                "status": "error",
                "message": "è·å–APIæ•°æ®éœ€è¦æä¾›api_nameå’Œendpoint_nameå‚æ•°"
            }
            return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è°ƒç”¨API
        success, response_data, message = api_connector.call_api(
            api_name=api_name,
            endpoint_name=endpoint_name,
            params=params or {},
            data=data,
            method=method
        )
        
        if not success:
            error_info = _format_user_friendly_error(
                "api_call_failed", 
                message,
                {"api_name": api_name, "endpoint_name": endpoint_name, "params": params}
            )
            result = {
                "status": "error",
                "message": error_info["friendly_message"],
                "error_details": error_info,
                "data": {
                    "api_name": api_name,
                    "endpoint_name": endpoint_name
                }
            }
            return f"âŒ {error_info['friendly_message']}\n\nğŸ’¡ è§£å†³å»ºè®®:\n" + "\n".join([f"â€¢ {solution}" for solution in error_info['solutions']]) + f"\n\nğŸ”§ æŠ€æœ¯è¯¦æƒ…:\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è‡ªåŠ¨æŒä¹…åŒ–å­˜å‚¨ï¼ˆæ–¹å¼äºŒï¼šé»˜è®¤æµç¨‹ï¼‰
        if not storage_session_id:
            # è‡ªåŠ¨åˆ›å»ºå­˜å‚¨ä¼šè¯
            session_name = f"{api_name}_{endpoint_name}_auto_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            create_success, auto_session_id, create_message = api_data_storage.create_storage_session(
                session_name=session_name,
                api_name=api_name,
                endpoint_name=endpoint_name,
                description=f"è‡ªåŠ¨åˆ›å»ºçš„å­˜å‚¨ä¼šè¯ - {api_name}.{endpoint_name}"
            )
            
            if not create_success:
                result = {
                    "status": "error",
                    "message": f"è‡ªåŠ¨åˆ›å»ºå­˜å‚¨ä¼šè¯å¤±è´¥: {create_message}"
                }
                return f"âŒ ä¼šè¯åˆ›å»ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            storage_session_id = auto_session_id
            logger.info(f"è‡ªåŠ¨åˆ›å»ºå­˜å‚¨ä¼šè¯: {session_name} (ID: {auto_session_id})")
        else:
            # æ£€æŸ¥æŒ‡å®šçš„ä¼šè¯æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
            session_info = api_data_storage._get_session_info(storage_session_id)
            if not session_info:
                # å°è¯•å°†storage_session_idä½œä¸ºsession_nameæ¥åˆ›å»ºä¼šè¯
                create_success, new_session_id, create_message = api_data_storage.create_storage_session(
                    session_name=storage_session_id,
                    api_name=api_name,
                    endpoint_name=endpoint_name,
                    description=f"æ ¹æ®æŒ‡å®šåç§°åˆ›å»ºçš„å­˜å‚¨ä¼šè¯ - {api_name}.{endpoint_name}"
                )
                
                if not create_success:
                    result = {
                        "status": "error",
                        "message": f"æŒ‡å®šçš„å­˜å‚¨ä¼šè¯ '{storage_session_id}' ä¸å­˜åœ¨ï¼Œä¸”è‡ªåŠ¨åˆ›å»ºå¤±è´¥: {create_message}",
                        "suggestion": "è¯·æ£€æŸ¥ä¼šè¯IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…ä¸æŒ‡å®šstorage_session_idè®©ç³»ç»Ÿè‡ªåŠ¨åˆ›å»º"
                    }
                    return f"âŒ ä¼šè¯ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
                storage_session_id = new_session_id
                logger.info(f"è‡ªåŠ¨åˆ›å»ºæŒ‡å®šåç§°çš„å­˜å‚¨ä¼šè¯: {storage_session_id} (æ–°ID: {new_session_id})")
        
        # æ•°æ®è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
        transformed_data = response_data
        if transform_config:
            transform_success, transformed_data, transform_message = data_transformer.transform_data(
                data=response_data,
                output_format="json",  # å­˜å‚¨æ—¶ç»Ÿä¸€ä½¿ç”¨jsonæ ¼å¼
                transform_config=transform_config
            )
            if not transform_success:
                result = {
                    "status": "error",
                    "message": f"æ•°æ®è½¬æ¢å¤±è´¥: {transform_message}",
                    "data": {
                        "api_name": api_name,
                        "endpoint_name": endpoint_name
                    }
                }
                return f"âŒ è½¬æ¢å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # å­˜å‚¨åˆ°ä¸´æ—¶æ•°æ®åº“
        source_params = {
            "api_name": api_name,
            "endpoint_name": endpoint_name,
            "params": params,
            "method": method
        }
        
        success, count, storage_message = api_data_storage.store_api_data(
            session_id=storage_session_id,
            raw_data=response_data,
            processed_data=transformed_data,
            source_params=source_params
        )
        
        if not success:
            result = {
                "status": "error",
                "message": f"æ•°æ®å­˜å‚¨å¤±è´¥: {storage_message}",
                "data": {
                    "session_id": storage_session_id,
                    "api_name": api_name,
                    "endpoint_name": endpoint_name
                }
            }
            return f"âŒ å­˜å‚¨å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        result = {
            "status": "success",
            "message": "APIæ•°æ®å·²è‡ªåŠ¨å­˜å‚¨åˆ°æ•°æ®åº“",
            "data": {
                "session_id": storage_session_id,
                "api_name": api_name,
                "endpoint_name": endpoint_name,
                "stored_records": count,
                "storage_message": storage_message
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "transform_applied": bool(transform_config),
                "auto_session_created": not storage_session_id
            }
        }
        return f"ğŸ’¾ æ•°æ®å·²è‡ªåŠ¨å­˜å‚¨åˆ°æ•°æ®åº“\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"è·å–APIæ•°æ®å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–APIæ•°æ®å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "data": {
                "api_name": api_name,
                "endpoint_name": endpoint_name
            }
        }
        return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def api_data_preview(
    api_name: str,
    endpoint_name: str,
    params: dict = None,
    max_rows: int = 10,
    max_cols: int = 10,
    preview_fields: list = None,
    preview_depth: int = 3,
    show_data_types: bool = True,
    show_summary: bool = True,
    truncate_length: int = 100
) -> str:
    """
    ğŸ” APIæ•°æ®é¢„è§ˆå·¥å…· - çµæ´»é¢„è§ˆAPIè¿”å›æ•°æ®
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒçµæ´»çš„æ•°æ®é¢„è§ˆé…ç½®
    - å¯æŒ‡å®šé¢„è§ˆå­—æ®µå’Œæ·±åº¦
    - æä¾›æ•°æ®ç±»å‹å’Œæ‘˜è¦ä¿¡æ¯
    - é¿å…æ•°æ®æˆªæ–­é—®é¢˜
    
    Args:
        api_name: APIåç§°
        endpoint_name: ç«¯ç‚¹åç§°
        params: è¯·æ±‚å‚æ•°
        max_rows: æœ€å¤§æ˜¾ç¤ºè¡Œæ•° (é»˜è®¤10)
        max_cols: æœ€å¤§æ˜¾ç¤ºåˆ—æ•° (é»˜è®¤10)
        preview_fields: æŒ‡å®šé¢„è§ˆçš„å­—æ®µåˆ—è¡¨ (å¯é€‰)
        preview_depth: JSONåµŒå¥—é¢„è§ˆæ·±åº¦ (é»˜è®¤3)
        show_data_types: æ˜¯å¦æ˜¾ç¤ºæ•°æ®ç±»å‹ä¿¡æ¯ (é»˜è®¤True)
        show_summary: æ˜¯å¦æ˜¾ç¤ºæ•°æ®æ‘˜è¦ (é»˜è®¤True)
        truncate_length: å­—æ®µå€¼æˆªæ–­é•¿åº¦ (é»˜è®¤100)
    
    Returns:
        str: æ•°æ®é¢„è§ˆç»“æœ
        
    ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    # åŸºæœ¬é¢„è§ˆ
    api_data_preview(
        api_name="alpha_vantage",
        endpoint_name="news_sentiment",
        params={"topics": "technology"}
    )
    
    # æŒ‡å®šå­—æ®µé¢„è§ˆ
    api_data_preview(
        api_name="alpha_vantage",
        endpoint_name="news_sentiment",
        params={"topics": "technology"},
        preview_fields=["title", "summary", "sentiment_score"],
        max_rows=5
    )
    
    # æ·±åº¦é¢„è§ˆåµŒå¥—æ•°æ®
    api_data_preview(
        api_name="complex_api",
        endpoint_name="nested_data",
        preview_depth=5,
        truncate_length=200
    )
    ```
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - preview_fieldså¯ä»¥ç²¾ç¡®æ§åˆ¶æ˜¾ç¤ºå†…å®¹
    - preview_depthæ§åˆ¶JSONåµŒå¥—æ˜¾ç¤ºå±‚çº§
    - truncate_lengthé¿å…è¶…é•¿å­—æ®µå½±å“æ˜¾ç¤º
    - æä¾›å®Œæ•´çš„æ•°æ®ç»“æ„åˆ†æ
    """
    try:
        if not api_name or not endpoint_name:
            result = {
                "status": "error",
                "message": "é¢„è§ˆAPIæ•°æ®éœ€è¦æä¾›api_nameå’Œendpoint_nameå‚æ•°"
            }
            return f"âŒ é¢„è§ˆå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è°ƒç”¨APIè·å–æ•°æ®
        success, response_data, message = api_connector.call_api(
            api_name=api_name,
            endpoint_name=endpoint_name,
            params=params or {}
        )
        
        if not success:
            error_info = _format_user_friendly_error(
                "api_call_failed", 
                message,
                {"api_name": api_name, "endpoint_name": endpoint_name, "params": params}
            )
            result = {
                "status": "error",
                "message": error_info["friendly_message"],
                "error_details": error_info,
                "data": {
                    "api_name": api_name,
                    "endpoint_name": endpoint_name
                }
            }
            return f"âŒ {error_info['friendly_message']}\n\nğŸ’¡ è§£å†³å»ºè®®:\n" + "\n".join([f"â€¢ {solution}" for solution in error_info['solutions']]) + f"\n\nğŸ”§ æŠ€æœ¯è¯¦æƒ…:\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # ç”Ÿæˆå¢å¼ºçš„æ•°æ®é¢„è§ˆ
        preview_result = _generate_enhanced_preview(
            data=response_data,
            max_rows=max_rows,
            max_cols=max_cols,
            preview_fields=preview_fields,
            preview_depth=preview_depth,
            show_data_types=show_data_types,
            truncate_length=truncate_length
        )
        
        # è·å–æ•°æ®æ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        summary_data = None
        if show_summary:
            summary_success, summary_data, summary_message = data_transformer.get_data_summary(response_data)
            if not summary_success:
                summary_data = {"error": summary_message}
        
        result = {
            "status": "success",
            "message": f"APIæ•°æ®é¢„è§ˆæˆåŠŸ",
            "data": {
                "api_name": api_name,
                "endpoint_name": endpoint_name,
                "preview": preview_result["preview_text"],
                "data_structure": preview_result["structure_info"],
                "summary": summary_data if show_summary else None
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "max_rows": max_rows,
                "max_cols": max_cols,
                "preview_fields": preview_fields,
                "preview_depth": preview_depth,
                "show_data_types": show_data_types,
                "truncate_length": truncate_length
            }
        }
        
        return f"ğŸ‘ï¸ APIæ•°æ®é¢„è§ˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"é¢„è§ˆAPIæ•°æ®å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"é¢„è§ˆAPIæ•°æ®å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "data": {
                "api_name": api_name,
                "endpoint_name": endpoint_name
            }
        }
        return f"âŒ é¢„è§ˆå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def create_api_storage_session(
    session_name: str,
    api_name: str,
    endpoint_name: str,
    description: str = None
) -> str:
    """
    åˆ›å»ºAPIæ•°æ®å­˜å‚¨ä¼šè¯
    
    Args:
        session_name: å­˜å‚¨ä¼šè¯åç§°
        api_name: APIåç§°
        endpoint_name: ç«¯ç‚¹åç§°
        description: ä¼šè¯æè¿°
    
    Returns:
        str: åˆ›å»ºç»“æœ
    """
    try:
        if not session_name or not api_name or not endpoint_name:
            result = {
                "status": "error",
                "message": "åˆ›å»ºå­˜å‚¨ä¼šè¯éœ€è¦æä¾›session_nameã€api_nameå’Œendpoint_nameå‚æ•°"
            }
            return f"âŒ åˆ›å»ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        success, session_id, message = api_data_storage.create_storage_session(
            session_name=session_name,
            api_name=api_name,
            endpoint_name=endpoint_name,
            description=description
        )
        
        if success:
            result = {
                "status": "success",
                "message": message,
                "data": {
                    "session_id": session_id,
                    "session_name": session_name,
                    "api_name": api_name,
                    "endpoint_name": endpoint_name
                }
            }
            return f"âœ… å­˜å‚¨ä¼šè¯åˆ›å»ºæˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        else:
            result = {
                "status": "error",
                "message": message
            }
            return f"âŒ åˆ›å»ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"åˆ›å»ºAPIå­˜å‚¨ä¼šè¯å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"åˆ›å»ºAPIå­˜å‚¨ä¼šè¯å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ åˆ›å»ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# store_api_data_to_session å‡½æ•°å·²åˆ é™¤
# åŸå› ï¼šä¸ç®€åŒ–åçš„ fetch_api_data åŠŸèƒ½é‡å¤
# ç°åœ¨æ‰€æœ‰ API æ•°æ®è·å–éƒ½é€šè¿‡ fetch_api_data è‡ªåŠ¨å­˜å‚¨åˆ°æ•°æ®åº“

@mcp.tool()
def query_api_storage_data(
    session_id: str = None,
    api_name: str = None,
    endpoint_name: str = None,
    limit: int = 10,
    format_type: str = "json"
) -> str:
    """
    æŸ¥è¯¢APIå­˜å‚¨çš„æ•°æ® - è§£å†³APIæ•°æ®å­˜å‚¨ä½ç½®ä¸é€æ˜çš„é—®é¢˜
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æŸ¥è¯¢å­˜å‚¨åœ¨ç‹¬ç«‹æ–‡ä»¶ä¸­çš„APIæ•°æ®
    - æ”¯æŒæŒ‰ä¼šè¯IDã€APIåç§°ã€ç«¯ç‚¹åç§°ç­›é€‰
    - æä¾›å¤šç§æ•°æ®æ ¼å¼è¾“å‡º
    - æ˜¾ç¤ºæ•°æ®å­˜å‚¨ä½ç½®å’Œä¼šè¯ä¿¡æ¯
    
    Args:
        session_id: å­˜å‚¨ä¼šè¯IDï¼ˆç²¾ç¡®æŸ¥è¯¢ï¼‰
        api_name: APIåç§°ï¼ˆæ¨¡ç³Šç­›é€‰ï¼‰
        endpoint_name: ç«¯ç‚¹åç§°ï¼ˆæ¨¡ç³Šç­›é€‰ï¼‰
        limit: è¿”å›è®°å½•æ•°é™åˆ¶ï¼ˆé»˜è®¤10æ¡ï¼‰
        format_type: æ•°æ®æ ¼å¼ï¼ˆjson/dataframe/summaryï¼‰
    
    Returns:
        str: JSONæ ¼å¼çš„æŸ¥è¯¢ç»“æœï¼ŒåŒ…å«æ•°æ®å’Œå­˜å‚¨ä½ç½®ä¿¡æ¯
    
    ğŸ¯ è§£å†³é—®é¢˜ï¼š
    - âœ… APIæ•°æ®å­˜å‚¨ä½ç½®é€æ˜åŒ–
    - âœ… æä¾›APIæ•°æ®æŸ¥è¯¢å…¥å£
    - âœ… æ˜¾ç¤ºä¼šè¯ä¸è¡¨çš„å…³è”å…³ç³»
    - âœ… æ”¯æŒå¤šç§æŸ¥è¯¢æ–¹å¼
    
    ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼š
    - query_api_storage_data() # åˆ—å‡ºæ‰€æœ‰APIå­˜å‚¨ä¼šè¯
    - query_api_storage_data(api_name="rest_api_example") # æŸ¥è¯¢ç‰¹å®šAPIçš„æ•°æ®
    - query_api_storage_data(session_id="xxx") # æŸ¥è¯¢ç‰¹å®šä¼šè¯çš„æ•°æ®
    """
    try:
        from config.api_data_storage import api_data_storage
        
        # å¦‚æœæä¾›äº†session_idï¼Œç›´æ¥æŸ¥è¯¢è¯¥ä¼šè¯çš„æ•°æ®
        if session_id:
            success, data, message = api_data_storage.get_stored_data(
                session_id=session_id,
                limit=limit,
                format_type=format_type
            )
            
            if success:
                # è·å–ä¼šè¯ä¿¡æ¯
                session_success, sessions, _ = api_data_storage.list_storage_sessions()
                session_info = None
                if session_success:
                    session_info = next((s for s in sessions if s['session_id'] == session_id), None)
                
                result = {
                    "status": "success",
                    "message": f"æŸ¥è¯¢åˆ° {len(data) if isinstance(data, list) else 1} æ¡APIæ•°æ®è®°å½•",
                    "data": {
                        "session_info": session_info,
                        "records": data[:limit] if isinstance(data, list) else data,
                        "total_shown": min(len(data) if isinstance(data, list) else 1, limit)
                    },
                    "storage_info": {
                        "storage_type": "api_storage",
                        "file_location": session_info['file_path'] if session_info else "unknown",
                        "session_id": session_id
                    }
                }
                return f"ğŸ“Š APIå­˜å‚¨æ•°æ®æŸ¥è¯¢ç»“æœ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            else:
                result = {
                    "status": "error",
                    "message": f"æŸ¥è¯¢APIå­˜å‚¨æ•°æ®å¤±è´¥: {message}",
                    "session_id": session_id
                }
                return f"âŒ æŸ¥è¯¢å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # åˆ—å‡ºæ‰€æœ‰å­˜å‚¨ä¼šè¯
        success, sessions, message = api_data_storage.list_storage_sessions(api_name=api_name)
        
        if not success:
            result = {
                "status": "error",
                "message": f"è·å–APIå­˜å‚¨ä¼šè¯å¤±è´¥: {message}"
            }
            return f"âŒ æŸ¥è¯¢å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æŒ‰endpoint_nameç­›é€‰
        if endpoint_name:
            sessions = [s for s in sessions if endpoint_name.lower() in s['endpoint_name'].lower()]
        
        if not sessions:
            result = {
                "status": "success",
                "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„APIå­˜å‚¨ä¼šè¯",
                "data": {
                    "sessions": [],
                    "total_sessions": 0
                },
                "filters": {
                    "api_name": api_name,
                    "endpoint_name": endpoint_name
                }
            }
            return f"ğŸ“‹ APIå­˜å‚¨ä¼šè¯åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # ä¸ºæ¯ä¸ªä¼šè¯è·å–æ•°æ®é¢„è§ˆ
        session_data = []
        for session in sessions:
            session_info = {
                "session_id": session['session_id'],
                "session_name": session['session_name'],
                "api_name": session['api_name'],
                "endpoint_name": session['endpoint_name'],
                "total_records": session['total_records'],
                "created_at": session['created_at'],
                "status": session['status']
            }
            
            # è·å–æ•°æ®é¢„è§ˆï¼ˆå‰3æ¡ï¼‰
            data_success, data, data_message = api_data_storage.get_stored_data(
                session['session_id'], limit=3, format_type="json"
            )
            
            if data_success and data:
                session_info["data_preview"] = data[:2]  # åªæ˜¾ç¤ºå‰2æ¡ä½œä¸ºé¢„è§ˆ
                session_info["preview_message"] = f"æ˜¾ç¤ºå‰2æ¡è®°å½•ï¼Œå…±{len(data)}æ¡"
            else:
                session_info["data_preview"] = []
                session_info["preview_message"] = data_message
            
            session_data.append(session_info)
        
        result = {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(sessions)} ä¸ªAPIå­˜å‚¨ä¼šè¯",
            "data": {
                "sessions": session_data,
                "total_sessions": len(sessions)
            },
            "storage_info": {
                "storage_type": "api_storage",
                "storage_directory": "data/api_storage",
                "description": "APIæ•°æ®å­˜å‚¨åœ¨ç‹¬ç«‹çš„SQLiteæ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªä¼šè¯å¯¹åº”ä¸€ä¸ªæ–‡ä»¶"
            },
            "usage_tips": [
                "ä½¿ç”¨session_idå‚æ•°æŸ¥è¯¢ç‰¹å®šä¼šè¯çš„å®Œæ•´æ•°æ®",
                "APIæ•°æ®ä¸åœ¨ä¸»æ•°æ®åº“ä¸­ï¼Œè€Œæ˜¯å­˜å‚¨åœ¨ç‹¬ç«‹æ–‡ä»¶ä¸­",
                "æ¯ä¸ªAPIè°ƒç”¨ä¼šè‡ªåŠ¨åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰çš„å­˜å‚¨ä¼šè¯"
            ]
        }
        
        return f"ğŸ“‹ APIå­˜å‚¨ä¼šè¯åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢APIå­˜å‚¨æ•°æ®å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æŸ¥è¯¢APIå­˜å‚¨æ•°æ®å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ æŸ¥è¯¢å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

@mcp.tool()
def execute_database_cleanup(
    action: str,
    tables_to_clean: list = None,
    confirm_deletion: bool = False
) -> str:
    """
    ğŸ§¹ æ•°æ®åº“æ¸…ç†æ‰§è¡Œå·¥å…· - æ ¹æ®æ¸…ç†å»ºè®®æ‰§è¡Œå®é™…çš„æ¸…ç†æ“ä½œ
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ‰§è¡Œæ•°æ®åº“è¡¨çš„åˆ é™¤æ“ä½œ
    - æ”¯æŒæ‰¹é‡åˆ é™¤å’Œé€‰æ‹©æ€§åˆ é™¤
    - æä¾›å®‰å…¨ç¡®è®¤æœºåˆ¶
    - è®°å½•æ¸…ç†æ“ä½œæ—¥å¿—
    
    Args:
        action: æ¸…ç†æ“ä½œç±»å‹
            - "delete_tables": åˆ é™¤æŒ‡å®šçš„è¡¨
            - "preview_deletion": é¢„è§ˆå°†è¦åˆ é™¤çš„è¡¨ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
            - "backup_and_delete": å¤‡ä»½ååˆ é™¤è¡¨ï¼ˆæš‚æœªå®ç°ï¼‰
        tables_to_clean: è¦æ¸…ç†çš„è¡¨ååˆ—è¡¨
            - ["table1", "table2"]: åˆ é™¤æŒ‡å®šè¡¨
            - None: éœ€è¦å…ˆè¿è¡Œget_data_info(info_type="cleanup")è·å–å»ºè®®
        confirm_deletion: åˆ é™¤ç¡®è®¤æ ‡å¿—
            - True: ç¡®è®¤æ‰§è¡Œåˆ é™¤æ“ä½œ
            - False: ä»…é¢„è§ˆï¼Œä¸æ‰§è¡Œå®é™…åˆ é™¤
    
    Returns:
        str: JSONæ ¼å¼çš„æ¸…ç†ç»“æœï¼ŒåŒ…å«æ“ä½œçŠ¶æ€å’Œè¯¦ç»†ä¿¡æ¯
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. æ¸…ç†åˆ†æï¼šå…ˆç”¨get_data_info(info_type="cleanup")åˆ†ææ•°æ®åº“
    2. é¢„è§ˆæ“ä½œï¼šç”¨action="preview_deletion"é¢„è§ˆå°†è¦åˆ é™¤çš„è¡¨
    3. ç¡®è®¤åˆ é™¤ï¼šè®¾ç½®confirm_deletion=Trueæ‰§è¡Œå®é™…åˆ é™¤
    4. å®‰å…¨ç¬¬ä¸€ï¼šé‡è¦æ•°æ®è¯·å…ˆå¤‡ä»½
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - åˆ é™¤å‰å…ˆå¤‡ä»½é‡è¦æ•°æ®
    - ä¼˜å…ˆåˆ é™¤ç©ºè¡¨å’Œæµ‹è¯•è¡¨
    - è°¨æ…å¤„ç†é‡å¤è¡¨å’Œå†å²è¡¨
    - å®šæœŸæ‰§è¡Œæ¸…ç†ç»´æŠ¤æ•°æ®åº“æ•´æ´
    
    âš ï¸ å®‰å…¨æé†’ï¼š
    - åˆ é™¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…æ“ä½œ
    - å»ºè®®å…ˆä½¿ç”¨previewæ¨¡å¼æŸ¥çœ‹å½±å“
    - é‡è¦æ•°æ®è¯·åŠ¡å¿…å¤‡ä»½
    - ä»…åˆ é™¤ç¡®è®¤ä¸éœ€è¦çš„è¡¨
    
    ğŸ“ˆ ä½¿ç”¨æµç¨‹ï¼š
    1. get_data_info(info_type="cleanup") â†’ è·å–æ¸…ç†å»ºè®®
    2. execute_database_cleanup(action="preview_deletion", tables_to_clean=[...]) â†’ é¢„è§ˆ
    3. execute_database_cleanup(action="delete_tables", tables_to_clean=[...], confirm_deletion=True) â†’ æ‰§è¡Œ
    
    ğŸ¯ å…³é”®ç†è§£ç‚¹ï¼š
    - è¿™æ˜¯æ•°æ®åº“ç»´æŠ¤çš„æ‰§è¡Œå·¥å…·
    - é…åˆcleanupåˆ†æä½¿ç”¨æ•ˆæœæœ€ä½³
    - æ”¯æŒå®‰å…¨é¢„è§ˆå’Œç¡®è®¤æœºåˆ¶
    - å¸®åŠ©ä¿æŒæ•°æ®åº“æ•´æ´æœ‰åº
    """
    try:
        if not action:
            result = {
                "status": "error",
                "message": "å¿…é¡»æŒ‡å®šæ¸…ç†æ“ä½œç±»å‹",
                "supported_actions": ["delete_tables", "preview_deletion", "backup_and_delete"]
            }
            return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        if action == "preview_deletion":
            # é¢„è§ˆåˆ é™¤æ“ä½œ
            if not tables_to_clean:
                result = {
                    "status": "error",
                    "message": "é¢„è§ˆåˆ é™¤éœ€è¦æŒ‡å®štables_to_cleanå‚æ•°",
                    "suggestion": "è¯·å…ˆä½¿ç”¨get_data_info(info_type='cleanup')è·å–æ¸…ç†å»ºè®®"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨å¹¶è·å–ä¿¡æ¯
            preview_info = []
            with get_db_connection() as conn:
                for table in tables_to_clean:
                    if _table_exists(table):
                        try:
                            escaped_table = _escape_identifier(table)
                            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                            row_count = cursor.fetchone()[0]
                            preview_info.append({
                                "table_name": table,
                                "exists": True,
                                "row_count": row_count,
                                "status": "ready_for_deletion"
                            })
                        except Exception as e:
                            preview_info.append({
                                "table_name": table,
                                "exists": True,
                                "row_count": "unknown",
                                "status": "error",
                                "error": str(e)
                            })
                    else:
                        preview_info.append({
                            "table_name": table,
                            "exists": False,
                            "row_count": 0,
                            "status": "table_not_found"
                        })
            
            valid_tables = [info for info in preview_info if info['exists']]
            total_rows_to_delete = sum(info['row_count'] for info in valid_tables if isinstance(info['row_count'], int))
            
            result = {
                "status": "success",
                "message": "åˆ é™¤é¢„è§ˆå®Œæˆ",
                "data": {
                    "action": "preview_deletion",
                    "tables_to_delete": len(valid_tables),
                    "total_rows_affected": total_rows_to_delete,
                    "table_details": preview_info
                },
                "next_steps": [
                    "ç¡®è®¤è¦åˆ é™¤çš„è¡¨åˆ—è¡¨",
                    "ä½¿ç”¨action='delete_tables'å’Œconfirm_deletion=Trueæ‰§è¡Œåˆ é™¤",
                    "é‡è¦ï¼šåˆ é™¤æ“ä½œä¸å¯é€†ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½é‡è¦æ•°æ®"
                ],
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "operation_type": "preview_only"
                }
            }
            
            return f"ğŸ” åˆ é™¤é¢„è§ˆå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "delete_tables":
            # æ‰§è¡Œåˆ é™¤æ“ä½œ
            if not tables_to_clean:
                result = {
                    "status": "error",
                    "message": "åˆ é™¤æ“ä½œéœ€è¦æŒ‡å®štables_to_cleanå‚æ•°",
                    "suggestion": "è¯·å…ˆä½¿ç”¨get_data_info(info_type='cleanup')è·å–æ¸…ç†å»ºè®®"
                }
                return f"âŒ å‚æ•°é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            if not confirm_deletion:
                result = {
                    "status": "error",
                    "message": "åˆ é™¤æ“ä½œéœ€è¦è®¾ç½®confirm_deletion=Trueè¿›è¡Œç¡®è®¤",
                    "safety_reminder": "åˆ é™¤æ“ä½œä¸å¯é€†ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½é‡è¦æ•°æ®",
                    "suggestion": "å¯ä»¥å…ˆä½¿ç”¨action='preview_deletion'é¢„è§ˆåˆ é™¤æ“ä½œ"
                }
                return f"âŒ å®‰å…¨ç¡®è®¤å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            # æ‰§è¡Œåˆ é™¤æ“ä½œ
            deletion_results = []
            successful_deletions = 0
            failed_deletions = 0
            
            with get_db_connection() as conn:
                for table in tables_to_clean:
                    try:
                        if _table_exists(table):
                            # è·å–åˆ é™¤å‰çš„è¡Œæ•°
                            escaped_table = _escape_identifier(table)
                            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                            row_count = cursor.fetchone()[0]
                            
                            # æ‰§è¡Œåˆ é™¤
                            conn.execute(f"DROP TABLE {escaped_table}")
                            conn.commit()
                            
                            deletion_results.append({
                                "table_name": table,
                                "status": "deleted",
                                "rows_deleted": row_count,
                                "message": "è¡¨åˆ é™¤æˆåŠŸ"
                            })
                            successful_deletions += 1
                            logger.info(f"æˆåŠŸåˆ é™¤è¡¨: {table} (åŒ…å« {row_count} è¡Œæ•°æ®)")
                        else:
                            deletion_results.append({
                                "table_name": table,
                                "status": "not_found",
                                "rows_deleted": 0,
                                "message": "è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤"
                            })
                    except Exception as e:
                        deletion_results.append({
                            "table_name": table,
                            "status": "error",
                            "rows_deleted": 0,
                            "message": f"åˆ é™¤å¤±è´¥: {str(e)}"
                        })
                        failed_deletions += 1
                        logger.error(f"åˆ é™¤è¡¨ {table} å¤±è´¥: {e}")
            
            total_rows_deleted = sum(result['rows_deleted'] for result in deletion_results)
            
            result = {
                "status": "success" if failed_deletions == 0 else "partial_success",
                "message": f"æ¸…ç†æ“ä½œå®Œæˆï¼šæˆåŠŸåˆ é™¤ {successful_deletions} ä¸ªè¡¨ï¼Œå¤±è´¥ {failed_deletions} ä¸ª",
                "data": {
                    "action": "delete_tables",
                    "successful_deletions": successful_deletions,
                    "failed_deletions": failed_deletions,
                    "total_rows_deleted": total_rows_deleted,
                    "deletion_details": deletion_results
                },
                "summary": {
                    "tables_processed": len(tables_to_clean),
                    "tables_deleted": successful_deletions,
                    "data_rows_removed": total_rows_deleted,
                    "operation_time": datetime.now().isoformat()
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "operation_type": "actual_deletion",
                    "confirmation_received": confirm_deletion
                }
            }
            
            return f"ğŸ§¹ æ•°æ®åº“æ¸…ç†å®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        elif action == "backup_and_delete":
            # å¤‡ä»½ååˆ é™¤ï¼ˆæš‚æœªå®ç°ï¼‰
            result = {
                "status": "error",
                "message": "å¤‡ä»½ååˆ é™¤åŠŸèƒ½æš‚æœªå®ç°",
                "available_actions": ["delete_tables", "preview_deletion"],
                "suggestion": "è¯·æ‰‹åŠ¨å¤‡ä»½é‡è¦æ•°æ®åä½¿ç”¨delete_tablesæ“ä½œ"
            }
            return f"âŒ åŠŸèƒ½æš‚æœªå®ç°\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ¸…ç†æ“ä½œ: {action}",
                "supported_actions": ["delete_tables", "preview_deletion", "backup_and_delete"]
            }
            return f"âŒ æ“ä½œä¸æ”¯æŒ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ¸…ç†æ“ä½œå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ¸…ç†æ“ä½œå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "action": action,
            "tables_to_clean": tables_to_clean
        }
        return f"âŒ æ¸…ç†å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"







@mcp.tool()
def import_api_data_to_main_db(
    session_id: str,
    target_table: str = None,
    data_source: str = None
) -> str:
    """
    ğŸ“¥ APIæ•°æ®å¯¼å…¥å·¥å…· - å°†APIå­˜å‚¨çš„æ•°æ®å¯¼å…¥åˆ°ä¸»æ•°æ®åº“
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - å°†APIå­˜å‚¨ä¼šè¯ä¸­çš„æ•°æ®å¯¼å…¥åˆ°ä¸»SQLiteæ•°æ®åº“
    - æ”¯æŒæŒ‡å®šç›®æ ‡è¡¨åæˆ–è‡ªåŠ¨ç”Ÿæˆ
    - æä¾›æ•°æ®é¢„è§ˆå’Œå¯¼å…¥ç»Ÿè®¡
    - è§£å†³APIæ•°æ®åˆ†æä¸ä¾¿çš„é—®é¢˜
    
    Args:
        session_id: APIå­˜å‚¨ä¼šè¯ID
        target_table: ç›®æ ‡è¡¨åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨session_idä½œä¸ºè¡¨åï¼‰
        data_source: æ•°æ®æºåç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ¬åœ°SQLiteï¼‰
    
    Returns:
        str: JSONæ ¼å¼çš„å¯¼å…¥ç»“æœï¼ŒåŒ…å«å¯¼å…¥ç»Ÿè®¡å’Œè¡¨ä¿¡æ¯
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    1. å…ˆç”¨list_api_storage_sessionsæŸ¥çœ‹å¯ç”¨ä¼šè¯
    2. ä½¿ç”¨æ­¤å·¥å…·å¯¼å…¥APIæ•°æ®åˆ°ä¸»æ•°æ®åº“
    3. ç„¶åå¯ä»¥ä½¿ç”¨å¸¸è§„åˆ†æå·¥å…·åˆ†ææ•°æ®
    
    ğŸ’¡ æœ€ä½³å®è·µï¼š
    - å¯¼å…¥å‰å…ˆæ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
    - ä½¿ç”¨æœ‰æ„ä¹‰çš„target_tableåç§°
    - å¯¼å…¥åéªŒè¯æ•°æ®å®Œæ•´æ€§
    """
    try:
        from config.api_data_storage import APIDataStorage
        
        # åˆå§‹åŒ–APIæ•°æ®å­˜å‚¨
        api_storage = APIDataStorage()
        
        # æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        success, sessions, message = api_storage.list_storage_sessions()
        if not success:
            result = {
                "status": "error",
                "message": f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {message}",
                "error_details": message
            }
            return f"âŒ è·å–ä¼šè¯å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        session_exists = any(session['session_id'] == session_id for session in sessions)
        
        if not session_exists:
            result = {
                "status": "error",
                "message": f"APIå­˜å‚¨ä¼šè¯ä¸å­˜åœ¨: {session_id}",
                "available_sessions": [s['session_id'] for s in sessions]
            }
            return f"âŒ ä¼šè¯ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·å–ä¼šè¯æ•°æ®
        success, session_data, data_message = api_storage.get_stored_data(session_id, format_type="dataframe")
        if not success or session_data is None or len(session_data) == 0:
            result = {
                "status": "error",
                "message": f"ä¼šè¯ {session_id} ä¸­æ²¡æœ‰æ•°æ®: {data_message}",
                "session_id": session_id
            }
            return f"âŒ æ— æ•°æ®å¯å¯¼å…¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # ç¡®å®šç›®æ ‡è¡¨å
        if not target_table:
            target_table = f"api_data_{session_id.replace('-', '_')}"
        
        # è·å–æ•°æ®åº“è¿æ¥
        if data_source:
            db_manager = DatabaseManager()
            conn = db_manager.get_connection(data_source)
        else:
            conn = get_db_connection()
        
        # æ•°æ®å·²ç»æ˜¯DataFrameæ ¼å¼
        df = session_data
        
        # å¯¼å…¥æ•°æ®åˆ°ä¸»æ•°æ®åº“
        df.to_sql(target_table, conn, if_exists='replace', index=False)
        
        # è·å–å¯¼å…¥ç»Ÿè®¡
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
        row_count = cursor.fetchone()[0]
        
        # è·å–åˆ—ä¿¡æ¯
        cursor.execute(f"PRAGMA table_info({target_table})")
        columns_info = cursor.fetchall()
        columns = [col[1] for col in columns_info]
        
        cursor.close()
        if not data_source:
            conn.close()
        
        result = {
            "status": "success",
            "message": f"APIæ•°æ®æˆåŠŸå¯¼å…¥åˆ°ä¸»æ•°æ®åº“",
            "data": {
                "session_id": session_id,
                "target_table": target_table,
                "rows_imported": row_count,
                "columns_count": len(columns),
                "columns": columns,
                "data_source": data_source or "æœ¬åœ°SQLite"
            },
            "next_steps": {
                "analyze_data": f"analyze_data(analysis_type='basic_stats', table_name='{target_table}')",
                "query_data": f"execute_sql(query='SELECT * FROM {target_table} LIMIT 10')",
                "export_data": f"export_data(export_type='excel', data_source='{target_table}')"
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "operation_type": "api_data_import",
                "source_type": "api_storage"
            }
        }
        
        return f"ğŸ“¥ APIæ•°æ®å¯¼å…¥æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except ImportError as e:
        result = {
            "status": "error",
            "message": "APIæ•°æ®å­˜å‚¨æ¨¡å—å¯¼å…¥å¤±è´¥",
            "error": str(e),
            "suggestion": "è¯·æ£€æŸ¥config/api_data_storage.pyæ–‡ä»¶æ˜¯å¦å­˜åœ¨"
        }
        return f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"APIæ•°æ®å¯¼å…¥å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"APIæ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "session_id": session_id,
            "target_table": target_table
        }
        return f"âŒ å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"


@mcp.tool()
def list_api_storage_sessions() -> str:
    """
    ğŸ“‹ APIå­˜å‚¨ä¼šè¯åˆ—è¡¨å·¥å…· - æŸ¥çœ‹æ‰€æœ‰APIæ•°æ®å­˜å‚¨ä¼šè¯
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - åˆ—å‡ºæ‰€æœ‰APIæ•°æ®å­˜å‚¨ä¼šè¯
    - æ˜¾ç¤ºä¼šè¯è¯¦ç»†ä¿¡æ¯å’Œæ•°æ®ç»Ÿè®¡
    - ä¸ºAPIæ•°æ®å¯¼å…¥æä¾›ä¼šè¯é€‰æ‹©
    
    Returns:
        str: JSONæ ¼å¼çš„ä¼šè¯åˆ—è¡¨ï¼ŒåŒ…å«ä¼šè¯ä¿¡æ¯å’Œæ•°æ®ç»Ÿè®¡
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    - åœ¨å¯¼å…¥APIæ•°æ®å‰å…ˆæŸ¥çœ‹å¯ç”¨ä¼šè¯
    - é€‰æ‹©åˆé€‚çš„ä¼šè¯è¿›è¡Œæ•°æ®å¯¼å…¥
    - äº†è§£æ¯ä¸ªä¼šè¯çš„æ•°æ®é‡å’Œç»“æ„
    """
    try:
        from config.api_data_storage import APIDataStorage
        
        # åˆå§‹åŒ–APIæ•°æ®å­˜å‚¨
        api_storage = APIDataStorage()
        
        # è·å–æ‰€æœ‰ä¼šè¯
        success, sessions, message = api_storage.list_storage_sessions()
        
        if not success:
            result = {
                "status": "error",
                "message": f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {message}",
                "error_details": message
            }
            return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        if not sessions:
            result = {
                "status": "success",
                "message": "æš‚æ— APIå­˜å‚¨ä¼šè¯",
                "data": {
                    "sessions_count": 0,
                    "sessions": []
                },
                "suggestion": "ä½¿ç”¨fetch_api_dataå·¥å…·åˆ›å»ºAPIæ•°æ®å­˜å‚¨ä¼šè¯"
            }
            return f"ğŸ“‹ æš‚æ— APIå­˜å‚¨ä¼šè¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # ä¸ºæ¯ä¸ªä¼šè¯è·å–æ•°æ®ç»Ÿè®¡
        sessions_with_stats = []
        for session in sessions:
            session_id = session['session_id']
            try:
                success_data, session_data, data_msg = api_storage.get_stored_data(session_id, format_type="dataframe")
                data_count = len(session_data) if success_data and session_data is not None else 0
                
                # è·å–æ•°æ®åˆ—ä¿¡æ¯
                columns = []
                if success_data and session_data is not None and len(session_data) > 0:
                    columns = list(session_data.columns) if hasattr(session_data, 'columns') else []
                
                session_info = {
                    **session,
                    "data_statistics": {
                        "rows_count": data_count,
                        "columns_count": len(columns),
                        "columns": columns[:10],  # åªæ˜¾ç¤ºå‰10åˆ—
                        "has_more_columns": len(columns) > 10
                    }
                }
                sessions_with_stats.append(session_info)
                
            except Exception as e:
                session_info = {
                    **session,
                    "data_statistics": {
                        "error": f"è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {str(e)}"
                    }
                }
                sessions_with_stats.append(session_info)
        
        result = {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(sessions)} ä¸ªAPIå­˜å‚¨ä¼šè¯",
            "data": {
                "sessions_count": len(sessions),
                "sessions": sessions_with_stats
            },
            "usage_tips": {
                "import_data": "ä½¿ç”¨import_api_data_to_main_dbå¯¼å…¥æ•°æ®åˆ°ä¸»æ•°æ®åº“",
                "preview_data": "ä¼šè¯æ•°æ®å·²åŒ…å«åœ¨data_statisticsä¸­",
                "analyze_data": "å¯¼å…¥åå¯ä½¿ç”¨analyze_dataç­‰å·¥å…·åˆ†æ"
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "operation_type": "list_api_sessions"
            }
        }
        
        return f"ğŸ“‹ APIå­˜å‚¨ä¼šè¯åˆ—è¡¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except ImportError as e:
        result = {
            "status": "error",
            "message": "APIæ•°æ®å­˜å‚¨æ¨¡å—å¯¼å…¥å¤±è´¥",
            "error": str(e),
            "suggestion": "è¯·æ£€æŸ¥config/api_data_storage.pyæ–‡ä»¶æ˜¯å¦å­˜åœ¨"
        }
        return f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
    
    except Exception as e:
        logger.error(f"è·å–APIå­˜å‚¨ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è·å–å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"


# ================================
# 4. å¯åŠ¨æœåŠ¡å™¨
# ================================
def main():
    """ä¸»å…¥å£å‡½æ•°"""
    logger.info(f"å¯åŠ¨ {TOOL_NAME}")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    try:
        mcp.run()
    except KeyboardInterrupt:
        logger.info("æ­£åœ¨å…³é—­...")
    finally:
        logger.info("æœåŠ¡å™¨å·²å…³é—­")

if __name__ == "__main__":
    main()

# ================================
# 5. ä½¿ç”¨è¯´æ˜
# ================================
"""
ğŸš€ DataMaster MCP ä½¿ç”¨æŒ‡å—ï¼š

1ï¸âƒ£ å¯¼å…¥Excelæ•°æ®ï¼š
   connect_data_source(
       source_type="excel",
       config={
           "file_path": "path/to/your/file.xlsx",
           "sheet_name": "Sheet1"  # å¯é€‰
       },
       target_table="my_table"  # å¯é€‰
   )

2ï¸âƒ£ æ‰§è¡ŒSQLæŸ¥è¯¢ï¼š
   execute_sql(
       query="SELECT * FROM my_table",
       limit=100
   )

3ï¸âƒ£ è·å–æ•°æ®ä¿¡æ¯ï¼š
   get_data_info(info_type="tables")  # è·å–æ‰€æœ‰è¡¨
   get_data_info(info_type="schema", table_name="my_table")  # è·å–è¡¨ç»“æ„

4ï¸âƒ£ æ•°æ®åˆ†æï¼š
   analyze_data(
       analysis_type="basic_stats",
       table_name="my_table",
       columns=["column1", "column2"]  # å¯é€‰
   )
   
   analyze_data(
       analysis_type="correlation",
       table_name="my_table"
   )
   
   analyze_data(
       analysis_type="outliers",
       table_name="my_table",
       options={"method": "iqr"}  # æˆ– "zscore"
   )



5ï¸âƒ£ æ•°æ®å¤„ç† (Excelæ•°æ®å¤„ç†)ï¼š
   process_data(
       operation_type="clean",
       data_source="my_table",
       config={
           "remove_duplicates": True,
           "fill_missing": {
               "column1": {"method": "mean"},
               "column2": {"method": "mode"}
           }
       }
   )
   
   process_data(
       operation_type="filter",
       data_source="my_table",
       config={
           "filter_condition": "age > 18",
           "select_columns": ["name", "age", "salary"]
       }
   )
   
   process_data(
       operation_type="aggregate",
       data_source="my_table",
       config={
           "group_by": {
               "columns": ["department"],
               "agg": {"salary": "mean", "age": "count"}
           }
       }
   )

6ï¸âƒ£ æ•°æ®å¯¼å‡ºï¼š
   export_data(
       export_type="excel",
       data_source="my_table",
       file_path="exports/data.xlsx"  # å¯é€‰
   )
   
   export_data(
       export_type="csv",
       data_source="SELECT * FROM my_table WHERE condition"
   )



ğŸ’¡ ç‰¹æ€§ï¼š
   - 6ä¸ªå¼ºå¤§çš„æ•°æ®åˆ†æå·¥å…·
   - Excelæ•°æ®å¤„ç†åŠŸèƒ½ (æ¸…æ´—ã€è½¬æ¢ã€ç­›é€‰ã€èšåˆã€åˆå¹¶ã€é‡å¡‘)
   - è‡ªåŠ¨æ•°æ®ç±»å‹æ¨æ–­
   - ç»Ÿä¸€çš„JSONè¿”å›æ ¼å¼
   - å®Œå–„çš„é”™è¯¯å¤„ç†
   - æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼
   - ä¸°å¯Œçš„ç»Ÿè®¡åˆ†æåŠŸèƒ½
   - é¢„ç•™APIå’ŒSQLæ•°æ®å¤„ç†æ‰©å±•
"""