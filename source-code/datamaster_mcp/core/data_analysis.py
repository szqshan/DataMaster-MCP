#!/usr/bin/env python3
"""
DataMaster MCP - æ•°æ®åˆ†ææ ¸å¿ƒæ¨¡å—

è¿™ä¸ªæ¨¡å—åŒ…å«æ‰€æœ‰æ•°æ®åˆ†æç›¸å…³çš„å·¥å…·å‡½æ•°ï¼š
- analyze_data: æ‰§è¡Œå„ç§ç»Ÿè®¡åˆ†æå’Œæ•°æ®è´¨é‡æ£€æŸ¥
- get_data_info: è·å–æ•°æ®åº“ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯

ä»¥åŠç›¸å…³çš„åˆ†æè¾…åŠ©å‡½æ•°ã€‚
"""

import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging
from scipy import stats

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("DataMaster_MCP.DataAnalysis")

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from ..config.database_manager import database_manager
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    current_dir = Path(__file__).parent.parent.parent
    if str(current_dir) not in sys.path:
        sys.path.insert(0, str(current_dir))
    
    from datamaster_mcp.config.database_manager import database_manager

# å¯¼å…¥æ•°æ®åº“ç›¸å…³å‡½æ•°
try:
    from .database import get_db_connection, _escape_identifier, _table_exists
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰æœ¬åœ°ç‰ˆæœ¬
    def get_db_connection():
        """è·å–æ•°æ®åº“è¿æ¥"""
        import sqlite3
        conn = sqlite3.connect("data/analysis.db")
        conn.row_factory = sqlite3.Row
        return conn
    
    def _escape_identifier(identifier: str) -> str:
        """è½¬ä¹‰SQLæ ‡è¯†ç¬¦"""
        return '"' + identifier.replace('"', '""') + '"'
    
    def _table_exists(table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            with get_db_connection() as conn:
                cursor = conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                    (table_name,)
                )
                return cursor.fetchone() is not None
        except Exception:
            return False

# ================================
# æ•°æ®åˆ†æå·¥å…·å‡½æ•°
# ================================

def analyze_data_impl(
    analysis_type: str,
    table_name: str,
    columns: list = None,
    options: dict = None
) -> str:
    """
    ğŸ” æ•°æ®åˆ†æå·¥å…· - æ‰§è¡Œå„ç§ç»Ÿè®¡åˆ†æå’Œæ•°æ®è´¨é‡æ£€æŸ¥
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æä¾›5ç§æ ¸å¿ƒæ•°æ®åˆ†æåŠŸèƒ½
    - æ”¯æŒæŒ‡å®šåˆ—åˆ†ææˆ–å…¨è¡¨åˆ†æ
    - è‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼
    - è¿”å›è¯¦ç»†çš„åˆ†æç»“æœå’Œå¯è§†åŒ–å»ºè®®
    
    Args:
        analysis_type: åˆ†æç±»å‹
            - "basic_stats": åŸºç¡€ç»Ÿè®¡åˆ†æï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰ï¼‰
            - "correlation": ç›¸å…³æ€§åˆ†æï¼ˆæ•°å€¼åˆ—ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼‰
            - "outliers": å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRã€Z-scoreæ–¹æ³•ï¼‰
            - "missing_values": ç¼ºå¤±å€¼åˆ†æï¼ˆç¼ºå¤±ç‡ã€åˆ†å¸ƒæ¨¡å¼ï¼‰
            - "duplicates": é‡å¤å€¼æ£€æµ‹ï¼ˆå®Œå…¨é‡å¤ã€éƒ¨åˆ†é‡å¤ï¼‰
        table_name: è¦åˆ†æçš„æ•°æ®è¡¨å
        columns: åˆ†æçš„åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            - None: åˆ†ææ‰€æœ‰é€‚ç”¨åˆ—
            - ["col1", "col2"]: åªåˆ†ææŒ‡å®šåˆ—
        options: åˆ†æé€‰é¡¹ï¼ˆå¯é€‰å­—å…¸ï¼‰
            - outliers: {"method": "iqr|zscore", "threshold": 1.5}
            - correlation: {"method": "pearson|spearman"}
            - basic_stats: {"percentiles": [25, 50, 75, 90, 95]}
    
    Returns:
        str: JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ç»Ÿè®¡æ•°æ®ã€å›¾è¡¨å»ºè®®å’Œæ´å¯Ÿ
    """
    try:
        # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        if not _table_exists(table_name):
            result = {
                "status": "error",
                "message": f"è¡¨ '{table_name}' ä¸å­˜åœ¨"
            }
            return f"âŒ è¡¨ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·¯ç”±åˆ°å…·ä½“çš„åˆ†æå‡½æ•°
        analysis_map = {
            "basic_stats": _calculate_basic_stats,
            "correlation": _calculate_correlation,
            "outliers": _detect_outliers,
            "missing_values": _check_missing_values,
            "duplicates": _check_duplicates
        }
        
        if analysis_type not in analysis_map:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}",
                "supported_types": list(analysis_map.keys())
            }
            return f"âŒ åˆ†æç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = analysis_map[analysis_type](table_name, columns or [], options or {})
        
        if "error" in analysis_result:
            result = {
                "status": "error",
                "message": analysis_result["error"]
            }
            return f"âŒ åˆ†æå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è¿”å›æˆåŠŸç»“æœ
        result = {
            "status": "success",
            "message": f"{analysis_type} åˆ†æå®Œæˆ",
            "data": analysis_result,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "table_name": table_name,
                "analysis_type": analysis_type,
                "columns": columns or []
            }
        }
        
        return f"âœ… åˆ†æå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ åˆ†æå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def get_data_info_impl(
    info_type: str = "tables",
    table_name: str = None,
    data_source: str = None
) -> str:
    """
    ğŸ“Š æ•°æ®ä¿¡æ¯è·å–å·¥å…· - æŸ¥çœ‹æ•°æ®åº“ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - è·å–æ•°æ®åº“è¡¨åˆ—è¡¨ã€è¡¨ç»“æ„ã€æ•°æ®ç»Ÿè®¡ç­‰ä¿¡æ¯
    - æ”¯æŒæœ¬åœ°SQLiteå’Œå¤–éƒ¨æ•°æ®åº“
    - æä¾›è¯¦ç»†çš„è¡¨ç»“æ„å’Œæ•°æ®æ¦‚è§ˆ
    - æ™ºèƒ½æ•°æ®åº“æ¸…ç†ç®¡ç†åŠŸèƒ½
    
    Args:
        info_type: ä¿¡æ¯ç±»å‹
            - "tables": è·å–æ‰€æœ‰è¡¨/é›†åˆåˆ—è¡¨ï¼ˆé»˜è®¤ï¼‰
            - "schema": è·å–æŒ‡å®šè¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆéœ€è¦table_nameï¼‰
            - "stats": è·å–æŒ‡å®šè¡¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆéœ€è¦table_nameï¼‰
            - "cleanup": æ™ºèƒ½æ£€æµ‹è¿‡æ—¶æ•°æ®å’Œè¡¨ï¼Œæä¾›æ¸…ç†å»ºè®®
        table_name: è¡¨åï¼ˆå½“info_typeä¸ºschemaæˆ–statsæ—¶å¿…éœ€ï¼‰
        data_source: æ•°æ®æºåç§°
            - None: ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“ï¼ˆé»˜è®¤ï¼‰
            - é…ç½®åç§°: ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“ï¼ˆéœ€å…ˆé€šè¿‡manage_database_configåˆ›å»ºé…ç½®ï¼‰
    
    Returns:
        str: JSONæ ¼å¼çš„æ•°æ®åº“ä¿¡æ¯ï¼ŒåŒ…å«çŠ¶æ€ã€æ•°æ®å’Œå…ƒæ•°æ®
    """
    try:
        if data_source:
            # ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“è¿æ¥
            try:
                if info_type == "tables":
                    tables = database_manager.get_table_list(data_source)
                    
                    result = {
                        "status": "success",
                        "message": f"è·å–åˆ° {len(tables)} ä¸ªè¡¨",
                        "data": {
                            "tables": tables,
                            "table_count": len(tables),
                            "data_source": data_source
                        },
                        "metadata": {
                            "timestamp": datetime.now().isoformat(),
                            "info_type": info_type,
                            "data_source": data_source
                        }
                    }
                    return f"âœ… è¡¨åˆ—è¡¨è·å–æˆåŠŸï¼ˆæ•°æ®æº: {data_source}ï¼‰\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                elif info_type == "schema":
                    if not table_name:
                        raise ValueError("è·å–è¡¨ç»“æ„éœ€è¦æŒ‡å®štable_nameå‚æ•°")
                    
                    schema = database_manager.get_table_schema(data_source, table_name)
                    
                    result = {
                        "status": "success",
                        "message": f"è¡¨ '{table_name}' ç»“æ„ä¿¡æ¯",
                        "data": {
                            "table_name": table_name,
                            "schema": schema,
                            "data_source": data_source
                        },
                        "metadata": {
                            "timestamp": datetime.now().isoformat(),
                            "info_type": info_type,
                            "table_name": table_name,
                            "data_source": data_source
                        }
                    }
                    return f"âœ… è¡¨ç»“æ„è·å–æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                elif info_type == "stats":
                    if not table_name:
                        raise ValueError("è·å–è¡¨ç»Ÿè®¡éœ€è¦æŒ‡å®štable_nameå‚æ•°")
                    
                    stats = database_manager.get_table_stats(data_source, table_name)
                    
                    result = {
                        "status": "success",
                        "message": f"è¡¨ '{table_name}' ç»Ÿè®¡ä¿¡æ¯",
                        "data": {
                            "table_name": table_name,
                            "stats": stats,
                            "data_source": data_source
                        },
                        "metadata": {
                            "timestamp": datetime.now().isoformat(),
                            "info_type": info_type,
                            "table_name": table_name,
                            "data_source": data_source
                        }
                    }
                    return f"âœ… è¡¨ç»Ÿè®¡è·å–æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                    
                else:
                    raise ValueError(f"å¤–éƒ¨æ•°æ®åº“ä¸æ”¯æŒ '{info_type}' æ“ä½œ")
                    
            except Exception as e:
                result = {
                    "status": "error",
                    "message": f"å¤–éƒ¨æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}",
                    "data_source": data_source
                }
                return f"âŒ å¤–éƒ¨æ•°æ®åº“æ“ä½œå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        else:
            # ä½¿ç”¨æœ¬åœ°SQLiteæ•°æ®åº“
            if info_type == "tables":
                return _get_local_tables()
            elif info_type == "schema":
                if not table_name:
                    raise ValueError("è·å–è¡¨ç»“æ„éœ€è¦æŒ‡å®štable_nameå‚æ•°")
                return _get_table_schema(table_name)
            elif info_type == "stats":
                if not table_name:
                    raise ValueError("è·å–è¡¨ç»Ÿè®¡éœ€è¦æŒ‡å®štable_nameå‚æ•°")
                return _get_table_stats(table_name)
            elif info_type == "cleanup":
                return _analyze_database_cleanup()
            else:
                result = {
                    "status": "error",
                    "message": f"ä¸æ”¯æŒçš„ä¿¡æ¯ç±»å‹: {info_type}",
                    "supported_types": ["tables", "schema", "stats", "cleanup"]
                }
                return f"âŒ ä¿¡æ¯ç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
                
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ è·å–ä¿¡æ¯å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# ================================
# æ•°æ®åˆ†æè¾…åŠ©å‡½æ•°
# ================================

def _calculate_basic_stats(table_name: str, columns: list, options: dict) -> dict:
    """è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ - æ™ºèƒ½å¤„ç†æ•°å€¼å’Œæ–‡æœ¬åˆ—"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–åˆ—ä¿¡æ¯
            if columns:
                target_columns = columns
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                target_columns = [col[1] for col in cursor.fetchall()]
            
            if not target_columns:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„åˆ—"}
            
            # åˆ†ææ¯ä¸€åˆ—
            stats_result = {}
            numeric_columns = []
            text_columns = []
            
            for col in target_columns:
                escaped_col = _escape_identifier(col)
                
                # æ£€æµ‹åˆ—ç±»å‹
                cursor = conn.execute(f"SELECT typeof({escaped_col}) FROM {escaped_table} WHERE {escaped_col} IS NOT NULL LIMIT 1")
                result = cursor.fetchone()
                col_type = result[0] if result else 'null'
                
                # è·å–åŸºæœ¬ä¿¡æ¯
                cursor = conn.execute(f"""
                    SELECT 
                        COUNT(*) as total_count,
                        COUNT({escaped_col}) as non_null_count,
                        COUNT(CASE WHEN {escaped_col} IS NULL THEN 1 END) as null_count
                    FROM {escaped_table}
                """)
                basic_info = cursor.fetchone()
                
                if col_type in ['integer', 'real']:
                    # æ•°å€¼åˆ—ç»Ÿè®¡
                    numeric_columns.append(col)
                    cursor = conn.execute(f"""
                        SELECT 
                            AVG({escaped_col}) as mean,
                            MIN({escaped_col}) as min_val,
                            MAX({escaped_col}) as max_val
                        FROM {escaped_table}
                        WHERE {escaped_col} IS NOT NULL
                    """)
                    numeric_stats = cursor.fetchone()
                    
                    # è®¡ç®—ä¸­ä½æ•°å’Œæ ‡å‡†å·®
                    cursor = conn.execute(f"SELECT {escaped_col} FROM {escaped_table} WHERE {escaped_col} IS NOT NULL ORDER BY {escaped_col}")
                    values = [row[0] for row in cursor.fetchall()]
                    
                    if values:
                        median = np.median(values)
                        std_dev = np.std(values)
                        q25 = np.percentile(values, 25)
                        q75 = np.percentile(values, 75)
                    else:
                        median = std_dev = q25 = q75 = None
                    
                    stats_result[col] = {
                        "column_type": "numeric",
                        "data_type": col_type,
                        "total_count": basic_info[0],
                        "non_null_count": basic_info[1],
                        "null_count": basic_info[2],
                        "null_percentage": round((basic_info[2] / basic_info[0]) * 100, 2) if basic_info[0] > 0 else 0,
                        "mean": round(numeric_stats[0], 4) if numeric_stats[0] else None,
                        "median": round(median, 4) if median is not None else None,
                        "std_dev": round(std_dev, 4) if std_dev is not None else None,
                        "min": numeric_stats[1],
                        "max": numeric_stats[2],
                        "q25": round(q25, 4) if q25 is not None else None,
                        "q75": round(q75, 4) if q75 is not None else None
                    }
                    
                else:
                    # æ–‡æœ¬åˆ—ç»Ÿè®¡
                    text_columns.append(col)
                    
                    # è·å–å”¯ä¸€å€¼æ•°é‡
                    cursor = conn.execute(f"SELECT COUNT(DISTINCT {escaped_col}) FROM {escaped_table} WHERE {escaped_col} IS NOT NULL")
                    unique_count = cursor.fetchone()[0]
                    
                    # è·å–æœ€å¸¸è§çš„å€¼ï¼ˆå‰5ä¸ªï¼‰
                    cursor = conn.execute(f"""
                        SELECT {escaped_col}, COUNT(*) as freq 
                        FROM {escaped_table} 
                        WHERE {escaped_col} IS NOT NULL 
                        GROUP BY {escaped_col} 
                        ORDER BY freq DESC 
                        LIMIT 5
                    """)
                    top_values = cursor.fetchall()
                    
                    # è®¡ç®—å­—ç¬¦ä¸²é•¿åº¦ç»Ÿè®¡ï¼ˆå¦‚æœæ˜¯æ–‡æœ¬ï¼‰
                    length_stats = None
                    if col_type == 'text':
                        cursor = conn.execute(f"""
                            SELECT 
                                AVG(LENGTH({escaped_col})) as avg_length,
                                MIN(LENGTH({escaped_col})) as min_length,
                                MAX(LENGTH({escaped_col})) as max_length
                            FROM {escaped_table}
                            WHERE {escaped_col} IS NOT NULL
                        """)
                        length_result = cursor.fetchone()
                        if length_result[0] is not None:
                            length_stats = {
                                "avg_length": round(length_result[0], 2),
                                "min_length": length_result[1],
                                "max_length": length_result[2]
                            }
                    
                    stats_result[col] = {
                        "column_type": "categorical",
                        "data_type": col_type,
                        "total_count": basic_info[0],
                        "non_null_count": basic_info[1],
                        "null_count": basic_info[2],
                        "null_percentage": round((basic_info[2] / basic_info[0]) * 100, 2) if basic_info[0] > 0 else 0,
                        "unique_count": unique_count,
                        "unique_percentage": round((unique_count / basic_info[1]) * 100, 2) if basic_info[1] > 0 else 0,
                        "top_values": [{
                            "value": str(val[0]),
                            "frequency": val[1],
                            "percentage": round((val[1] / basic_info[1]) * 100, 2) if basic_info[1] > 0 else 0
                        } for val in top_values],
                        "length_stats": length_stats
                    }
            
            # æ·»åŠ æ±‡æ€»ä¿¡æ¯
            summary = {
                "total_columns": len(target_columns),
                "numeric_columns": len(numeric_columns),
                "categorical_columns": len(text_columns),
                "numeric_column_names": numeric_columns,
                "categorical_column_names": text_columns
            }
            
            return {
                "column_stats": stats_result,
                "summary": summary
            }
            
    except Exception as e:
        return {"error": f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"}

def _calculate_correlation(table_name: str, columns: list, options: dict) -> dict:
    """è®¡ç®—ç›¸å…³ç³»æ•°"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–æ•°å€¼åˆ—
            if columns and len(columns) >= 2:
                numeric_columns = columns[:10]  # é™åˆ¶æœ€å¤š10åˆ—
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                all_columns = cursor.fetchall()
                numeric_columns = [col[1] for col in all_columns if col[2] in ['INTEGER', 'REAL', 'NUMERIC']][:10]
            
            if len(numeric_columns) < 2:
                return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼åˆ—æ¥è®¡ç®—ç›¸å…³æ€§"}
            
            # è·å–æ•°æ®
            escaped_columns = [_escape_identifier(col) for col in numeric_columns]
            columns_str = ", ".join(escaped_columns)
            df = pd.read_sql(f"SELECT {columns_str} FROM {escaped_table}", conn)
            
            # é‡å‘½ååˆ—ä¸ºåŸå§‹åç§°ï¼ˆå»æ‰è½¬ä¹‰ç¬¦å·ï¼‰
            df.columns = numeric_columns
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            correlation_matrix = df.corr().round(4)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            result = {}
            for i, col1 in enumerate(numeric_columns):
                result[col1] = {}
                for j, col2 in enumerate(numeric_columns):
                    result[col1][col2] = correlation_matrix.iloc[i, j]
            
            return {
                "correlation_matrix": result,
                "columns": numeric_columns,
                "method": "pearson"
            }
            
    except Exception as e:
        return {"error": f"è®¡ç®—ç›¸å…³æ€§å¤±è´¥: {str(e)}"}

def _detect_outliers(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æµ‹å¼‚å¸¸å€¼"""
    try:
        method = options.get("method", "iqr")  # iqr æˆ– zscore
        threshold = options.get("threshold", 1.5)
        
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–æ•°å€¼åˆ—
            if columns:
                numeric_columns = columns[:5]  # é™åˆ¶æœ€å¤š5åˆ—
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                all_columns = cursor.fetchall()
                numeric_columns = [col[1] for col in all_columns if col[2] in ['INTEGER', 'REAL', 'NUMERIC']][:5]
            
            if not numeric_columns:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—æ¥æ£€æµ‹å¼‚å¸¸å€¼"}
            
            outliers_result = {}
            
            for col in numeric_columns:
                escaped_col = _escape_identifier(col)
                
                # è·å–æ•°æ®
                cursor = conn.execute(f"SELECT {escaped_col} FROM {escaped_table} WHERE {escaped_col} IS NOT NULL")
                values = [row[0] for row in cursor.fetchall()]
                
                if len(values) < 4:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                    outliers_result[col] = {
                        "method": method,
                        "outliers": [],
                        "outlier_count": 0,
                        "total_count": len(values),
                        "note": "æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•æ£€æµ‹å¼‚å¸¸å€¼"
                    }
                    continue
                
                outliers = []
                
                if method == "iqr":
                    # IQRæ–¹æ³•
                    q1 = np.percentile(values, 25)
                    q3 = np.percentile(values, 75)
                    iqr = q3 - q1
                    lower_bound = q1 - threshold * iqr
                    upper_bound = q3 + threshold * iqr
                    
                    outliers = [v for v in values if v < lower_bound or v > upper_bound]
                    
                    outliers_result[col] = {
                        "method": "IQR",
                        "threshold": threshold,
                        "q1": round(q1, 4),
                        "q3": round(q3, 4),
                        "iqr": round(iqr, 4),
                        "lower_bound": round(lower_bound, 4),
                        "upper_bound": round(upper_bound, 4),
                        "outliers": sorted(set(outliers)),
                        "outlier_count": len(outliers),
                        "total_count": len(values),
                        "outlier_percentage": round((len(outliers) / len(values)) * 100, 2)
                    }
                    
                elif method == "zscore":
                    # Z-scoreæ–¹æ³•
                    mean_val = np.mean(values)
                    std_val = np.std(values)
                    
                    if std_val == 0:
                        outliers_result[col] = {
                            "method": "Z-score",
                            "outliers": [],
                            "outlier_count": 0,
                            "total_count": len(values),
                            "note": "æ ‡å‡†å·®ä¸º0ï¼Œæ— æ³•ä½¿ç”¨Z-scoreæ–¹æ³•"
                        }
                        continue
                    
                    z_scores = [(v - mean_val) / std_val for v in values]
                    outliers = [values[i] for i, z in enumerate(z_scores) if abs(z) > threshold]
                    
                    outliers_result[col] = {
                        "method": "Z-score",
                        "threshold": threshold,
                        "mean": round(mean_val, 4),
                        "std": round(std_val, 4),
                        "outliers": sorted(set(outliers)),
                        "outlier_count": len(outliers),
                        "total_count": len(values),
                        "outlier_percentage": round((len(outliers) / len(values)) * 100, 2)
                    }
            
            return {
                "outliers_by_column": outliers_result,
                "columns_analyzed": numeric_columns,
                "method": method
            }
            
    except Exception as e:
        return {"error": f"å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {str(e)}"}

def _check_missing_values(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æŸ¥ç¼ºå¤±å€¼"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–åˆ—ä¿¡æ¯
            if columns:
                target_columns = columns
            else:
                cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                target_columns = [col[1] for col in cursor.fetchall()]
            
            if not target_columns:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„åˆ—"}
            
            # è·å–æ€»è¡Œæ•°
            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
            total_rows = cursor.fetchone()[0]
            
            missing_result = {}
            
            for col in target_columns:
                escaped_col = _escape_identifier(col)
                
                # è®¡ç®—ç¼ºå¤±å€¼
                cursor = conn.execute(f"""
                    SELECT 
                        COUNT(*) as total,
                        COUNT({escaped_col}) as non_null,
                        COUNT(CASE WHEN {escaped_col} IS NULL THEN 1 END) as null_count
                    FROM {escaped_table}
                """)
                stats = cursor.fetchone()
                
                null_count = stats[2]
                null_percentage = (null_count / total_rows) * 100 if total_rows > 0 else 0
                
                missing_result[col] = {
                    "total_count": total_rows,
                    "non_null_count": stats[1],
                    "null_count": null_count,
                    "null_percentage": round(null_percentage, 2),
                    "completeness": round(100 - null_percentage, 2)
                }
            
            # æ±‡æ€»ç»Ÿè®¡
            total_missing = sum(col_data["null_count"] for col_data in missing_result.values())
            total_cells = total_rows * len(target_columns)
            overall_completeness = ((total_cells - total_missing) / total_cells) * 100 if total_cells > 0 else 0
            
            summary = {
                "total_rows": total_rows,
                "total_columns": len(target_columns),
                "total_cells": total_cells,
                "total_missing_cells": total_missing,
                "overall_completeness": round(overall_completeness, 2),
                "columns_with_missing": [col for col, data in missing_result.items() if data["null_count"] > 0],
                "complete_columns": [col for col, data in missing_result.items() if data["null_count"] == 0]
            }
            
            return {
                "missing_by_column": missing_result,
                "summary": summary
            }
            
    except Exception as e:
        return {"error": f"ç¼ºå¤±å€¼æ£€æŸ¥å¤±è´¥: {str(e)}"}

def _check_duplicates(table_name: str, columns: list, options: dict) -> dict:
    """æ£€æŸ¥é‡å¤å€¼"""
    try:
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–æ€»è¡Œæ•°
            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
            total_rows = cursor.fetchone()[0]
            
            if total_rows == 0:
                return {"error": "è¡¨ä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥é‡å¤å€¼"}
            
            # æ£€æŸ¥å®Œå…¨é‡å¤çš„è¡Œ
            if columns:
                # æ£€æŸ¥æŒ‡å®šåˆ—çš„é‡å¤
                escaped_columns = [_escape_identifier(col) for col in columns]
                columns_str = ", ".join(escaped_columns)
                
                cursor = conn.execute(f"""
                    SELECT COUNT(*) as unique_count
                    FROM (SELECT DISTINCT {columns_str} FROM {escaped_table})
                """)
                unique_rows = cursor.fetchone()[0]
                
                # è·å–é‡å¤çš„ç»„åˆ
                cursor = conn.execute(f"""
                    SELECT {columns_str}, COUNT(*) as freq
                    FROM {escaped_table}
                    GROUP BY {columns_str}
                    HAVING COUNT(*) > 1
                    ORDER BY freq DESC
                    LIMIT 10
                """)
                duplicates = cursor.fetchall()
                
                duplicate_count = total_rows - unique_rows
                
                result = {
                    "check_type": "specified_columns",
                    "columns_checked": columns,
                    "total_rows": total_rows,
                    "unique_combinations": unique_rows,
                    "duplicate_rows": duplicate_count,
                    "duplicate_percentage": round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else 0,
                    "duplicate_groups": [{
                        "values": dict(zip(columns, dup[:-1])),
                        "frequency": dup[-1]
                    } for dup in duplicates]
                }
                
            else:
                # æ£€æŸ¥å®Œå…¨é‡å¤çš„è¡Œï¼ˆæ‰€æœ‰åˆ—ï¼‰
                cursor = conn.execute(f"SELECT COUNT(*) FROM (SELECT DISTINCT * FROM {escaped_table})")
                unique_rows = cursor.fetchone()[0]
                
                duplicate_count = total_rows - unique_rows
                
                result = {
                    "check_type": "complete_rows",
                    "total_rows": total_rows,
                    "unique_rows": unique_rows,
                    "duplicate_rows": duplicate_count,
                    "duplicate_percentage": round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else 0,
                    "note": "æ£€æŸ¥äº†æ‰€æœ‰åˆ—çš„å®Œå…¨é‡å¤"
                }
            
            return result
            
    except Exception as e:
        return {"error": f"é‡å¤å€¼æ£€æŸ¥å¤±è´¥: {str(e)}"}

# ================================
# æ•°æ®ä¿¡æ¯è·å–è¾…åŠ©å‡½æ•°
# ================================

def _get_local_tables() -> str:
    """è·å–æœ¬åœ°æ•°æ®åº“è¡¨åˆ—è¡¨"""
    try:
        with get_db_connection() as conn:
            cursor = conn.execute("""
                SELECT name, sql FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
                ORDER BY name
            """)
            tables = cursor.fetchall()
            
            # è·å–æ¯ä¸ªè¡¨çš„è¡Œæ•°
            table_info = []
            for table_name, create_sql in tables:
                try:
                    escaped_table = _escape_identifier(table_name)
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                    row_count = cursor.fetchone()[0]
                    
                    # è·å–åˆ—æ•°
                    cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
                    columns = cursor.fetchall()
                    column_count = len(columns)
                    
                    table_info.append({
                        "table_name": table_name,
                        "row_count": row_count,
                        "column_count": column_count,
                        "create_sql": create_sql
                    })
                except Exception as e:
                    table_info.append({
                        "table_name": table_name,
                        "row_count": "error",
                        "column_count": "error",
                        "error": str(e)
                    })
            
            result = {
                "status": "success",
                "message": f"æ‰¾åˆ° {len(table_info)} ä¸ªè¡¨",
                "data": {
                    "tables": table_info,
                    "table_count": len(table_info)
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "info_type": "tables",
                    "data_source": "æœ¬åœ°SQLite"
                }
            }
            
            return f"âœ… è¡¨åˆ—è¡¨è·å–æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}"
        }
        return f"âŒ è·å–è¡¨åˆ—è¡¨å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _get_table_schema(table_name: str) -> str:
    """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
    try:
        if not _table_exists(table_name):
            result = {
                "status": "error",
                "message": f"è¡¨ '{table_name}' ä¸å­˜åœ¨"
            }
            return f"âŒ è¡¨ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–åˆ—ä¿¡æ¯
            cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
            columns = cursor.fetchall()
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            cursor = conn.execute(f"PRAGMA index_list({escaped_table})")
            indexes = cursor.fetchall()
            
            # è·å–å¤–é”®ä¿¡æ¯
            cursor = conn.execute(f"PRAGMA foreign_key_list({escaped_table})")
            foreign_keys = cursor.fetchall()
            
            # æ ¼å¼åŒ–åˆ—ä¿¡æ¯
            column_info = []
            for col in columns:
                column_info.append({
                    "column_id": col[0],
                    "name": col[1],
                    "type": col[2],
                    "not_null": bool(col[3]),
                    "default_value": col[4],
                    "primary_key": bool(col[5])
                })
            
            # æ ¼å¼åŒ–ç´¢å¼•ä¿¡æ¯
            index_info = []
            for idx in indexes:
                index_info.append({
                    "name": idx[1],
                    "unique": bool(idx[2]),
                    "origin": idx[3]
                })
            
            result = {
                "status": "success",
                "message": f"è¡¨ '{table_name}' ç»“æ„ä¿¡æ¯",
                "data": {
                    "table_name": table_name,
                    "columns": column_info,
                    "column_count": len(column_info),
                    "indexes": index_info,
                    "foreign_keys": [dict(zip(["id", "seq", "table", "from", "to", "on_update", "on_delete", "match"], fk)) for fk in foreign_keys]
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "info_type": "schema",
                    "table_name": table_name,
                    "data_source": "æœ¬åœ°SQLite"
                }
            }
            
            return f"âœ… è¡¨ç»“æ„è·å–æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–è¡¨ç»“æ„å¤±è´¥: {str(e)}"
        }
        return f"âŒ è·å–è¡¨ç»“æ„å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _get_table_stats(table_name: str) -> str:
    """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not _table_exists(table_name):
            result = {
                "status": "error",
                "message": f"è¡¨ '{table_name}' ä¸å­˜åœ¨"
            }
            return f"âŒ è¡¨ä¸å­˜åœ¨\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        escaped_table = _escape_identifier(table_name)
        
        with get_db_connection() as conn:
            # è·å–åŸºæœ¬ç»Ÿè®¡
            cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
            row_count = cursor.fetchone()[0]
            
            # è·å–åˆ—ä¿¡æ¯
            cursor = conn.execute(f"PRAGMA table_info({escaped_table})")
            columns = cursor.fetchall()
            column_count = len(columns)
            
            # è·å–è¡¨å¤§å°ï¼ˆè¿‘ä¼¼ï¼‰
            cursor = conn.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            db_size = cursor.fetchone()[0]
            
            # ç®€å•çš„åˆ—ç±»å‹ç»Ÿè®¡
            column_types = {}
            for col in columns:
                col_type = col[2].upper()
                column_types[col_type] = column_types.get(col_type, 0) + 1
            
            result = {
                "status": "success",
                "message": f"è¡¨ '{table_name}' ç»Ÿè®¡ä¿¡æ¯",
                "data": {
                    "table_name": table_name,
                    "row_count": row_count,
                    "column_count": column_count,
                    "estimated_size_bytes": db_size,
                    "column_types": column_types,
                    "columns": [{
                        "name": col[1],
                        "type": col[2],
                        "nullable": not bool(col[3]),
                        "primary_key": bool(col[5])
                    } for col in columns]
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "info_type": "stats",
                    "table_name": table_name,
                    "data_source": "æœ¬åœ°SQLite"
                }
            }
            
            return f"âœ… è¡¨ç»Ÿè®¡è·å–æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"è·å–è¡¨ç»Ÿè®¡å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"è·å–è¡¨ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }
        return f"âŒ è·å–è¡¨ç»Ÿè®¡å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def _analyze_database_cleanup() -> str:
    """åˆ†ææ•°æ®åº“å¹¶æä¾›æ¸…ç†å»ºè®®"""
    try:
        with get_db_connection() as conn:
            # è·å–æ‰€æœ‰è¡¨ï¼ˆæ’é™¤å…ƒæ•°æ®è¡¨ï¼‰
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name != '_metadata' AND name != 'data_metadata'"
            )
            all_tables = [row[0] for row in cursor.fetchall()]
            
            if not all_tables:
                result = {
                    "status": "success",
                    "message": "æ•°æ®åº“ä¸­æ²¡æœ‰ç”¨æˆ·è¡¨",
                    "data": {
                        "total_tables": 0,
                        "cleanup_suggestions": []
                    }
                }
                return f"âœ… æ•°æ®åº“æ¸…ç†åˆ†æå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
            cleanup_suggestions = []
            empty_tables = []
            test_tables = []
            temp_tables = []
            
            for table_name in all_tables:
                try:
                    escaped_table = _escape_identifier(table_name)
                    
                    # æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºç©º
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {escaped_table}")
                    row_count = cursor.fetchone()[0]
                    
                    if row_count == 0:
                        empty_tables.append(table_name)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•è¡¨æˆ–ä¸´æ—¶è¡¨
                    table_lower = table_name.lower()
                    if any(keyword in table_lower for keyword in ['test', 'temp', 'tmp', 'demo', 'sample']):
                        if 'test' in table_lower:
                            test_tables.append(table_name)
                        else:
                            temp_tables.append(table_name)
                    
                except Exception as e:
                    logger.warning(f"åˆ†æè¡¨ {table_name} æ—¶å‡ºé”™: {e}")
            
            # ç”Ÿæˆæ¸…ç†å»ºè®®
            if empty_tables:
                cleanup_suggestions.append({
                    "type": "empty_tables",
                    "description": "å‘ç°ç©ºè¡¨ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤",
                    "tables": empty_tables,
                    "count": len(empty_tables),
                    "risk_level": "low"
                })
            
            if test_tables:
                cleanup_suggestions.append({
                    "type": "test_tables",
                    "description": "å‘ç°æµ‹è¯•è¡¨ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤",
                    "tables": test_tables,
                    "count": len(test_tables),
                    "risk_level": "medium"
                })
            
            if temp_tables:
                cleanup_suggestions.append({
                    "type": "temp_tables",
                    "description": "å‘ç°ä¸´æ—¶è¡¨ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤",
                    "tables": temp_tables,
                    "count": len(temp_tables),
                    "risk_level": "low"
                })
            
            result = {
                "status": "success",
                "message": f"æ•°æ®åº“æ¸…ç†åˆ†æå®Œæˆï¼Œå‘ç° {len(cleanup_suggestions)} ç±»æ¸…ç†å»ºè®®",
                "data": {
                    "total_tables": len(all_tables),
                    "cleanup_suggestions": cleanup_suggestions,
                    "summary": {
                        "empty_tables": len(empty_tables),
                        "test_tables": len(test_tables),
                        "temp_tables": len(temp_tables),
                        "total_suggested_for_cleanup": len(empty_tables) + len(test_tables) + len(temp_tables)
                    }
                },
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "info_type": "cleanup",
                    "data_source": "æœ¬åœ°SQLite"
                }
            }
            
            return f"âœ… æ•°æ®åº“æ¸…ç†åˆ†æå®Œæˆ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
            
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ¸…ç†åˆ†æå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®åº“æ¸…ç†åˆ†æå¤±è´¥: {str(e)}"
        }
        return f"âŒ æ•°æ®åº“æ¸…ç†åˆ†æå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# ================================
# æ¨¡å—åˆå§‹åŒ–å‡½æ•°
# ================================

def init_data_analysis_module():
    """åˆå§‹åŒ–æ•°æ®åˆ†ææ¨¡å—"""
    logger.info("æ•°æ®åˆ†ææ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    return {
        "analyze_data_impl": analyze_data_impl,
        "get_data_info_impl": get_data_info_impl
    }