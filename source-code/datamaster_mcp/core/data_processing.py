#!/usr/bin/env python3
"""
DataMaster MCP - æ•°æ®å¤„ç†æ ¸å¿ƒæ¨¡å—

è¿™ä¸ªæ¨¡å—åŒ…å«æ‰€æœ‰æ•°æ®å¤„ç†ç›¸å…³çš„å·¥å…·å‡½æ•°ï¼š
- process_data: æ‰§è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€ç­›é€‰ç­‰æ“ä½œ
- export_data: å°†æ•°æ®å¯¼å‡ºä¸ºå„ç§æ ¼å¼æ–‡ä»¶

ä»¥åŠç›¸å…³çš„å¤„ç†è¾…åŠ©å‡½æ•°ã€‚
"""

import json
import sqlite3
import pandas as pd
import numpy as np
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("DataMaster_MCP.DataProcessing")

# å¯¼å…¥æ•°æ®åº“ç›¸å…³å‡½æ•°
try:
    from .database import get_db_connection, _escape_identifier, _table_exists
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰æœ¬åœ°ç‰ˆæœ¬
    def get_db_connection():
        """è·å–æ•°æ®åº“è¿æ¥"""
        import sqlite3
        conn = sqlite3.connect("data/analysis.db")
        conn.row_factory = sqlite3.Row
        return conn
    
    def _escape_identifier(identifier: str) -> str:
        """è½¬ä¹‰SQLæ ‡è¯†ç¬¦"""
        return '"' + identifier.replace('"', '""') + '"'
    
    def _table_exists(table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            with get_db_connection() as conn:
                cursor = conn.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                    (table_name,)
                )
                return cursor.fetchone() is not None
        except Exception:
            return False

# ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
EXPORTS_DIR = "exports"
if not os.path.exists(EXPORTS_DIR):
    os.makedirs(EXPORTS_DIR)

# ================================
# æ•°æ®å¤„ç†å·¥å…·å‡½æ•°
# ================================

def process_data_impl(
    operation_type: str,
    data_source: str,
    config: dict,
    target_table: str = None
) -> str:
    """
    âš™ï¸ æ•°æ®å¤„ç†å·¥å…· - æ‰§è¡Œæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€ç­›é€‰ç­‰æ“ä½œ
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æä¾›6ç§æ ¸å¿ƒæ•°æ®å¤„ç†åŠŸèƒ½
    - æ”¯æŒè¡¨å’ŒSQLæŸ¥è¯¢ä½œä¸ºæ•°æ®æº
    - çµæ´»çš„é…ç½®å‚æ•°ç³»ç»Ÿ
    - å¯æŒ‡å®šç›®æ ‡è¡¨æˆ–è¦†ç›–åŸè¡¨
    
    Args:
        operation_type: å¤„ç†æ“ä½œç±»å‹
            - "clean": æ•°æ®æ¸…æ´—ï¼ˆå»é‡ã€å¡«å……ç¼ºå¤±å€¼ã€æ•°æ®ç±»å‹è½¬æ¢ï¼‰
            - "transform": æ•°æ®è½¬æ¢ï¼ˆåˆ—é‡å‘½åã€æ ‡å‡†åŒ–ã€æ–°åˆ—è®¡ç®—ï¼‰
            - "filter": æ•°æ®ç­›é€‰ï¼ˆæ¡ä»¶è¿‡æ»¤ã€åˆ—é€‰æ‹©ã€æ•°æ®é‡‡æ ·ï¼‰
            - "aggregate": æ•°æ®èšåˆï¼ˆåˆ†ç»„ç»Ÿè®¡ã€æ±‡æ€»è®¡ç®—ï¼‰
            - "merge": æ•°æ®åˆå¹¶ï¼ˆè¡¨è¿æ¥ã€æ•°æ®æ‹¼æ¥ï¼‰
            - "reshape": æ•°æ®é‡å¡‘ï¼ˆé€è§†è¡¨ã€å®½é•¿è½¬æ¢ï¼‰
        data_source: æ•°æ®æº
            - è¡¨å: å¤„ç†æ•´ä¸ªè¡¨
            - SQLæŸ¥è¯¢: å¤„ç†æŸ¥è¯¢ç»“æœ
        config: æ“ä½œé…ç½®å­—å…¸ï¼ˆå¿…éœ€ï¼‰
        target_table: ç›®æ ‡è¡¨åï¼ˆå¯é€‰ï¼‰
            - None: è¦†ç›–åŸè¡¨ï¼ˆé»˜è®¤ï¼‰
            - è¡¨å: ä¿å­˜åˆ°æ–°è¡¨
    
    Returns:
        str: JSONæ ¼å¼çš„å¤„ç†ç»“æœï¼ŒåŒ…å«æ“ä½œè¯¦æƒ…ã€å½±å“è¡Œæ•°å’Œæ–°è¡¨ä¿¡æ¯
    """
    try:
        # è·¯ç”±æ˜ å°„
        processors = {
            "clean": _process_clean,
            "transform": _process_transform,
            "filter": _process_filter,
            "aggregate": _process_aggregate,
            "merge": _process_merge,
            "reshape": _process_reshape
        }
        
        if operation_type not in processors:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation_type}",
                "supported_types": list(processors.keys())
            }
            return f"âŒ æ“ä½œç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # è·¯ç”±åˆ°å¯¹åº”å¤„ç†å™¨
        process_result = processors[operation_type](data_source, config, target_table)
        
        if "error" in process_result:
            result = {
                "status": "error",
                "message": process_result["error"]
            }
            return f"âŒ å¤„ç†å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        result = {
            "status": "success",
            "message": f"æ•°æ®å¤„ç†å®Œæˆ",
            "data": {
                "operation_type": operation_type,
                "data_source": data_source,
                "target_table": process_result.get("target_table"),
                "processed_rows": process_result.get("processed_rows", process_result.get("filtered_rows")),
                "operations": process_result.get("operations", []),
                "columns": process_result.get("columns")
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "processing_category": "data_processing"
            }
        }
        
        return f"âœ… æ•°æ®å¤„ç†æˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ å¤„ç†å¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

def export_data_impl(
    export_type: str,
    data_source: str,
    file_path: str = None,
    options: dict = None
) -> str:
    """
    ğŸ“¤ æ•°æ®å¯¼å‡ºå·¥å…· - å°†æ•°æ®å¯¼å‡ºä¸ºå„ç§æ ¼å¼æ–‡ä»¶
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼šExcelã€CSVã€JSON
    - å¯å¯¼å‡ºè¡¨æ•°æ®æˆ–SQLæŸ¥è¯¢ç»“æœ
    - è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨æŒ‡å®šè·¯å¾„
    - æ”¯æŒå¯¼å‡ºé€‰é¡¹è‡ªå®šä¹‰
    
    Args:
        export_type: å¯¼å‡ºæ ¼å¼ç±»å‹
            - "excel": Excelæ–‡ä»¶(.xlsx)
            - "csv": CSVæ–‡ä»¶(.csv)
            - "json": JSONæ–‡ä»¶(.json)
        data_source: æ•°æ®æº
            - è¡¨å: ç›´æ¥å¯¼å‡ºæ•´ä¸ªè¡¨
            - SQLæŸ¥è¯¢: å¯¼å‡ºæŸ¥è¯¢ç»“æœï¼ˆä»¥SELECTå¼€å¤´ï¼‰
        file_path: å¯¼å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            - None: è‡ªåŠ¨ç”Ÿæˆè·¯å¾„åˆ°exports/ç›®å½•
            - æŒ‡å®šè·¯å¾„: ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
        options: å¯¼å‡ºé€‰é¡¹ï¼ˆå¯é€‰å­—å…¸ï¼‰
            - Excel: {"sheet_name": "å·¥ä½œè¡¨å", "auto_adjust_columns": True}
            - CSV: {"encoding": "utf-8", "separator": ","}
            - JSON: {"orient": "records", "indent": 2}
    
    Returns:
        str: JSONæ ¼å¼çš„å¯¼å‡ºç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€è®°å½•æ•°ç­‰ä¿¡æ¯
    """
    try:
        # ç”Ÿæˆé»˜è®¤æ–‡ä»¶è·¯å¾„
        if not file_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä»æ•°æ®æºæå–åç§°
            if data_source.upper().startswith('SELECT'):
                source_name = "query_result"
            else:
                source_name = data_source
            
            # æ˜ å°„å¯¼å‡ºç±»å‹åˆ°æ–‡ä»¶æ‰©å±•å
            extension_map = {
                "excel": "xlsx",
                "csv": "csv",
                "json": "json"
            }
            extension = extension_map.get(export_type, export_type)
            file_path = f"{EXPORTS_DIR}/{source_name}_{timestamp}.{extension}"
        
        # è·¯ç”±åˆ°å…·ä½“çš„å¯¼å‡ºå‡½æ•°
        export_map = {
            "excel": _export_to_excel,
            "csv": _export_to_csv,
            "json": _export_to_json
        }
        
        if export_type not in export_map:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {export_type}",
                "supported_types": list(export_map.keys())
            }
            return f"âŒ å¯¼å‡ºç±»å‹é”™è¯¯\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        # æ‰§è¡Œå¯¼å‡º
        export_result = export_map[export_type](data_source, file_path, options or {})
        
        if "error" in export_result:
            result = {
                "status": "error",
                "message": export_result["error"]
            }
            return f"âŒ å¯¼å‡ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
        result = {
            "status": "success",
            "message": f"æ•°æ®å¯¼å‡ºå®Œæˆ",
            "data": {
                "file_path": file_path,
                "export_type": export_type,
                "file_size": export_result.get("file_size"),
                "record_count": export_result.get("record_count"),
                "columns": export_result.get("columns")
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "data_source": data_source
            }
        }
        
        return f"âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"
        
    except Exception as e:
        logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        result = {
            "status": "error",
            "message": f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        return f"âŒ å¯¼å‡ºå¤±è´¥\n\n{json.dumps(result, indent=2, ensure_ascii=False)}"

# ================================
# æ•°æ®å¤„ç†è¾…åŠ©å‡½æ•°
# ================================

def _process_clean(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®æ¸…æ´—å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            original_count = len(df)
            operations_performed = []
            
            # åˆ é™¤é‡å¤è¡Œ
            if config.get('remove_duplicates', False):
                before_count = len(df)
                df = df.drop_duplicates()
                removed_count = before_count - len(df)
                operations_performed.append(f"åˆ é™¤é‡å¤è¡Œ: {removed_count}è¡Œ")
            
            # å¤„ç†ç¼ºå¤±å€¼
            if 'fill_missing' in config:
                fill_config = config['fill_missing']
                for column, fill_method in fill_config.items():
                    if column in df.columns:
                        method = fill_method.get('method', 'mean')
                        missing_count = df[column].isnull().sum()
                        
                        if method == 'mean' and df[column].dtype in ['int64', 'float64']:
                            df[column] = df[column].fillna(df[column].mean())
                        elif method == 'median' and df[column].dtype in ['int64', 'float64']:
                            df[column] = df[column].fillna(df[column].median())
                        elif method == 'mode':
                            mode_val = df[column].mode()
                            if not mode_val.empty:
                                df[column] = df[column].fillna(mode_val.iloc[0])
                        elif method == 'forward':
                            df[column] = df[column].fillna(method='ffill')
                        elif method == 'backward':
                            df[column] = df[column].fillna(method='bfill')
                        else:
                            # è‡ªå®šä¹‰å€¼
                            fill_value = fill_method.get('value', '')
                            df[column] = df[column].fillna(fill_value)
                        
                        operations_performed.append(f"å¡«å……ç¼ºå¤±å€¼ {column}: {missing_count}ä¸ª")
            
            # å¼‚å¸¸å€¼å¤„ç†
            if 'remove_outliers' in config:
                outlier_config = config['remove_outliers']
                columns = outlier_config.get('columns', [])
                method = outlier_config.get('method', 'iqr')
                threshold = outlier_config.get('threshold', 1.5)
                
                for col in columns:
                    if col in df.columns and df[col].dtype in ['int64', 'float64']:
                        before_count = len(df)
                        
                        if method == 'iqr':
                            Q1 = df[col].quantile(0.25)
                            Q3 = df[col].quantile(0.75)
                            IQR = Q3 - Q1
                            lower_bound = Q1 - threshold * IQR
                            upper_bound = Q3 + threshold * IQR
                            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
                        elif method == 'zscore':
                            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                            df = df[z_scores <= threshold]
                        
                        removed_count = before_count - len(df)
                        operations_performed.append(f"ç§»é™¤å¼‚å¸¸å€¼ {col}: {removed_count}è¡Œ")
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or data_source
            if not data_source.upper().startswith('SELECT'):
                # å¦‚æœæ˜¯è¡¨åï¼Œä¿å­˜åˆ°ç›®æ ‡è¡¨
                df.to_sql(final_table, conn, if_exists='replace', index=False)
            else:
                # å¦‚æœæ˜¯æŸ¥è¯¢ï¼Œå¿…é¡»æŒ‡å®šç›®æ ‡è¡¨
                if not target_table:
                    return {"error": "å¤„ç†æŸ¥è¯¢ç»“æœæ—¶å¿…é¡»æŒ‡å®štarget_table"}
                df.to_sql(target_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "processed_rows": len(df),
                "original_rows": original_count,
                "operations": operations_performed,
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}"}

def _process_transform(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®è½¬æ¢å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            operations_performed = []
            
            # åˆ—é‡å‘½å
            if 'rename_columns' in config:
                rename_map = config['rename_columns']
                df = df.rename(columns=rename_map)
                operations_performed.append(f"é‡å‘½ååˆ—: {list(rename_map.keys())} -> {list(rename_map.values())}")
            
            # æ•°æ®æ ‡å‡†åŒ–
            if 'normalize' in config:
                normalize_config = config['normalize']
                columns = normalize_config.get('columns', [])
                method = normalize_config.get('method', 'minmax')  # minmax, zscore
                
                for col in columns:
                    if col in df.columns and df[col].dtype in ['int64', 'float64']:
                        if method == 'minmax':
                            df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())
                        elif method == 'zscore':
                            df[col] = (df[col] - df[col].mean()) / df[col].std()
                        
                        operations_performed.append(f"æ ‡å‡†åŒ–åˆ— {col} (æ–¹æ³•: {method})")
            
            # æ–°åˆ—è®¡ç®—
            if 'add_columns' in config:
                add_config = config['add_columns']
                for new_col, formula in add_config.items():
                    try:
                        # ç®€å•çš„å…¬å¼è®¡ç®—ï¼ˆå®‰å…¨æ€§è€ƒè™‘ï¼Œåªæ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
                        df[new_col] = df.eval(formula)
                        operations_performed.append(f"æ·»åŠ æ–°åˆ— {new_col}: {formula}")
                    except Exception as e:
                        operations_performed.append(f"æ·»åŠ æ–°åˆ— {new_col} å¤±è´¥: {str(e)}")
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if 'convert_types' in config:
                type_config = config['convert_types']
                for col, new_type in type_config.items():
                    if col in df.columns:
                        try:
                            if new_type == 'int':
                                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
                            elif new_type == 'float':
                                df[col] = pd.to_numeric(df[col], errors='coerce')
                            elif new_type == 'str':
                                df[col] = df[col].astype(str)
                            elif new_type == 'datetime':
                                df[col] = pd.to_datetime(df[col], errors='coerce')
                            
                            operations_performed.append(f"è½¬æ¢åˆ—ç±»å‹ {col} -> {new_type}")
                        except Exception as e:
                            operations_performed.append(f"è½¬æ¢åˆ—ç±»å‹ {col} å¤±è´¥: {str(e)}")
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or data_source
            if not data_source.upper().startswith('SELECT'):
                df.to_sql(final_table, conn, if_exists='replace', index=False)
            else:
                if not target_table:
                    return {"error": "å¤„ç†æŸ¥è¯¢ç»“æœæ—¶å¿…é¡»æŒ‡å®štarget_table"}
                df.to_sql(target_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "processed_rows": len(df),
                "operations": operations_performed,
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®è½¬æ¢å¤±è´¥: {str(e)}"}

def _process_filter(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®ç­›é€‰å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            original_count = len(df)
            operations_performed = []
            
            # æ¡ä»¶ç­›é€‰
            if 'filter_condition' in config:
                condition = config['filter_condition']
                try:
                    df = df.query(condition)
                    operations_performed.append(f"æ¡ä»¶ç­›é€‰: {condition}")
                except Exception as e:
                    return {"error": f"ç­›é€‰æ¡ä»¶é”™è¯¯: {str(e)}"}
            
            # åˆ—é€‰æ‹©
            if 'select_columns' in config:
                columns = config['select_columns']
                available_columns = [col for col in columns if col in df.columns]
                if available_columns:
                    df = df[available_columns]
                    operations_performed.append(f"é€‰æ‹©åˆ—: {available_columns}")
                else:
                    return {"error": "æŒ‡å®šçš„åˆ—éƒ½ä¸å­˜åœ¨"}
            
            # æ•°æ®é‡‡æ ·
            if 'sample' in config:
                sample_config = config['sample']
                sample_type = sample_config.get('type', 'random')  # random, head, tail
                sample_size = sample_config.get('size', 1000)
                
                if sample_type == 'random':
                    if sample_size < len(df):
                        df = df.sample(n=sample_size, random_state=42)
                        operations_performed.append(f"éšæœºé‡‡æ ·: {sample_size}è¡Œ")
                elif sample_type == 'head':
                    df = df.head(sample_size)
                    operations_performed.append(f"å¤´éƒ¨é‡‡æ ·: {sample_size}è¡Œ")
                elif sample_type == 'tail':
                    df = df.tail(sample_size)
                    operations_performed.append(f"å°¾éƒ¨é‡‡æ ·: {sample_size}è¡Œ")
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or data_source
            if not data_source.upper().startswith('SELECT'):
                df.to_sql(final_table, conn, if_exists='replace', index=False)
            else:
                if not target_table:
                    return {"error": "å¤„ç†æŸ¥è¯¢ç»“æœæ—¶å¿…é¡»æŒ‡å®štarget_table"}
                df.to_sql(target_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "filtered_rows": len(df),
                "original_rows": original_count,
                "operations": operations_performed,
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®ç­›é€‰å¤±è´¥: {str(e)}"}

def _process_aggregate(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®èšåˆå¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            operations_performed = []
            
            # åˆ†ç»„èšåˆ
            if 'group_by' in config:
                group_config = config['group_by']
                group_columns = group_config.get('columns', [])
                agg_config = group_config.get('agg', {})
                
                if not group_columns:
                    return {"error": "åˆ†ç»„èšåˆéœ€è¦æŒ‡å®šgroup_byåˆ—"}
                
                # æ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨
                missing_cols = [col for col in group_columns if col not in df.columns]
                if missing_cols:
                    return {"error": f"åˆ†ç»„åˆ—ä¸å­˜åœ¨: {missing_cols}"}
                
                # æ‰§è¡Œåˆ†ç»„èšåˆ
                try:
                    if agg_config:
                        df = df.groupby(group_columns).agg(agg_config).reset_index()
                        # æ‰å¹³åŒ–å¤šçº§åˆ—å
                        df.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in df.columns]
                    else:
                        # é»˜è®¤è®¡æ•°
                        df = df.groupby(group_columns).size().reset_index(name='count')
                    
                    operations_performed.append(f"åˆ†ç»„èšåˆ: {group_columns} -> {list(agg_config.keys()) if agg_config else ['count']}")
                except Exception as e:
                    return {"error": f"åˆ†ç»„èšåˆå¤±è´¥: {str(e)}"}
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or f"{data_source}_aggregated"
            if data_source.upper().startswith('SELECT') and not target_table:
                final_table = "query_aggregated"
            
            df.to_sql(final_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "processed_rows": len(df),
                "operations": operations_performed,
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®èšåˆå¤±è´¥: {str(e)}"}

def _process_merge(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®åˆå¹¶å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–å·¦è¡¨æ•°æ®
            if data_source.upper().startswith('SELECT'):
                left_df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                left_df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            # è·å–å³è¡¨æ•°æ®
            right_table = config.get('right_table')
            if not right_table:
                return {"error": "åˆå¹¶æ“ä½œéœ€è¦æŒ‡å®šright_table"}
            
            if not _table_exists(right_table):
                return {"error": f"å³è¡¨ '{right_table}' ä¸å­˜åœ¨"}
            
            escaped_right = _escape_identifier(right_table)
            right_df = pd.read_sql(f'SELECT * FROM {escaped_right}', conn)
            
            # åˆå¹¶å‚æ•°
            on_columns = config.get('on', [])
            how = config.get('how', 'inner')  # inner, left, right, outer
            
            if not on_columns:
                return {"error": "åˆå¹¶æ“ä½œéœ€è¦æŒ‡å®šonå‚æ•°ï¼ˆå…³è”åˆ—ï¼‰"}
            
            # æ£€æŸ¥å…³è”åˆ—æ˜¯å¦å­˜åœ¨
            missing_left = [col for col in on_columns if col not in left_df.columns]
            missing_right = [col for col in on_columns if col not in right_df.columns]
            
            if missing_left:
                return {"error": f"å·¦è¡¨ç¼ºå°‘å…³è”åˆ—: {missing_left}"}
            if missing_right:
                return {"error": f"å³è¡¨ç¼ºå°‘å…³è”åˆ—: {missing_right}"}
            
            # æ‰§è¡Œåˆå¹¶
            try:
                merged_df = pd.merge(left_df, right_df, on=on_columns, how=how, suffixes=('_left', '_right'))
                operations_performed = [f"è¡¨åˆå¹¶: {data_source} {how} join {right_table} on {on_columns}"]
            except Exception as e:
                return {"error": f"è¡¨åˆå¹¶å¤±è´¥: {str(e)}"}
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or f"{data_source}_merged"
            if data_source.upper().startswith('SELECT') and not target_table:
                final_table = "query_merged"
            
            merged_df.to_sql(final_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "processed_rows": len(merged_df),
                "operations": operations_performed,
                "columns": list(merged_df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}"}

def _process_reshape(data_source: str, config: dict, target_table: str = None) -> dict:
    """æ•°æ®é‡å¡‘å¤„ç†å™¨"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            operations_performed = []
            
            # é€è§†è¡¨
            if 'pivot' in config:
                pivot_config = config['pivot']
                index = pivot_config.get('index')
                columns = pivot_config.get('columns')
                values = pivot_config.get('values')
                
                if not all([index, columns, values]):
                    return {"error": "é€è§†è¡¨éœ€è¦æŒ‡å®šindex, columns, valueså‚æ•°"}
                
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                missing_cols = [col for col in [index, columns, values] if col not in df.columns]
                if missing_cols:
                    return {"error": f"é€è§†è¡¨åˆ—ä¸å­˜åœ¨: {missing_cols}"}
                
                try:
                    df = df.pivot_table(index=index, columns=columns, values=values, aggfunc='mean').reset_index()
                    df.columns.name = None  # ç§»é™¤åˆ—å
                    operations_performed.append(f"é€è§†è¡¨: index={index}, columns={columns}, values={values}")
                except Exception as e:
                    return {"error": f"é€è§†è¡¨æ“ä½œå¤±è´¥: {str(e)}"}
            
            # å®½è¡¨è½¬é•¿è¡¨
            elif 'melt' in config:
                melt_config = config['melt']
                id_vars = melt_config.get('id_vars', [])
                value_vars = melt_config.get('value_vars', [])
                var_name = melt_config.get('var_name', 'variable')
                value_name = melt_config.get('value_name', 'value')
                
                try:
                    df = pd.melt(df, id_vars=id_vars, value_vars=value_vars, 
                               var_name=var_name, value_name=value_name)
                    operations_performed.append(f"å®½è¡¨è½¬é•¿è¡¨: id_vars={id_vars}, value_vars={value_vars}")
                except Exception as e:
                    return {"error": f"å®½è¡¨è½¬é•¿è¡¨å¤±è´¥: {str(e)}"}
            
            else:
                return {"error": "é‡å¡‘æ“ä½œéœ€è¦æŒ‡å®špivotæˆ–melté…ç½®"}
            
            # ä¿å­˜ç»“æœ
            final_table = target_table or f"{data_source}_reshaped"
            if data_source.upper().startswith('SELECT') and not target_table:
                final_table = "query_reshaped"
            
            df.to_sql(final_table, conn, if_exists='replace', index=False)
            
            return {
                "target_table": final_table,
                "processed_rows": len(df),
                "operations": operations_performed,
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"æ•°æ®é‡å¡‘å¤±è´¥: {str(e)}"}

# ================================
# æ•°æ®å¯¼å‡ºè¾…åŠ©å‡½æ•°
# ================================

def _export_to_excel(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°Excelæ–‡ä»¶"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å¯¼å‡ºé€‰é¡¹
            sheet_name = options.get('sheet_name', 'Sheet1')
            auto_adjust = options.get('auto_adjust_columns', True)
            
            # å¯¼å‡ºåˆ°Excel
            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                if auto_adjust:
                    worksheet = writer.sheets[sheet_name]
                    for column in worksheet.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 50)
                        worksheet.column_dimensions[column_letter].width = adjusted_width
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"Excelå¯¼å‡ºå¤±è´¥: {str(e)}"}

def _export_to_csv(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°CSVæ–‡ä»¶"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å¯¼å‡ºé€‰é¡¹
            encoding = options.get('encoding', 'utf-8')
            separator = options.get('separator', ',')
            
            # å¯¼å‡ºåˆ°CSV
            df.to_csv(file_path, index=False, encoding=encoding, sep=separator)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"CSVå¯¼å‡ºå¤±è´¥: {str(e)}"}

def _export_to_json(data_source: str, file_path: str, options: dict) -> dict:
    """å¯¼å‡ºåˆ°JSONæ–‡ä»¶"""
    try:
        with get_db_connection() as conn:
            # è·å–æ•°æ®
            if data_source.upper().startswith('SELECT'):
                df = pd.read_sql(data_source, conn)
            else:
                if not _table_exists(data_source):
                    return {"error": f"è¡¨ '{data_source}' ä¸å­˜åœ¨"}
                escaped_table = _escape_identifier(data_source)
                df = pd.read_sql(f'SELECT * FROM {escaped_table}', conn)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å¯¼å‡ºé€‰é¡¹
            orient = options.get('orient', 'records')  # records, index, values, split, table
            indent = options.get('indent', 2)
            
            # å¯¼å‡ºåˆ°JSON
            df.to_json(file_path, orient=orient, indent=indent, force_ascii=False)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            
            return {
                "file_size": file_size,
                "record_count": len(df),
                "columns": list(df.columns)
            }
            
    except Exception as e:
        return {"error": f"JSONå¯¼å‡ºå¤±è´¥: {str(e)}"}

# ================================
# æ¨¡å—åˆå§‹åŒ–å‡½æ•°
# ================================

def init_data_processing_module():
    """åˆå§‹åŒ–æ•°æ®å¤„ç†æ¨¡å—"""
    logger.info("æ•°æ®å¤„ç†æ¨¡å—å·²åˆå§‹åŒ–")
    
    # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(EXPORTS_DIR):
        os.makedirs(EXPORTS_DIR)
        logger.info(f"åˆ›å»ºå¯¼å‡ºç›®å½•: {EXPORTS_DIR}")